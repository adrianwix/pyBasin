{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyBasin","text":"<p>Basin stability estimation for dynamical systems</p> <p> </p> <p>pyBasin is a Python library for estimating basin stability in dynamical systems. It's a port of the MATLAB bSTAB library with additional features including adaptive sampling and neural network-based classification.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Basin Stability Estimation: Calculate the probability that a system ends up in a specific attractor</li> <li>Adaptive Sampling: Intelligent sampling strategies that focus on uncertain regions</li> <li>Multiple Solvers: Support for various ODE solvers including neural ODE</li> <li>Visualization Tools: Built-in plotting utilities for basin stability results</li> <li>Extensible: Easy to add custom feature extractors and classifiers</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pybasin\n</code></pre> <p>For development:</p> <pre><code># Clone the repository\ngit clone https://github.com/adrianwix/pyBasin.git\ncd pyBasinWorkspace\n\n# Install with UV\nuv add -e \".[dev,docs]\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from pybasin import BasinStabilityEstimator, ODESystem\nimport numpy as np\n\n# Define your dynamical system\nclass MySystem(ODESystem):\n    def dynamics(self, t, state):\n        x, y = state\n        dx = -x + y\n        dy = -y - x**3\n        return np.array([dx, dy])\n\n    def classify_attractor(self, solution):\n        # Classify final state\n        final_state = solution.y[:, -1]\n        if np.linalg.norm(final_state) &lt; 0.1:\n            return 0  # Attractor 1\n        return 1  # Attractor 2\n\n# Create estimator\nsystem = MySystem()\nestimator = BasinStabilityEstimator(system)\n\n# Define sampling region\nbounds = [(-2, 2), (-2, 2)]  # x and y bounds\n\n# Estimate basin stability\nresults = estimator.estimate(bounds, n_samples=1000)\n\nprint(f\"Basin stability: {results.basin_stability}\")\nprint(f\"Attractor distribution: {results.attractor_counts}\")\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation is available at https://adrianwix.github.io/pyBasin/</p>"},{"location":"#case-studies","title":"Case Studies","text":"<p>This repository includes several case studies from the original bSTAB paper:</p> <ul> <li>Duffing Oscillator: Forced oscillator with two attractors</li> <li>Lorenz System: Classic chaotic system</li> <li>Pendulum: Forced pendulum with bifurcations</li> <li>Friction System: System with friction effects</li> </ul> <p>See the <code>case_studies/</code> directory for implementations.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>pyBasinWorkspace/\n\u251c\u2500\u2500 src/pybasin/          # Main library code\n\u251c\u2500\u2500 case_studies/         # Research case studies\n\u251c\u2500\u2500 tests/                # Unit and integration tests\n\u251c\u2500\u2500 docs/                 # Documentation source\n\u251c\u2500\u2500 artifacts/            # Generated figures and results\n\u2514\u2500\u2500 notebooks/            # Jupyter notebook examples\n</code></pre>"},{"location":"#development","title":"Development","text":""},{"location":"#setup","title":"Setup","text":"<pre><code># Install all dependencies including dev tools\nuv add -e \".[all]\"\n</code></pre>"},{"location":"#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"#building-documentation","title":"Building Documentation","text":"<pre><code>mkdocs serve  # Local preview\nmkdocs build  # Build static site\n</code></pre>"},{"location":"#related-projects","title":"Related Projects","text":"<ul> <li>bSTAB: Original MATLAB implementation - GitHub</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use pyBasin in your research, please cite:</p> <pre><code>@software{pybasin2025,\n  author = {Wix, Adrian},\n  title = {pyBasin: Basin Stability Estimation for Dynamical Systems},\n  year = {2025},\n  url = {https://github.com/adrianwix/pyBasin}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Based on the bSTAB MATLAB library</li> <li>Part of a bachelor thesis on basin stability estimation</li> </ul>"},{"location":"macros/","title":"Macros","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"MkDocs macros for case study documentation.\n\nThis module provides macros for rendering comparison tables from JSON artifacts\nand loading code snippets from source files.\n\"\"\"\n</pre> \"\"\"MkDocs macros for case study documentation.  This module provides macros for rendering comparison tables from JSON artifacts and loading code snippets from source files. \"\"\" <p>pyright: reportUnknownMemberType=false, reportUnknownArgumentType=false pyright: reportUnknownVariableType=false, reportMissingTypeArgument=false</p> In\u00a0[\u00a0]: Copied! <pre>import ast\nimport json\nfrom pathlib import Path\nfrom typing import Any\n</pre> import ast import json from pathlib import Path from typing import Any In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom scipy import stats as scipy_stats\n</pre> import numpy as np import pandas as pd from scipy import stats as scipy_stats In\u00a0[\u00a0]: Copied! <pre>ARTIFACTS_DIR = Path(__file__).parent.parent / \"artifacts\" / \"results\"\n</pre> ARTIFACTS_DIR = Path(__file__).parent.parent / \"artifacts\" / \"results\" In\u00a0[\u00a0]: Copied! <pre>def _format_bs_with_se(bs: float, se: float) -&gt; str:\n    \"\"\"Format basin stability with standard error.\"\"\"\n    return f\"{bs:.4f} \u00b1 {se:.4f}\"\n</pre> def _format_bs_with_se(bs: float, se: float) -&gt; str:     \"\"\"Format basin stability with standard error.\"\"\"     return f\"{bs:.4f} \u00b1 {se:.4f}\" In\u00a0[\u00a0]: Copied! <pre>def comparison_table(case_id: str) -&gt; str:\n    \"\"\"Render a comparison table from a JSON artifact.\n\n    For single-point tests, renders a table with columns:\n    Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC\n\n    For parameter sweep tests, adds a Parameter column first:\n    Parameter | Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC\n\n    For unsupervised tests, adds cluster quality metrics and purity column:\n    Attractor | DBSCAN | Purity | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC\n\n    For paper validation tests (no ground truth labels), shows confidence intervals:\n    Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status\n\n    Also shows overall Macro F1 and MCC in a summary section.\n\n    :param case_id: Case identifier (e.g., \"pendulum_case1\", \"pendulum_case2\").\n    :return: Markdown table string.\n    \"\"\"\n    json_path = ARTIFACTS_DIR / f\"{case_id}_comparison.json\"\n\n    if not json_path.exists():\n        return f'!!! warning \"Missing Data\"\\n    Comparison data not found: `{case_id}_comparison.json`\\n    Run tests with `--generate-artifacts` to generate.'\n\n    with open(json_path) as f:\n        data: dict[str, Any] = json.load(f)\n\n    # Detect paper validation case: explicit flag in JSON\n    is_paper_validation = data.get(\"paper_validation\", False)\n    if not is_paper_validation and \"parameter_results\" in data:\n        param_results: list[dict[str, Any]] = data[\"parameter_results\"]\n        if param_results:\n            is_paper_validation = param_results[0].get(\"paper_validation\", False)\n\n    if is_paper_validation:\n        if \"parameter_results\" in data:\n            return _render_paper_validation_sweep_table(data)\n        return _render_paper_validation_table(data)\n\n    if \"parameter_results\" in data:\n        return _render_parameter_sweep_table(data)\n    if \"overall_agreement\" in data:\n        return _render_unsupervised_table(data)\n    return _render_single_point_table(data)\n</pre> def comparison_table(case_id: str) -&gt; str:     \"\"\"Render a comparison table from a JSON artifact.      For single-point tests, renders a table with columns:     Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC      For parameter sweep tests, adds a Parameter column first:     Parameter | Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC      For unsupervised tests, adds cluster quality metrics and purity column:     Attractor | DBSCAN | Purity | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 | MCC      For paper validation tests (no ground truth labels), shows confidence intervals:     Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status      Also shows overall Macro F1 and MCC in a summary section.      :param case_id: Case identifier (e.g., \"pendulum_case1\", \"pendulum_case2\").     :return: Markdown table string.     \"\"\"     json_path = ARTIFACTS_DIR / f\"{case_id}_comparison.json\"      if not json_path.exists():         return f'!!! warning \"Missing Data\"\\n    Comparison data not found: `{case_id}_comparison.json`\\n    Run tests with `--generate-artifacts` to generate.'      with open(json_path) as f:         data: dict[str, Any] = json.load(f)      # Detect paper validation case: explicit flag in JSON     is_paper_validation = data.get(\"paper_validation\", False)     if not is_paper_validation and \"parameter_results\" in data:         param_results: list[dict[str, Any]] = data[\"parameter_results\"]         if param_results:             is_paper_validation = param_results[0].get(\"paper_validation\", False)      if is_paper_validation:         if \"parameter_results\" in data:             return _render_paper_validation_sweep_table(data)         return _render_paper_validation_table(data)      if \"parameter_results\" in data:         return _render_parameter_sweep_table(data)     if \"overall_agreement\" in data:         return _render_unsupervised_table(data)     return _render_single_point_table(data) In\u00a0[\u00a0]: Copied! <pre>def _render_paper_validation_table(data: dict[str, Any]) -&gt; str:\n    \"\"\"Render table for paper validation (statistical comparison).\"\"\"\n    attractors: list[dict[str, Any]] = data.get(\"attractors\", [])\n\n    if not attractors:\n        return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'\n\n    # Table header\n    table_lines: list[str] = [\n        \"| Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status |\",\n        \"|-----------|-----------------|---------------|------------|--------|--------|\",\n    ]\n\n    for a in attractors:\n        python_bs: float = a[\"python_bs\"]\n        python_se: float = a[\"python_se\"]\n        matlab_bs: float = a[\"matlab_bs\"]\n        matlab_se: float = a[\"matlab_se\"]\n\n        python_str = _format_bs_with_se(python_bs, python_se)\n        matlab_str = _format_bs_with_se(matlab_bs, matlab_se)\n\n        # Compute difference and 95% confidence interval\n        diff = python_bs - matlab_bs\n        # For 95% confidence, z = 1.96\n        combined_se = (python_se**2 + matlab_se**2) ** 0.5\n        ci_margin = 1.96 * combined_se\n\n        # Check if difference is within CI (difference should contain 0)\n        within_ci = abs(diff) &lt;= ci_margin\n        status = \"\u2713\" if within_ci else \"\u2717\"\n\n        diff_str = f\"{diff:+.4f}\"\n        ci_str = f\"\u00b1{ci_margin:.4f}\"\n\n        table_lines.append(\n            f\"| {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | {status} |\"\n        )\n\n    return \"\\n\".join(table_lines)\n</pre> def _render_paper_validation_table(data: dict[str, Any]) -&gt; str:     \"\"\"Render table for paper validation (statistical comparison).\"\"\"     attractors: list[dict[str, Any]] = data.get(\"attractors\", [])      if not attractors:         return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'      # Table header     table_lines: list[str] = [         \"| Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status |\",         \"|-----------|-----------------|---------------|------------|--------|--------|\",     ]      for a in attractors:         python_bs: float = a[\"python_bs\"]         python_se: float = a[\"python_se\"]         matlab_bs: float = a[\"matlab_bs\"]         matlab_se: float = a[\"matlab_se\"]          python_str = _format_bs_with_se(python_bs, python_se)         matlab_str = _format_bs_with_se(matlab_bs, matlab_se)          # Compute difference and 95% confidence interval         diff = python_bs - matlab_bs         # For 95% confidence, z = 1.96         combined_se = (python_se**2 + matlab_se**2) ** 0.5         ci_margin = 1.96 * combined_se          # Check if difference is within CI (difference should contain 0)         within_ci = abs(diff) &lt;= ci_margin         status = \"\u2713\" if within_ci else \"\u2717\"          diff_str = f\"{diff:+.4f}\"         ci_str = f\"\u00b1{ci_margin:.4f}\"          table_lines.append(             f\"| {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | {status} |\"         )      return \"\\n\".join(table_lines) In\u00a0[\u00a0]: Copied! <pre>def _render_single_point_table(data: dict[str, Any]) -&gt; str:\n    \"\"\"Render table for single-point comparison.\"\"\"\n    attractors: list[dict[str, Any]] = data.get(\"attractors\", [])\n\n    if not attractors:\n        return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'\n\n    # Get overall metrics\n    macro_f1 = data.get(\"macro_f1\", 0.0)\n    mcc = data.get(\"matthews_corrcoef\", 0.0)\n\n    # Summary metrics\n    summary_lines: list[str] = [\n        \"**Overall Classification Quality:**\",\n        \"\",\n        f\"- Macro F1-score: {macro_f1:.4f}\",\n        f\"- Matthews Correlation Coefficient: {mcc:.4f}\",\n        \"\",\n    ]\n\n    # Attractor table\n    table_lines: list[str] = [\n        \"| Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 |\",\n        \"|-----------|-----------------|---------------|-----|\",\n    ]\n\n    for a in attractors:\n        python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])\n        matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])\n        f1_score: float = a[\"f1_score\"]\n\n        table_lines.append(f\"| {a['label']} | {python_str} | {matlab_str} | {f1_score:.4f} |\")\n\n    return \"\\n\".join(summary_lines + table_lines)\n</pre> def _render_single_point_table(data: dict[str, Any]) -&gt; str:     \"\"\"Render table for single-point comparison.\"\"\"     attractors: list[dict[str, Any]] = data.get(\"attractors\", [])      if not attractors:         return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'      # Get overall metrics     macro_f1 = data.get(\"macro_f1\", 0.0)     mcc = data.get(\"matthews_corrcoef\", 0.0)      # Summary metrics     summary_lines: list[str] = [         \"**Overall Classification Quality:**\",         \"\",         f\"- Macro F1-score: {macro_f1:.4f}\",         f\"- Matthews Correlation Coefficient: {mcc:.4f}\",         \"\",     ]      # Attractor table     table_lines: list[str] = [         \"| Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 |\",         \"|-----------|-----------------|---------------|-----|\",     ]      for a in attractors:         python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])         matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])         f1_score: float = a[\"f1_score\"]          table_lines.append(f\"| {a['label']} | {python_str} | {matlab_str} | {f1_score:.4f} |\")      return \"\\n\".join(summary_lines + table_lines) In\u00a0[\u00a0]: Copied! <pre>def _render_unsupervised_table(data: dict[str, Any]) -&gt; str:\n    \"\"\"Render table for unsupervised clustering comparison.\"\"\"\n    attractors: list[dict[str, Any]] = data.get(\"attractors\", [])\n\n    if not attractors:\n        return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'\n\n    # Cluster quality metrics summary\n    n_found = data.get(\"n_clusters_found\", 0)\n    n_expected = data.get(\"n_clusters_expected\", 0)\n    agreement = data.get(\"overall_agreement\", 0.0)\n    ari = data.get(\"adjusted_rand_index\", 0.0)\n    macro_f1 = data.get(\"macro_f1\", 0.0)\n    mcc = data.get(\"matthews_corrcoef\", 0.0)\n\n    summary_lines: list[str] = [\n        \"**Cluster Quality Metrics:**\",\n        \"\",\n        f\"- Clusters found: {n_found} (expected: {n_expected})\",\n        f\"- Overall agreement: {agreement:.1%}\",\n        f\"- Adjusted Rand Index: {ari:.4f}\",\n        f\"- Macro F1-score: {macro_f1:.4f}\",\n        f\"- Matthews Correlation Coefficient: {mcc:.4f}\",\n        \"\",\n    ]\n\n    # Attractor table with purity info\n    table_lines: list[str] = [\n        \"| Attractor | DBSCAN | Purity | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 |\",\n        \"|-----------|--------|--------|-----------------|---------------|-----|\",\n    ]\n\n    for a in attractors:\n        python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])\n        matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])\n        f1_score: float = a[\"f1_score\"]\n        dbscan_label = a.get(\"dbscan_label\", \"-\")\n        purity = a.get(\"purity\", 0.0)\n        purity_str = f\"{purity:.1%}\"\n\n        table_lines.append(\n            f\"| {a['label']} | {dbscan_label} | {purity_str} | {python_str} | {matlab_str} | {f1_score:.4f} |\"\n        )\n\n    return \"\\n\".join(summary_lines + table_lines)\n</pre> def _render_unsupervised_table(data: dict[str, Any]) -&gt; str:     \"\"\"Render table for unsupervised clustering comparison.\"\"\"     attractors: list[dict[str, Any]] = data.get(\"attractors\", [])      if not attractors:         return '!!! warning \"No Data\"\\n    No attractor data found in comparison.'      # Cluster quality metrics summary     n_found = data.get(\"n_clusters_found\", 0)     n_expected = data.get(\"n_clusters_expected\", 0)     agreement = data.get(\"overall_agreement\", 0.0)     ari = data.get(\"adjusted_rand_index\", 0.0)     macro_f1 = data.get(\"macro_f1\", 0.0)     mcc = data.get(\"matthews_corrcoef\", 0.0)      summary_lines: list[str] = [         \"**Cluster Quality Metrics:**\",         \"\",         f\"- Clusters found: {n_found} (expected: {n_expected})\",         f\"- Overall agreement: {agreement:.1%}\",         f\"- Adjusted Rand Index: {ari:.4f}\",         f\"- Macro F1-score: {macro_f1:.4f}\",         f\"- Matthews Correlation Coefficient: {mcc:.4f}\",         \"\",     ]      # Attractor table with purity info     table_lines: list[str] = [         \"| Attractor | DBSCAN | Purity | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | F1 |\",         \"|-----------|--------|--------|-----------------|---------------|-----|\",     ]      for a in attractors:         python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])         matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])         f1_score: float = a[\"f1_score\"]         dbscan_label = a.get(\"dbscan_label\", \"-\")         purity = a.get(\"purity\", 0.0)         purity_str = f\"{purity:.1%}\"          table_lines.append(             f\"| {a['label']} | {dbscan_label} | {purity_str} | {python_str} | {matlab_str} | {f1_score:.4f} |\"         )      return \"\\n\".join(summary_lines + table_lines) In\u00a0[\u00a0]: Copied! <pre>def _render_paper_validation_sweep_table(data: dict[str, Any]) -&gt; str:\n    \"\"\"Render table for paper validation parameter sweep as a single consolidated table.\"\"\"\n    parameter_results: list[dict[str, Any]] = data.get(\"parameter_results\", [])\n\n    if not parameter_results:\n        return '!!! warning \"No Data\"\\n    No parameter data found in comparison.'\n\n    param_name: str = data.get(\"parameter_name\", \"Parameter\")\n\n    # Build single consolidated table\n    table_lines: list[str] = [\n        f\"| {param_name} | Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status |\",\n        \"|-------------|-----------|-----------------|---------------|------------|--------|--------|\",\n    ]\n\n    for result in parameter_results:\n        param_value = result.get(\"parameter_value\")\n        if param_value is None:\n            param_str = \"-\"\n        elif isinstance(param_value, float):\n            if param_value &lt; 0.01:\n                param_str = f\"{param_value:.1e}\"\n            else:\n                param_str = f\"{param_value:.4f}\".rstrip(\"0\").rstrip(\".\")\n        else:\n            param_str = str(param_value)\n\n        attractors: list[dict[str, Any]] = result.get(\"attractors\", [])\n        for i, a in enumerate(attractors):\n            python_bs: float = a[\"python_bs\"]\n            python_se: float = a[\"python_se\"]\n            matlab_bs: float = a[\"matlab_bs\"]\n            matlab_se: float = a[\"matlab_se\"]\n\n            python_str = _format_bs_with_se(python_bs, python_se)\n            matlab_str = _format_bs_with_se(matlab_bs, matlab_se)\n\n            # Compute difference and 95% confidence interval\n            diff = python_bs - matlab_bs\n            combined_se = (python_se**2 + matlab_se**2) ** 0.5\n            ci_margin = 1.96 * combined_se\n\n            within_ci = abs(diff) &lt;= ci_margin\n            status = \"\u2713\" if within_ci else \"\u2717\"\n\n            diff_str = f\"{diff:+.4f}\"\n            ci_str = f\"\u00b1{ci_margin:.4f}\"\n\n            if i == 0:\n                table_lines.append(\n                    f\"| {param_str} | {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | {status} |\"\n                )\n            else:\n                table_lines.append(\n                    f\"| | {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | |\"\n                )\n\n    return \"\\n\".join(table_lines)\n</pre> def _render_paper_validation_sweep_table(data: dict[str, Any]) -&gt; str:     \"\"\"Render table for paper validation parameter sweep as a single consolidated table.\"\"\"     parameter_results: list[dict[str, Any]] = data.get(\"parameter_results\", [])      if not parameter_results:         return '!!! warning \"No Data\"\\n    No parameter data found in comparison.'      param_name: str = data.get(\"parameter_name\", \"Parameter\")      # Build single consolidated table     table_lines: list[str] = [         f\"| {param_name} | Attractor | pyBasin BS \u00b1 SE | Paper BS \u00b1 SE | Difference | 95% CI | Status |\",         \"|-------------|-----------|-----------------|---------------|------------|--------|--------|\",     ]      for result in parameter_results:         param_value = result.get(\"parameter_value\")         if param_value is None:             param_str = \"-\"         elif isinstance(param_value, float):             if param_value &lt; 0.01:                 param_str = f\"{param_value:.1e}\"             else:                 param_str = f\"{param_value:.4f}\".rstrip(\"0\").rstrip(\".\")         else:             param_str = str(param_value)          attractors: list[dict[str, Any]] = result.get(\"attractors\", [])         for i, a in enumerate(attractors):             python_bs: float = a[\"python_bs\"]             python_se: float = a[\"python_se\"]             matlab_bs: float = a[\"matlab_bs\"]             matlab_se: float = a[\"matlab_se\"]              python_str = _format_bs_with_se(python_bs, python_se)             matlab_str = _format_bs_with_se(matlab_bs, matlab_se)              # Compute difference and 95% confidence interval             diff = python_bs - matlab_bs             combined_se = (python_se**2 + matlab_se**2) ** 0.5             ci_margin = 1.96 * combined_se              within_ci = abs(diff) &lt;= ci_margin             status = \"\u2713\" if within_ci else \"\u2717\"              diff_str = f\"{diff:+.4f}\"             ci_str = f\"\u00b1{ci_margin:.4f}\"              if i == 0:                 table_lines.append(                     f\"| {param_str} | {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | {status} |\"                 )             else:                 table_lines.append(                     f\"| | {a['label']} | {python_str} | {matlab_str} | {diff_str} | {ci_str} | |\"                 )      return \"\\n\".join(table_lines) In\u00a0[\u00a0]: Copied! <pre>def _render_parameter_sweep_table(data: dict[str, Any]) -&gt; str:\n    \"\"\"Render table for parameter sweep comparison as a single consolidated table.\"\"\"\n    parameter_results: list[dict[str, Any]] = data.get(\"parameter_results\", [])\n\n    if not parameter_results:\n        return '!!! warning \"No Data\"\\n    No parameter data found in comparison.'\n\n    param_name: str = data.get(\"parameter_name\", \"Parameter\")\n\n    # Compute average MCC, excluding trivial cases (single attractor with matching BS)\n    mcc_values_for_avg: list[float] = []\n    excluded_count = 0\n\n    for result in parameter_results:\n        mcc = result.get(\"matthews_corrcoef\", 0.0)\n        attractors = result.get(\"attractors\", [])\n\n        # Check if this is a trivial case: single attractor with matching BS\n        is_trivial = False\n        if len(attractors) == 1:\n            a = attractors[0]\n            if abs(a[\"python_bs\"] - a[\"matlab_bs\"]) &lt; 1e-8:\n                is_trivial = True\n                excluded_count += 1\n\n        if not is_trivial:\n            mcc_values_for_avg.append(mcc)\n\n    # Build header with average\n    header_lines: list[str] = []\n    if mcc_values_for_avg:\n        avg_mcc = sum(mcc_values_for_avg) / len(mcc_values_for_avg)\n        header_lines.append(f\"**Average MCC = {avg_mcc:.4f}**\")\n        if excluded_count &gt; 0:\n            header_lines.append(\"\")\n            header_lines.append(\n                \"*The average excludes cases where there is only a single attractor and the basin stability \"\n                \"values are the same since MCC is 0 for single class cases, and would therefore drop the average.*\"\n            )\n        header_lines.append(\"\")\n\n    # Build single consolidated table\n    table_lines: list[str] = [\n        f\"| {param_name} | Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | MCC |\",\n        \"|-------------|-----------|-----------------|---------------|-----|\",\n    ]\n\n    for result in parameter_results:\n        param_value = result.get(\"parameter_value\")\n        if param_value is None:\n            param_str = \"-\"\n        elif isinstance(param_value, float):\n            if param_value &lt; 0.01:\n                param_str = f\"{param_value:.1e}\"\n            else:\n                param_str = f\"{param_value:.4f}\".rstrip(\"0\").rstrip(\".\")\n        else:\n            param_str = str(param_value)\n\n        mcc = result.get(\"matthews_corrcoef\", 0.0)\n        attractors: list[dict[str, Any]] = result.get(\"attractors\", [])\n\n        for i, a in enumerate(attractors):\n            python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])\n            matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])\n\n            if i == 0:\n                table_lines.append(\n                    f\"| {param_str} | {a['label']} | {python_str} | {matlab_str} | {mcc:.4f} |\"\n                )\n            else:\n                table_lines.append(f\"| | {a['label']} | {python_str} | {matlab_str} | |\")\n\n    return \"\\n\".join(header_lines + table_lines)\n</pre> def _render_parameter_sweep_table(data: dict[str, Any]) -&gt; str:     \"\"\"Render table for parameter sweep comparison as a single consolidated table.\"\"\"     parameter_results: list[dict[str, Any]] = data.get(\"parameter_results\", [])      if not parameter_results:         return '!!! warning \"No Data\"\\n    No parameter data found in comparison.'      param_name: str = data.get(\"parameter_name\", \"Parameter\")      # Compute average MCC, excluding trivial cases (single attractor with matching BS)     mcc_values_for_avg: list[float] = []     excluded_count = 0      for result in parameter_results:         mcc = result.get(\"matthews_corrcoef\", 0.0)         attractors = result.get(\"attractors\", [])          # Check if this is a trivial case: single attractor with matching BS         is_trivial = False         if len(attractors) == 1:             a = attractors[0]             if abs(a[\"python_bs\"] - a[\"matlab_bs\"]) &lt; 1e-8:                 is_trivial = True                 excluded_count += 1          if not is_trivial:             mcc_values_for_avg.append(mcc)      # Build header with average     header_lines: list[str] = []     if mcc_values_for_avg:         avg_mcc = sum(mcc_values_for_avg) / len(mcc_values_for_avg)         header_lines.append(f\"**Average MCC = {avg_mcc:.4f}**\")         if excluded_count &gt; 0:             header_lines.append(\"\")             header_lines.append(                 \"*The average excludes cases where there is only a single attractor and the basin stability \"                 \"values are the same since MCC is 0 for single class cases, and would therefore drop the average.*\"             )         header_lines.append(\"\")      # Build single consolidated table     table_lines: list[str] = [         f\"| {param_name} | Attractor | pyBasin BS \u00b1 SE | bSTAB BS \u00b1 SE | MCC |\",         \"|-------------|-----------|-----------------|---------------|-----|\",     ]      for result in parameter_results:         param_value = result.get(\"parameter_value\")         if param_value is None:             param_str = \"-\"         elif isinstance(param_value, float):             if param_value &lt; 0.01:                 param_str = f\"{param_value:.1e}\"             else:                 param_str = f\"{param_value:.4f}\".rstrip(\"0\").rstrip(\".\")         else:             param_str = str(param_value)          mcc = result.get(\"matthews_corrcoef\", 0.0)         attractors: list[dict[str, Any]] = result.get(\"attractors\", [])          for i, a in enumerate(attractors):             python_str = _format_bs_with_se(a[\"python_bs\"], a[\"python_se\"])             matlab_str = _format_bs_with_se(a[\"matlab_bs\"], a[\"matlab_se\"])              if i == 0:                 table_lines.append(                     f\"| {param_str} | {a['label']} | {python_str} | {matlab_str} | {mcc:.4f} |\"                 )             else:                 table_lines.append(f\"| | {a['label']} | {python_str} | {matlab_str} | |\")      return \"\\n\".join(header_lines + table_lines) In\u00a0[\u00a0]: Copied! <pre>def load_snippet(spec: str) -&gt; str:\n    \"\"\"Load a code snippet from a source file.\n\n    :param spec: Specification in format \"path/to/file.py::function_name\"\n                 Path should be relative to the workspace root.\n    :return: Markdown-formatted code block with the extracted function.\n    \"\"\"\n    try:\n        file_path_str, func_name = spec.split(\"::\")\n    except ValueError:\n        return f'!!! error \"Invalid Format\"\\n    Expected format: `path/to/file.py::function_name`\\n    Got: `{spec}`'\n\n    workspace_root = Path(__file__).parent.parent\n    file_path = workspace_root / file_path_str\n\n    if not file_path.exists():\n        return f'!!! error \"File Not Found\"\\n    Could not find file: `{file_path_str}`'\n\n    try:\n        source_code = file_path.read_text()\n        tree = ast.parse(source_code)\n\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef) and node.name == func_name:\n                lines = source_code.splitlines()\n                start_line = node.lineno - 1\n                end_line = node.end_lineno if node.end_lineno else len(lines)\n\n                function_code = \"\\n\".join(lines[start_line:end_line])\n\n                return f\"```python\\n{function_code}\\n```\"\n\n        return f'!!! warning \"Function Not Found\"\\n    Could not find function `{func_name}` in `{file_path_str}`'\n\n    except SyntaxError as e:\n        return f'!!! error \"Syntax Error\"\\n    Failed to parse `{file_path_str}`: {e}'\n    except Exception as e:\n        return f'!!! error \"Error\"\\n    Failed to load snippet: {e}'\n</pre> def load_snippet(spec: str) -&gt; str:     \"\"\"Load a code snippet from a source file.      :param spec: Specification in format \"path/to/file.py::function_name\"                  Path should be relative to the workspace root.     :return: Markdown-formatted code block with the extracted function.     \"\"\"     try:         file_path_str, func_name = spec.split(\"::\")     except ValueError:         return f'!!! error \"Invalid Format\"\\n    Expected format: `path/to/file.py::function_name`\\n    Got: `{spec}`'      workspace_root = Path(__file__).parent.parent     file_path = workspace_root / file_path_str      if not file_path.exists():         return f'!!! error \"File Not Found\"\\n    Could not find file: `{file_path_str}`'      try:         source_code = file_path.read_text()         tree = ast.parse(source_code)          for node in ast.walk(tree):             if isinstance(node, ast.FunctionDef) and node.name == func_name:                 lines = source_code.splitlines()                 start_line = node.lineno - 1                 end_line = node.end_lineno if node.end_lineno else len(lines)                  function_code = \"\\n\".join(lines[start_line:end_line])                  return f\"```python\\n{function_code}\\n```\"          return f'!!! warning \"Function Not Found\"\\n    Could not find function `{func_name}` in `{file_path_str}`'      except SyntaxError as e:         return f'!!! error \"Syntax Error\"\\n    Failed to parse `{file_path_str}`: {e}'     except Exception as e:         return f'!!! error \"Error\"\\n    Failed to load snippet: {e}' In\u00a0[\u00a0]: Copied! <pre>BENCHMARK_RESULTS_DIR = Path(__file__).parent.parent / \"benchmarks\" / \"end_to_end\" / \"results\"\nSOLVER_COMPARISON_RESULTS_DIR = (\n    Path(__file__).parent.parent / \"benchmarks\" / \"solver_comparison\" / \"results\"\n)\n</pre> BENCHMARK_RESULTS_DIR = Path(__file__).parent.parent / \"benchmarks\" / \"end_to_end\" / \"results\" SOLVER_COMPARISON_RESULTS_DIR = (     Path(__file__).parent.parent / \"benchmarks\" / \"solver_comparison\" / \"results\" ) In\u00a0[\u00a0]: Copied! <pre>def solver_comparison_table() -&gt; str:\n    \"\"\"Render solver comparison table from CSV data.\n\n    :return: Markdown table string.\n    \"\"\"\n    csv_path = SOLVER_COMPARISON_RESULTS_DIR / \"solver_comparison.csv\"\n\n    if not csv_path.exists():\n        return '!!! warning \"Missing Data\"\\n    Solver comparison data not found. Run `uv run python benchmarks/solver_comparison/compare_matlab_vs_python.py` to generate.'\n\n    df = pd.read_csv(csv_path)\n\n    n_values = sorted(df[\"N\"].unique())\n    sections: list[str] = []\n\n    for n in n_values:\n        n_data = df[df[\"N\"] == n].sort_values(\"mean_time\")\n        matlab_row = n_data[n_data[\"solver\"] == \"MATLAB ode45\"]\n        matlab_time = matlab_row[\"mean_time\"].values[0] if len(matlab_row) &gt; 0 else None\n\n        table_lines: list[str] = [\n            f\"### N = {n:,}\",\n            \"\",\n            \"| Solver | Device | Time (s) | Std Dev | vs MATLAB |\",\n            \"|--------|--------|----------:|--------:|----------:|\",\n        ]\n\n        for _, row in n_data.iterrows():\n            if matlab_time is not None:\n                speedup = matlab_time / row[\"mean_time\"]\n                speedup_str = f\"{speedup:.2f}x\"\n            else:\n                speedup_str = \"-\"\n            table_lines.append(\n                f\"| {row['solver']} | {row['device'].upper()} | {row['mean_time']:.2f} | \u00b1{row['std_time']:.2f} | {speedup_str} |\"\n            )\n\n        sections.append(\"\\n\".join(table_lines))\n\n    return \"\\n\\n\".join(sections)\n</pre> def solver_comparison_table() -&gt; str:     \"\"\"Render solver comparison table from CSV data.      :return: Markdown table string.     \"\"\"     csv_path = SOLVER_COMPARISON_RESULTS_DIR / \"solver_comparison.csv\"      if not csv_path.exists():         return '!!! warning \"Missing Data\"\\n    Solver comparison data not found. Run `uv run python benchmarks/solver_comparison/compare_matlab_vs_python.py` to generate.'      df = pd.read_csv(csv_path)      n_values = sorted(df[\"N\"].unique())     sections: list[str] = []      for n in n_values:         n_data = df[df[\"N\"] == n].sort_values(\"mean_time\")         matlab_row = n_data[n_data[\"solver\"] == \"MATLAB ode45\"]         matlab_time = matlab_row[\"mean_time\"].values[0] if len(matlab_row) &gt; 0 else None          table_lines: list[str] = [             f\"### N = {n:,}\",             \"\",             \"| Solver | Device | Time (s) | Std Dev | vs MATLAB |\",             \"|--------|--------|----------:|--------:|----------:|\",         ]          for _, row in n_data.iterrows():             if matlab_time is not None:                 speedup = matlab_time / row[\"mean_time\"]                 speedup_str = f\"{speedup:.2f}x\"             else:                 speedup_str = \"-\"             table_lines.append(                 f\"| {row['solver']} | {row['device'].upper()} | {row['mean_time']:.2f} | \u00b1{row['std_time']:.2f} | {speedup_str} |\"             )          sections.append(\"\\n\".join(table_lines))      return \"\\n\\n\".join(sections) In\u00a0[\u00a0]: Copied! <pre>def solver_matlab_speedup_table() -&gt; str:\n    \"\"\"Render speedup vs MATLAB table from CSV data.\n\n    :return: Markdown table string.\n    \"\"\"\n    csv_path = SOLVER_COMPARISON_RESULTS_DIR / \"solver_comparison.csv\"\n\n    if not csv_path.exists():\n        return '!!! warning \"Missing Data\"\\n    Solver comparison data not found.'\n\n    df = pd.read_csv(csv_path)\n\n    n_values = sorted(df[\"N\"].unique())\n\n    table_lines: list[str] = [\n        \"| N | Solver | Device | Time (s) | vs MATLAB |\",\n        \"|--:|--------|--------|----------:|----------:|\",\n    ]\n\n    for n in n_values:\n        n_data = df[df[\"N\"] == n]\n        matlab_row = n_data[n_data[\"solver\"] == \"MATLAB ode45\"]\n\n        if matlab_row.empty:\n            continue\n\n        matlab_time = matlab_row[\"mean_time\"].values[0]\n\n        for _, row in (\n            n_data[n_data[\"solver\"] != \"MATLAB ode45\"].sort_values(\"mean_time\").iterrows()\n        ):\n            speedup = matlab_time / row[\"mean_time\"]\n            direction = \"faster\" if speedup &gt; 1 else \"slower\"\n            speedup_str = f\"{speedup:.2f}x {direction}\"\n            table_lines.append(\n                f\"| {n:,} | {row['solver']} | {row['device'].upper()} | {row['mean_time']:.2f} | {speedup_str} |\"\n            )\n\n        table_lines.append(f\"| {n:,} | MATLAB ode45 | CPU | {matlab_time:.2f} | *baseline* |\")\n\n    return \"\\n\".join(table_lines)\n</pre> def solver_matlab_speedup_table() -&gt; str:     \"\"\"Render speedup vs MATLAB table from CSV data.      :return: Markdown table string.     \"\"\"     csv_path = SOLVER_COMPARISON_RESULTS_DIR / \"solver_comparison.csv\"      if not csv_path.exists():         return '!!! warning \"Missing Data\"\\n    Solver comparison data not found.'      df = pd.read_csv(csv_path)      n_values = sorted(df[\"N\"].unique())      table_lines: list[str] = [         \"| N | Solver | Device | Time (s) | vs MATLAB |\",         \"|--:|--------|--------|----------:|----------:|\",     ]      for n in n_values:         n_data = df[df[\"N\"] == n]         matlab_row = n_data[n_data[\"solver\"] == \"MATLAB ode45\"]          if matlab_row.empty:             continue          matlab_time = matlab_row[\"mean_time\"].values[0]          for _, row in (             n_data[n_data[\"solver\"] != \"MATLAB ode45\"].sort_values(\"mean_time\").iterrows()         ):             speedup = matlab_time / row[\"mean_time\"]             direction = \"faster\" if speedup &gt; 1 else \"slower\"             speedup_str = f\"{speedup:.2f}x {direction}\"             table_lines.append(                 f\"| {n:,} | {row['solver']} | {row['device'].upper()} | {row['mean_time']:.2f} | {speedup_str} |\"             )          table_lines.append(f\"| {n:,} | MATLAB ode45 | CPU | {matlab_time:.2f} | *baseline* |\")      return \"\\n\".join(table_lines) In\u00a0[\u00a0]: Copied! <pre>def benchmark_comparison_table() -&gt; str:\n    \"\"\"Render benchmark comparison table from CSV data.\n\n    :return: Markdown table string.\n    \"\"\"\n    csv_path = BENCHMARK_RESULTS_DIR / \"end_to_end_comparison.csv\"\n\n    if not csv_path.exists():\n        return '!!! warning \"Missing Data\"\\n    Benchmark data not found. Run `uv run python benchmarks/end_to_end/compare_matlab_vs_python.py` to generate.'\n\n    df = pd.read_csv(csv_path)\n\n    n_values = sorted(df[\"N\"].unique())\n\n    table_lines: list[str] = [\n        \"| N | MATLAB (s) | Python CPU (s) | Python CUDA (s) | CPU vs MATLAB | GPU vs MATLAB |\",\n        \"|--:|----------:|---------------:|----------------:|--------------:|--------------:|\",\n    ]\n\n    for n in n_values:\n        n_data = df[df[\"N\"] == n]\n\n        matlab_row = n_data[n_data[\"implementation\"] == \"MATLAB\"]\n        cpu_row = n_data[n_data[\"implementation\"] == \"Python CPU\"]\n        cuda_row = n_data[n_data[\"implementation\"] == \"Python CUDA\"]\n\n        matlab_time = matlab_row[\"mean_time\"].values[0] if len(matlab_row) &gt; 0 else float(\"nan\")\n        cpu_time = cpu_row[\"mean_time\"].values[0] if len(cpu_row) &gt; 0 else float(\"nan\")\n        cuda_time = cuda_row[\"mean_time\"].values[0] if len(cuda_row) &gt; 0 else float(\"nan\")\n\n        cpu_speedup = matlab_time / cpu_time if cpu_time &gt; 0 else float(\"nan\")\n        gpu_speedup = matlab_time / cuda_time if cuda_time &gt; 0 else float(\"nan\")\n\n        table_lines.append(\n            f\"| {n:,} | {matlab_time:.2f} | {cpu_time:.2f} | {cuda_time:.2f} | {cpu_speedup:.1f}x | {gpu_speedup:.1f}x |\"\n        )\n\n    return \"\\n\".join(table_lines)\n</pre> def benchmark_comparison_table() -&gt; str:     \"\"\"Render benchmark comparison table from CSV data.      :return: Markdown table string.     \"\"\"     csv_path = BENCHMARK_RESULTS_DIR / \"end_to_end_comparison.csv\"      if not csv_path.exists():         return '!!! warning \"Missing Data\"\\n    Benchmark data not found. Run `uv run python benchmarks/end_to_end/compare_matlab_vs_python.py` to generate.'      df = pd.read_csv(csv_path)      n_values = sorted(df[\"N\"].unique())      table_lines: list[str] = [         \"| N | MATLAB (s) | Python CPU (s) | Python CUDA (s) | CPU vs MATLAB | GPU vs MATLAB |\",         \"|--:|----------:|---------------:|----------------:|--------------:|--------------:|\",     ]      for n in n_values:         n_data = df[df[\"N\"] == n]          matlab_row = n_data[n_data[\"implementation\"] == \"MATLAB\"]         cpu_row = n_data[n_data[\"implementation\"] == \"Python CPU\"]         cuda_row = n_data[n_data[\"implementation\"] == \"Python CUDA\"]          matlab_time = matlab_row[\"mean_time\"].values[0] if len(matlab_row) &gt; 0 else float(\"nan\")         cpu_time = cpu_row[\"mean_time\"].values[0] if len(cpu_row) &gt; 0 else float(\"nan\")         cuda_time = cuda_row[\"mean_time\"].values[0] if len(cuda_row) &gt; 0 else float(\"nan\")          cpu_speedup = matlab_time / cpu_time if cpu_time &gt; 0 else float(\"nan\")         gpu_speedup = matlab_time / cuda_time if cuda_time &gt; 0 else float(\"nan\")          table_lines.append(             f\"| {n:,} | {matlab_time:.2f} | {cpu_time:.2f} | {cuda_time:.2f} | {cpu_speedup:.1f}x | {gpu_speedup:.1f}x |\"         )      return \"\\n\".join(table_lines) In\u00a0[\u00a0]: Copied! <pre>def benchmark_scaling_analysis() -&gt; str:\n    \"\"\"Render scaling analysis summary from benchmark data.\n\n    :return: Markdown summary string.\n    \"\"\"\n    csv_path = BENCHMARK_RESULTS_DIR / \"end_to_end_comparison.csv\"\n\n    if not csv_path.exists():\n        return '!!! warning \"Missing Data\"\\n    Benchmark data not found.'\n\n    df = pd.read_csv(csv_path)\n\n    implementations = [\"MATLAB\", \"Python CPU\", \"Python CUDA\"]\n    results: list[str] = [\n        \"| Implementation | Scaling | Exponent \u03b1 | R\u00b2 |\",\n        \"|----------------|---------|------------|-----|\",\n    ]\n\n    for impl in implementations:\n        impl_data = df[df[\"implementation\"] == impl].sort_values(\"N\")\n        if len(impl_data) &lt; 3:\n            continue\n\n        n_vals = impl_data[\"N\"].values.astype(float)\n        t_vals = impl_data[\"mean_time\"].values\n\n        log_n = np.log(n_vals)\n        log_t = np.log(t_vals)\n        slope, _, r_value, _, std_err = scipy_stats.linregress(log_n, log_t)\n        alpha = slope\n        alpha_ci = 1.96 * std_err\n        r2 = r_value**2\n\n        if alpha &lt; 0.15:\n            complexity = \"O(1)\"\n        elif abs(alpha - 1.0) &lt; 0.15:\n            complexity = \"O(N)\"\n        elif abs(alpha - 2.0) &lt; 0.15:\n            complexity = \"O(N\u00b2)\"\n        else:\n            complexity = f\"O(N^{alpha:.2f})\"\n\n        results.append(f\"| {impl} | {complexity} | {alpha:.2f} \u00b1 {alpha_ci:.2f} | {r2:.3f} |\")\n\n    return \"\\n\".join(results)\n</pre> def benchmark_scaling_analysis() -&gt; str:     \"\"\"Render scaling analysis summary from benchmark data.      :return: Markdown summary string.     \"\"\"     csv_path = BENCHMARK_RESULTS_DIR / \"end_to_end_comparison.csv\"      if not csv_path.exists():         return '!!! warning \"Missing Data\"\\n    Benchmark data not found.'      df = pd.read_csv(csv_path)      implementations = [\"MATLAB\", \"Python CPU\", \"Python CUDA\"]     results: list[str] = [         \"| Implementation | Scaling | Exponent \u03b1 | R\u00b2 |\",         \"|----------------|---------|------------|-----|\",     ]      for impl in implementations:         impl_data = df[df[\"implementation\"] == impl].sort_values(\"N\")         if len(impl_data) &lt; 3:             continue          n_vals = impl_data[\"N\"].values.astype(float)         t_vals = impl_data[\"mean_time\"].values          log_n = np.log(n_vals)         log_t = np.log(t_vals)         slope, _, r_value, _, std_err = scipy_stats.linregress(log_n, log_t)         alpha = slope         alpha_ci = 1.96 * std_err         r2 = r_value**2          if alpha &lt; 0.15:             complexity = \"O(1)\"         elif abs(alpha - 1.0) &lt; 0.15:             complexity = \"O(N)\"         elif abs(alpha - 2.0) &lt; 0.15:             complexity = \"O(N\u00b2)\"         else:             complexity = f\"O(N^{alpha:.2f})\"          results.append(f\"| {impl} | {complexity} | {alpha:.2f} \u00b1 {alpha_ci:.2f} | {r2:.3f} |\")      return \"\\n\".join(results) In\u00a0[\u00a0]: Copied! <pre>def define_env(env: Any) -&gt; None:\n    \"\"\"Define macros for mkdocs-macros-plugin.\n\n    :param env: The macro environment.\n    \"\"\"\n    env.macro(comparison_table, \"comparison_table\")\n    env.macro(load_snippet, \"load_snippet\")\n    env.macro(benchmark_comparison_table, \"benchmark_comparison_table\")\n    env.macro(benchmark_scaling_analysis, \"benchmark_scaling_analysis\")\n    env.macro(solver_comparison_table, \"solver_comparison_table\")\n    env.macro(solver_matlab_speedup_table, \"solver_matlab_speedup_table\")\n</pre> def define_env(env: Any) -&gt; None:     \"\"\"Define macros for mkdocs-macros-plugin.      :param env: The macro environment.     \"\"\"     env.macro(comparison_table, \"comparison_table\")     env.macro(load_snippet, \"load_snippet\")     env.macro(benchmark_comparison_table, \"benchmark_comparison_table\")     env.macro(benchmark_scaling_analysis, \"benchmark_scaling_analysis\")     env.macro(solver_comparison_table, \"solver_comparison_table\")     env.macro(solver_matlab_speedup_table, \"solver_matlab_speedup_table\")"},{"location":"api/adaptive-sampling/","title":"BasinStabilityStudy","text":""},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy","title":"pybasin.basin_stability_study.BasinStabilityStudy","text":"<p>Basin Stability Study.</p>"},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy-attributes","title":"Attributes","text":""},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy.parameter_values","title":"parameter_values  <code>property</code>","text":"<pre><code>parameter_values: list[Any]\n</code></pre> <p>Legacy access to parameter values (for backward compatibility).</p> <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of parameter values from labels.</p>"},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy.__init__","title":"__init__","text":"<pre><code>__init__(\n    n: int,\n    ode_system: ODESystemProtocol,\n    sampler: Sampler,\n    solver: SolverProtocol,\n    feature_extractor: FeatureExtractor,\n    estimator: Any,\n    study_params: StudyParams,\n    template_integrator: TemplateIntegrator | None = None,\n    save_to: str | None = \"results\",\n    verbose: bool = False,\n)\n</code></pre> <p>Initialize the Basin Stability Study.</p> <p>Sets up the estimator for a parameter study where one or more parameters are systematically varied across multiple values. Parameters can be in any component (ODE system, sampler, solver, feature extractor, or predictor).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of initial conditions (samples) to generate for each parameter value.</p> required <code>ode_system</code> <code>ODESystemProtocol</code> <p>The ODE system model (ODESystem or JaxODESystem).</p> required <code>sampler</code> <code>Sampler</code> <p>The Sampler object to generate initial conditions.</p> required <code>solver</code> <code>SolverProtocol</code> <p>The Solver object to integrate the ODE system (Solver or JaxSolver).</p> required <code>feature_extractor</code> <code>FeatureExtractor</code> <p>The FeatureExtractor object to extract features from trajectories.</p> required <code>estimator</code> <code>Any</code> <p>Any sklearn-compatible estimator (classifier or clusterer).</p> required <code>study_params</code> <code>StudyParams</code> <p>Parameter study specification (SweepStudyParams, GridStudyParams, etc.).</p> required <code>template_integrator</code> <code>TemplateIntegrator | None</code> <p>Template integrator for supervised classifiers.</p> <code>None</code> <code>save_to</code> <code>str | None</code> <p>Folder path where results will be saved, or None to disable saving.</p> <code>'results'</code> <code>verbose</code> <code>bool</code> <p>If True, show detailed logs from BasinStabilityEstimator instances. If False, suppress INFO logs to reduce output clutter during parameter sweeps.</p> <code>False</code>"},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy.estimate_as_bs","title":"estimate_as_bs","text":"<pre><code>estimate_as_bs() -&gt; tuple[\n    list[dict[str, Any]],\n    list[dict[str, float]],\n    list[AdaptiveStudyResult],\n]\n</code></pre> <p>Estimate basin stability for each parameter combination in the study.</p> <p>Performs basin stability estimation by systematically varying parameters across the provided combinations. For each configuration:</p> <ol> <li>Applies all parameter assignments from the RunConfig</li> <li>Creates a new BasinStabilityEstimator instance</li> <li>Estimates basin stability and computes error estimates</li> <li>Stores results including basin stability values, errors, and solution metadata</li> </ol> <p>Uses GPU acceleration automatically when available for significant performance gains. Memory is explicitly freed after each iteration to prevent accumulation.</p> <p>Returns:</p> Type Description <code>tuple[list[dict[str, Any]], list[dict[str, float]], list[AdaptiveStudyResult]]</code> <p>Tuple of three lists with matching indices:  - labels: List of label dictionaries identifying each run - basin_stabilities: List of basin stability dictionaries (label -&gt; fraction) - results: List of AdaptiveStudyResult with complete information including errors</p>"},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Save the basin stability results to a JSON file. Handles numpy arrays and Solution objects by converting them to standard Python types.</p>"},{"location":"api/adaptive-sampling/#pybasin.basin_stability_study.BasinStabilityStudy.get_errors","title":"get_errors","text":"<pre><code>get_errors(param_index: int) -&gt; dict[str, ErrorInfo]\n</code></pre> <p>Get error information for basin stability estimates at a specific parameter value.</p> <p>Retrieves the pre-computed error estimates (absolute and relative standard errors) for all attractor labels at the specified parameter index.</p> <p>Parameters:</p> Name Type Description Default <code>param_index</code> <code>int</code> <p>Index of the parameter value in the adaptive study (0-based).</p> required <p>Returns:</p> Type Description <code>dict[str, ErrorInfo]</code> <p>Dictionary mapping each attractor label to its ErrorInfo containing e_abs and e_rel.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If estimate_as_bs() has not been called yet.</p> <code>ValueError</code> <p>If param_index is out of range.</p>"},{"location":"api/adaptive-sampling/#study-parameters","title":"Study Parameters","text":"<p>options: heading_level: 3</p> <p>options: heading_level: 3</p> <p>options: heading_level: 3</p> <p>options: heading_level: 3</p> <p>options: heading_level: 3</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.StudyParams","title":"pybasin.study_params.StudyParams","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for study parameter generators.</p> <p>Subclasses must implement iter to yield RunConfig objects and len to return the total number of runs.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.StudyParams-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.study_params.StudyParams.__iter__","title":"__iter__  <code>abstractmethod</code>","text":"<pre><code>__iter__() -&gt; Iterator[RunConfig]\n</code></pre> <p>Yield RunConfig for each parameter combination.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.StudyParams.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the total number of runs.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.SweepStudyParams","title":"pybasin.study_params.SweepStudyParams","text":"<p>               Bases: <code>StudyParams</code></p> <p>Single parameter sweep (current behavior).</p> <p>Iterates over a single parameter's values, yielding one RunConfig per value.</p> <p>Example:</p> <pre><code>study_params = SweepStudyParams(\n    name='ode_system.params[\"T\"]',\n    values=np.arange(0.01, 0.97, 0.05),\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>name</code> <p>The parameter path to vary.</p> <code>values</code> <code>list[Any]</code> <p>List of values to sweep through.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.SweepStudyParams-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.study_params.SweepStudyParams.__init__","title":"__init__","text":"<pre><code>__init__(name: str, values: list[Any]) -&gt; None\n</code></pre> <p>Initialize the sweep study parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The parameter path to vary, e.g., 'ode_system.params[\"T\"]'.</p> required <code>values</code> <code>list[Any]</code> <p>Array or list of values to sweep through.</p> required"},{"location":"api/adaptive-sampling/#pybasin.study_params.SweepStudyParams.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[RunConfig]\n</code></pre> <p>Yield RunConfig for each parameter value.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.SweepStudyParams.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the number of parameter values.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.GridStudyParams","title":"pybasin.study_params.GridStudyParams","text":"<p>               Bases: <code>StudyParams</code></p> <p>Cartesian product of multiple parameters.</p> <p>Creates all combinations of the provided parameter values (grid study).</p> <p>Example:</p> <pre><code>study_params = GridStudyParams(\n    **{\n        'ode_system.params[\"K\"]': k_values,\n        'ode_system.params[\"sigma\"]': sigma_values,\n    }\n)\n# Runs: K[0]\u00d7sigma[0], K[0]\u00d7sigma[1], ..., K[n]\u00d7sigma[m]\n</code></pre>"},{"location":"api/adaptive-sampling/#pybasin.study_params.GridStudyParams-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.study_params.GridStudyParams.__init__","title":"__init__","text":"<pre><code>__init__(**params: list[Any]) -&gt; None\n</code></pre> <p>Initialize the grid study parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[Any]</code> <p>Keyword arguments mapping parameter names to value arrays. e.g., GridStudyParams(**{'ode_system.params[\"T\"]': t_values})</p> <code>{}</code>"},{"location":"api/adaptive-sampling/#pybasin.study_params.GridStudyParams.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[RunConfig]\n</code></pre> <p>Yield RunConfig for each parameter combination (Cartesian product).</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.GridStudyParams.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the total number of combinations.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.ZipStudyParams","title":"pybasin.study_params.ZipStudyParams","text":"<p>               Bases: <code>StudyParams</code></p> <p>Parallel iteration of multiple parameters (must have same length).</p> <p>Iterates through parameters in parallel (like Python's zip), where values at the same index are used together.</p> <p>Example:</p> <pre><code>t_values = np.arange(0.01, 0.97, 0.05)\nsamplers = [CsvSampler(f\"gt_T_{t:.2f}.csv\") for t in t_values]\n\nstudy_params = ZipStudyParams(\n    **{\n        'ode_system.params[\"T\"]': t_values,\n        \"sampler\": samplers,\n    }\n)\n</code></pre>"},{"location":"api/adaptive-sampling/#pybasin.study_params.ZipStudyParams-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.study_params.ZipStudyParams.__init__","title":"__init__","text":"<pre><code>__init__(**params: list[Any]) -&gt; None\n</code></pre> <p>Initialize the zip study parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[Any]</code> <p>Keyword arguments mapping parameter names to value arrays. All arrays must have the same length.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameter arrays have different lengths.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.ZipStudyParams.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[RunConfig]\n</code></pre> <p>Yield RunConfig for each parameter tuple (parallel iteration).</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.ZipStudyParams.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the number of parameter tuples.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams","title":"pybasin.study_params.CustomStudyParams","text":"<p>               Bases: <code>StudyParams</code></p> <p>User-provided list of configurations.</p> <p>Allows full control over parameter combinations by providing explicit RunConfig objects.</p> <p>Example:</p> <pre><code>configs = [\n    RunConfig(\n        assignments=[\n            ParamAssignment(\"ode_system\", ode),\n            ParamAssignment('ode_system.params[\"K\"]', K),\n        ],\n        label={\"K\": K, \"p\": p},\n    )\n    for K, p in product(k_values, p_values)\n]\nstudy_params = CustomStudyParams(configs)\n</code></pre>"},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams-functions","title":"Functions","text":""},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams.__init__","title":"__init__","text":"<pre><code>__init__(configs: list[RunConfig]) -&gt; None\n</code></pre> <p>Initialize with a list of RunConfig objects.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>list[RunConfig]</code> <p>List of RunConfig objects defining each run.</p> required"},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[RunConfig]\n</code></pre> <p>Yield each RunConfig.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the number of configurations.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.CustomStudyParams.from_dicts","title":"from_dicts  <code>classmethod</code>","text":"<pre><code>from_dicts(\n    param_dicts: list[dict[str, Any]],\n) -&gt; CustomStudyParams\n</code></pre> <p>Create from a list of {param_name: value} dictionaries.</p> <p>Example:</p> <pre><code>study_params = CustomStudyParams.from_dicts(\n    [\n        {'ode_system.params[\"K\"]': 0.1, \"n\": 500},\n        {'ode_system.params[\"K\"]': 0.2, \"n\": 1000},\n    ]\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>param_dicts</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries where each dict maps parameter names to values for one run.</p> required <p>Returns:</p> Type Description <code>CustomStudyParams</code> <p>CustomStudyParams instance.</p>"},{"location":"api/adaptive-sampling/#configuration-types","title":"Configuration Types","text":"<p>options: heading_level: 3</p> <p>options: heading_level: 3</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.ParamAssignment","title":"pybasin.study_params.ParamAssignment  <code>dataclass</code>","text":"<p>A single parameter assignment.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The parameter path, e.g., 'ode_system.params[\"T\"]' or 'sampler'.</p> <code>value</code> <code>Any</code> <p>The value to assign to the parameter.</p>"},{"location":"api/adaptive-sampling/#pybasin.study_params.RunConfig","title":"pybasin.study_params.RunConfig  <code>dataclass</code>","text":"<p>Configuration for a single BSE run with multiple parameter assignments.</p> <p>Attributes:</p> Name Type Description <code>assignments</code> <code>list[ParamAssignment]</code> <p>List of parameter assignments to apply for this run.</p> <code>label</code> <code>dict[str, Any]</code> <p>Dictionary for results indexing, e.g., {\"T\": 0.5, \"p\": 0.2}.</p>"},{"location":"api/adaptive-sampling/#result-types","title":"Result Types","text":"<p>options: heading_level: 3</p> <p>options: heading_level: 3 heading_level: 3</p>"},{"location":"api/adaptive-sampling/#pybasin.types.AdaptiveStudyResult","title":"pybasin.types.AdaptiveStudyResult","text":"<p>               Bases: <code>TypedDict</code></p> <p>Results for a single parameter value in an adaptive parameter study.</p> <p>Contains complete information about basin stability estimation at one parameter value, including the basin stability values, error estimates, sample metadata, and optional detailed solution data.</p> <p>Attributes:</p> Name Type Description <code>param_value</code> <code>float | None</code> <p>The parameter value used for this estimation, or None if no parameter is being varied.</p> <code>basin_stability</code> <code>dict[str, float]</code> <p>Dictionary mapping attractor labels to their basin stability values (fraction of samples).</p> <code>errors</code> <code>dict[str, ErrorInfo]</code> <p>Dictionary mapping attractor labels to their ErrorInfo (absolute and relative errors).</p> <code>n_samples</code> <code>int</code> <p>Number of initial conditions actually used (may differ from requested N due to grid rounding).</p> <code>labels</code> <code>ndarray[Any, Any] | None</code> <p>Array of attractor labels for each initial condition, or None if not available.</p> <code>bifurcation_amplitudes</code> <code>Tensor | None</code> <p>Amplitude values for bifurcation analysis, or None if not computed.</p>"},{"location":"api/adaptive-sampling/#pybasin.types.ErrorInfo","title":"pybasin.types.ErrorInfo","text":"<p>               Bases: <code>TypedDict</code></p> <p>Standard error information for basin stability estimates.</p> <p>Basin stability errors are computed using Bernoulli experiment statistics:</p> <ul> <li>e_abs = sqrt(S_B(A) * (1 - S_B(A)) / N) - absolute standard error</li> <li>e_rel = 1 / sqrt(N * S_B(A)) - relative standard error</li> </ul> <p>Attributes:</p> Name Type Description <code>e_abs</code> <code>float</code> <p>Absolute standard error of the basin stability estimate.</p> <code>e_rel</code> <code>float</code> <p>Relative standard error of the basin stability estimate.</p>"},{"location":"api/basin-stability-estimator/","title":"BasinStabilityEstimator","text":"<p>Supervised Classification</p> <p>For supervised classification workflows, see the TemplateIntegrator API.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator","title":"pybasin.basin_stability_estimator.BasinStabilityEstimator","text":"<p>Core class for basin stability analysis.</p> <p>Configures the analysis with an ODE system, sampler, and solver, and provides methods to estimate basin stability and save results.</p> <p>Attributes:</p> Name Type Description <code>bs_vals</code> <code>dict[str, float] | None</code> <p>Basin stability values (fraction of samples per class).</p> <code>y0</code> <code>Tensor | None</code> <p>Initial conditions tensor.</p> <code>solution</code> <code>Solution | None</code> <p>Solution instance containing trajectory and analysis results.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator-functions","title":"Functions","text":""},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator.__init__","title":"__init__","text":"<pre><code>__init__(\n    ode_system: ODESystemProtocol,\n    sampler: Sampler,\n    n: int = 10000,\n    solver: SolverProtocol | None = None,\n    feature_extractor: FeatureExtractor | None = None,\n    predictor: BaseEstimator | None = None,\n    template_integrator: TemplateIntegrator | None = None,\n    feature_selector: BaseEstimator | None = UNSET,\n    detect_unbounded: bool = True,\n    save_to: str | None = None,\n)\n</code></pre> <p>Initialize the BasinStabilityEstimator.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of initial conditions (samples) to generate.</p> <code>10000</code> <code>ode_system</code> <code>ODESystemProtocol</code> <p>The ODE system model (ODESystem or JaxODESystem).</p> required <code>sampler</code> <code>Sampler</code> <p>The Sampler object to generate initial conditions.</p> required <code>solver</code> <code>SolverProtocol | None</code> <p>The Solver object to integrate the ODE system (Solver or JaxSolver). If None, automatically instantiates JaxSolver for JaxODESystem or TorchDiffEqSolver for ODESystem with time_span=(0, 1000), n_steps=1000, and device from sampler.</p> <code>None</code> <code>feature_extractor</code> <code>FeatureExtractor | None</code> <p>The FeatureExtractor object to extract features from trajectories. If None, defaults to TorchFeatureExtractor with minimal+dynamical features.</p> <code>None</code> <code>predictor</code> <code>BaseEstimator | None</code> <p>Any sklearn-compatible estimator (classifier or clusterer). Classifiers (<code>is_classifier(predictor)</code> is True) require a <code>template_integrator</code> for supervised learning. Clusterers (<code>is_clusterer(predictor)</code> is True) work unsupervised. Regressors are rejected. If None, defaults to <code>HDBSCANClusterer(auto_tune=True, assign_noise=True)</code>.</p> <code>None</code> <code>template_integrator</code> <code>TemplateIntegrator | None</code> <p>Template integrator for supervised classifiers. Required when <code>predictor</code> is a classifier. Holds template initial conditions, labels, and ODE params for training.</p> <code>None</code> <code>feature_selector</code> <code>BaseEstimator | None</code> <p>Feature filtering sklearn transformer with get_support() method. Defaults to DefaultFeatureSelector(). Pass None to disable filtering. Accepts any sklearn transformer (VarianceThreshold, SelectKBest, etc.) or Pipeline.</p> <code>UNSET</code> <code>detect_unbounded</code> <code>bool</code> <p>Enable unboundedness detection before feature extraction (default: True). Only activates when solver has event_fn configured (e.g., JaxSolver with event_fn). When enabled, unbounded trajectories are separated and labeled as \"unbounded\" before feature extraction to prevent imputed Inf values from contaminating features.</p> <code>True</code> <code>save_to</code> <code>str | None</code> <p>Optional file path to save results.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>predictor</code> is a regressor.</p> <code>ValueError</code> <p>If <code>predictor</code> is a classifier but no <code>template_integrator</code> is provided.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator.estimate_bs","title":"estimate_bs","text":"<pre><code>estimate_bs(\n    parallel_integration: bool = True,\n) -&gt; dict[str, float]\n</code></pre> <p>Estimate basin stability by:     1. Generating initial conditions using the sampler.     2. Integrating the ODE system for each sample (in parallel) to produce a Solution.     3. Extracting features from each Solution.     4. Clustering/classifying the feature space.     5. Computing the fraction of samples in each basin.</p> <p>This method sets:     - self.y0     - self.solution     - self.bs_vals</p> <p>Parameters:</p> Name Type Description Default <code>parallel_integration</code> <code>bool</code> <p>If True and using a supervised classifier with template integrator, run main and template integration in parallel.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary of basin stability values per class.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator.get_errors","title":"get_errors","text":"<pre><code>get_errors() -&gt; dict[str, ErrorInfo]\n</code></pre> <p>Compute absolute and relative errors for basin stability estimates.</p> <p>The errors are based on Bernoulli experiment statistics:</p> <ul> <li>e_abs = sqrt(S_B(A) * (1 - S_B(A)) / N) \u2014 absolute standard error</li> <li>e_rel = 1 / sqrt(N * S_B(A)) \u2014 relative error</li> </ul> <p>Returns:</p> Type Description <code>dict[str, ErrorInfo]</code> <p>Dictionary mapping each label to an ErrorInfo with <code>e_abs</code> and <code>e_rel</code> keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>estimate_bs()</code> has not been called yet.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Save the basin stability results to a JSON file.</p> <p>Converts numpy arrays and Solution objects to standard Python types.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>estimate_bs()</code> has not been called yet.</p> <code>ValueError</code> <p>If <code>save_to</code> path is not defined.</p>"},{"location":"api/basin-stability-estimator/#pybasin.basin_stability_estimator.BasinStabilityEstimator.save_to_excel","title":"save_to_excel","text":"<pre><code>save_to_excel() -&gt; None\n</code></pre> <p>Save the basin stability results to an Excel file.</p> <p>Includes grid samples, labels, and bifurcation amplitudes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>estimate_bs()</code> has not been called yet.</p> <code>ValueError</code> <p>If <code>save_to</code> path is not defined.</p> <code>ValueError</code> <p>If no solution data is available.</p>"},{"location":"api/feature-extractors/","title":"Feature Extractors","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor","title":"pybasin.feature_extractors.feature_extractor.FeatureExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for extracting features from ODE solutions.</p> <p>Feature extractors transform ODE solution trajectories into feature vectors that can be used for basin of attraction classification. This class provides utilities for filtering solutions by time (to remove transients).</p> <pre><code>class AmplitudeExtractor(FeatureExtractor):\n    def extract_features(self, solution: Solution) -&gt; torch.Tensor:\n        y_filtered = self.filter_time(solution)\n        return torch.max(torch.abs(y_filtered), dim=0)[0]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>time_steady</code> <code>float | None</code> <p>Time threshold for filtering transients. Only data after this time will be used for feature extraction. If <code>None</code> (default), uses 85% of the integration time span (derived from <code>solution.time</code> at extraction time). Set to <code>0.0</code> to explicitly use the entire time series.</p> <code>None</code>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor-attributes","title":"Attributes","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names: list[str]\n</code></pre> <p>Return the list of feature names.</p> <p>If not explicitly set by a subclass, automatically generates names using the pattern: _. The class name is converted to snake_case and the suffix 'FeatureExtractor' is removed (if present). <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names. Length must match the number of features (F) in the output tensor from extract_features().</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor-functions","title":"Functions","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor.extract_features","title":"extract_features  <code>abstractmethod</code>","text":"<pre><code>extract_features(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Extract features from an ODE solution.</p> <p>This method must be implemented by subclasses to define how features are computed from solution trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>Solution</code> <p>ODE solution containing time series data for one or more trajectories. The solution.y tensor has shape (N, B, S) where N is the number of time steps, B is the batch size (number of initial conditions), and S is the number of state variables.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Feature tensor of shape (B, F) where B is the batch size and F is the number of features extracted per trajectory.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.feature_extractor.FeatureExtractor.filter_time","title":"filter_time","text":"<pre><code>filter_time(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Filter out transient behavior by removing early time steps.</p> <p>Removes time steps before <code>time_steady</code> to exclude transient dynamics from feature extraction. This ensures features are computed only from steady-state or long-term behavior.</p> <p>If <code>time_steady</code> is <code>None</code>, defaults to 85% of the integration time span (i.e. keeps the last 15% of time points).</p> <pre><code>extractor = FeatureExtractor(time_steady=9.0)  # if time_span=(0, 10)\nfiltered = extractor.filter_time(solution)\n# Only time points t &gt;= 9.0 are included\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>Solution</code> <p>ODE solution with time tensor of shape (N,) and y tensor of shape (N, B, S) where N is time steps, B is batch size, and S is number of state variables.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Filtered tensor of shape (N', B, S) where N' is the number of time steps after time_steady. If time_steady is 0 or less than all time points, returns the original solution.y unchanged.</p>"},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor","title":"pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor","text":"<p>               Bases: <code>FeatureExtractor</code></p> <p>PyTorch-based feature extractor for time series features.</p> <p>Supports per-state variable feature configuration using tsfresh-style FCParameters dictionaries, allowing different feature sets for different state variables.</p> <p>For CPU extraction, uses multiprocessing to parallelize across batches. For GPU extraction, uses batched CUDA operations for optimal performance.</p> <pre><code># Default: use comprehensive features for all states on CPU\nextractor = TorchFeatureExtractor(time_steady=9.0)\n\n# GPU extraction with default features\nextractor = TorchFeatureExtractor(time_steady=9.0, device=\"gpu\")\n\n# Custom features for specific states, skip others\nextractor = TorchFeatureExtractor(\n    time_steady=9.0,\n    features=None,  # Don't extract features by default\n    features_per_state={\n        1: {\"maximum\": None, \"minimum\": None},  # Only extract for state 1\n    },\n)\n\n# Global features with per-state override\nextractor = TorchFeatureExtractor(\n    time_steady=9.0,\n    features_per_state={\n        0: {\"maximum\": None},  # Override state 0\n        1: None,  # Skip state 1\n    },\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>time_steady</code> <code>float | None</code> <p>Time threshold for filtering transients. If <code>None</code> (default), uses 85% of the integration time span. Set to <code>0.0</code> to use the entire series.</p> <code>None</code> <code>features</code> <code>Literal['comprehensive', 'minimal'] | FCParameters | None</code> <p>Default feature configuration to apply to all states. Can be:  - 'comprehensive': Use TORCH_COMPREHENSIVE_FC_PARAMETERS (default) - 'minimal': Use TORCH_MINIMAL_FC_PARAMETERS (10 basic features) - FCParameters dict: Custom feature configuration - None: Skip states not explicitly configured in features_per_state</p> <code>'comprehensive'</code> <code>features_per_state</code> <code>dict[int, FCParameters | None] | None</code> <p>Optional dict mapping state indices to FCParameters. Overrides <code>features</code> for specified states. Use None as value to skip a state. States not in this dict use the global <code>features</code> config.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to apply z-score normalization. Default True.</p> <code>True</code> <code>device</code> <code>Literal['cpu', 'gpu']</code> <p>Execution device ('cpu' or 'gpu'). Default 'cpu'.</p> <code>'cpu'</code> <code>n_jobs</code> <code>int | None</code> <p>Number of worker processes for CPU extraction. If None, uses all available CPU cores. Ignored when device='gpu'.</p> <code>None</code> <code>impute_method</code> <code>Literal['extreme', 'tsfresh']</code> <p>Method for handling NaN/inf values in features:  - 'extreme': Replace with extreme values (1e10) to distinguish unbounded trajectories. Best for systems with divergent solutions. (default) - 'tsfresh': Replace using tsfresh-style imputation (inf-&gt;max/min, NaN-&gt;median). Better when all trajectories are bounded.</p> <code>'extreme'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If device='gpu' but CUDA is not available.</p>"},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor-attributes","title":"Attributes","text":""},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names: list[str]\n</code></pre> <p>Return the list of feature names in the format 'state_X__feature_name'.</p>"},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor-functions","title":"Functions","text":""},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Extract features from an ODE solution using PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>Solution</code> <p>ODE solution containing time series data with y tensor of shape (N, B, S) where N=timesteps, B=batch size, S=state variables.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Feature tensor of shape (B, F) where F is the total number of features.</p>"},{"location":"api/feature-extractors/#pybasin.ts_torch.torch_feature_extractor.TorchFeatureExtractor.reset_scaler","title":"reset_scaler","text":"<pre><code>reset_scaler() -&gt; None\n</code></pre> <p>Reset the normalization parameters.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor","title":"pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor","text":"<p>               Bases: <code>FeatureExtractor</code></p> <p>JAX-based feature extractor for time series features.</p> <p>Supports per-state variable feature configuration using tsfresh-style FCParameters dictionaries, allowing different feature sets for different state variables.</p> <p>Warning:     Using JAX_COMPREHENSIVE_FC_PARAMETERS may cause very long JIT compile times     (~40 minutes). Use JAX_MINIMAL_FC_PARAMETERS or a custom subset for faster     compilation.</p> <pre><code># Default: use minimal features for all states\nextractor = JaxFeatureExtractor(time_steady=9.0)\n\n# Custom features for specific states, skip others\nextractor = JaxFeatureExtractor(\n    time_steady=9.0,\n    features=None,  # Don't extract features by default\n    features_per_state={\n        1: {\"log_delta\": None},  # Only extract for state 1\n    },\n)\n\n# Global features with per-state override\nextractor = JaxFeatureExtractor(\n    time_steady=9.0,\n    features_per_state={\n        0: {\"maximum\": None},  # Override state 0\n        1: None,  # Skip state 1\n    },\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>time_steady</code> <code>float | None</code> <p>Time threshold for filtering transients. If <code>None</code> (default), uses 85% of the integration time span. Set to <code>0.0</code> to use the entire series.</p> <code>None</code> <code>features</code> <code>FCParameters | None</code> <p>Default FCParameters configuration to apply to all states. Defaults to JAX_MINIMAL_FC_PARAMETERS. Set to None to skip states not explicitly configured in features_per_state.</p> <code>JAX_MINIMAL_FC_PARAMETERS</code> <code>features_per_state</code> <code>dict[int, FCParameters | None] | None</code> <p>Optional dict mapping state indices to FCParameters. Overrides <code>features</code> for specified states. Use None as value to skip a state. States not in this dict use the global <code>features</code> config.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to apply z-score normalization. Default True.</p> <code>True</code> <code>use_jit</code> <code>bool</code> <p>Whether to JIT-compile extraction. Default True.</p> <code>True</code> <code>device</code> <code>str | None</code> <p>JAX device to use ('cpu', 'gpu', 'cuda', 'cuda:N', or None for auto).</p> <code>None</code> <code>impute_method</code> <code>Literal['extreme', 'tsfresh']</code> <p>Method for handling NaN/inf values in features. Options:  - 'extreme': Replace with extreme values (1e10) to distinguish unbounded trajectories. Best for systems with divergent solutions. (default)  - 'tsfresh': Replace using tsfresh-style imputation (inf-&gt;max/min, NaN-&gt;median). Better when all trajectories are bounded.</p> <code>'extreme'</code>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor-attributes","title":"Attributes","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names: list[str]\n</code></pre> <p>Return the list of feature names in the format 'state_X__feature_name'.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor-functions","title":"Functions","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Extract features from an ODE solution using JAX.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.jax.jax_feature_extractor.JaxFeatureExtractor.reset_scaler","title":"reset_scaler","text":"<pre><code>reset_scaler() -&gt; None\n</code></pre> <p>Reset the normalization parameters.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor","title":"pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor","text":"<p>               Bases: <code>FeatureExtractor</code></p> <p>Feature extractor using tsfresh for comprehensive time series analysis.</p> <p>This extractor uses the tsfresh library to automatically extract a large number of time series features from ODE solutions. It converts PyTorch/JAX tensors to pandas DataFrames for tsfresh processing, then converts the results back to tensors.</p> <p>Supports per-state variable feature configuration using tsfresh's kind_to_fc_parameters mechanism, allowing you to apply different feature sets to different state variables based on domain knowledge.</p> <p>Internally, the solution tensor is converted to tsfresh's wide/flat DataFrame format where each state variable becomes a column (<code>state_0</code>, <code>state_1</code>, etc.). The <code>kind_to_fc_parameters</code> accepts integer state indices and maps them to these column names automatically.</p> <pre><code># Same minimal features for all states\nextractor = TsfreshFeatureExtractor(\n    time_steady=9.0, default_fc_parameters=MinimalFCParameters(), n_jobs=-1, normalize=True\n)\n\n# Specific features for all states\nextractor = TsfreshFeatureExtractor(\n    time_steady=950.0,\n    default_fc_parameters={\"mean\": None, \"std\": None, \"maximum\": None},\n    n_jobs=-1,\n)\n\n# Different features per state (e.g., pendulum: position vs velocity)\nfrom tsfresh.feature_extraction import MinimalFCParameters, ComprehensiveFCParameters\n\nextractor = TsfreshFeatureExtractor(\n    time_steady=950.0,\n    kind_to_fc_parameters={\n        0: {\"mean\": None, \"maximum\": None, \"minimum\": None},\n        1: ComprehensiveFCParameters(),\n    },\n    n_jobs=1,  # Use n_jobs=1 for deterministic results\n)\n</code></pre> <p>Note on parallelism:     Setting n_jobs &gt; 1 enables parallel feature extraction but introduces     non-determinism due to floating-point arithmetic order. This can cause     inconsistent classification results. Use n_jobs=1 for reproducible results.</p> <p>Note on normalization:     When normalize=True, the scaler is fitted on the FIRST dataset that calls     extract_features(). For best results with supervised classifiers:     - Either set normalize=False (recommended for KNN with few templates)     - Or call fit_scaler() explicitly with representative data before extraction</p> <p>Parameters:</p> Name Type Description Default <code>time_steady</code> <code>float | None</code> <p>Time threshold for filtering transients. If <code>None</code> (default), uses 85% of the integration time span. Set to <code>0.0</code> to use the entire series.</p> <code>None</code> <code>default_fc_parameters</code> <code>dict[str, Any] | Any | None</code> <p>Default feature extraction parameters for all states. Can be one of: - MinimalFCParameters() - Fast extraction with ~20 features - ComprehensiveFCParameters() - Full extraction with ~800 features - Custom dict like {\"mean\": None, \"maximum\": None} for specific features - None - must provide kind_to_fc_parameters Default is MinimalFCParameters().</p> <code>None</code> <code>kind_to_fc_parameters</code> <code>dict[int, dict[str, Any] | Any] | None</code> <p>Optional dict mapping state indices (e.g. <code>0</code>, <code>1</code>) to FCParameters. Indices correspond to the state dimension of the solution tensor. If provided, overrides <code>default_fc_parameters</code> for those states.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for feature extraction. Default is 1. Set to -1 to use all available cores.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>Whether to apply StandardScaler normalization to features. Highly recommended for distance-based classifiers like KNN. Default is True.</p> <code>True</code>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor-attributes","title":"Attributes","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names: list[str]\n</code></pre> <p>Return the list of feature names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of feature names from tsfresh extraction.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If extract_features has not been called yet.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor-functions","title":"Functions","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor.reset_scaler","title":"reset_scaler","text":"<pre><code>reset_scaler() -&gt; None\n</code></pre> <p>Reset the scaler to unfitted state.</p> <p>Call this if you need to refit the scaler on different data.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.tsfresh_feature_extractor.TsfreshFeatureExtractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Extract features from an ODE solution using tsfresh.</p> <p>Converts the solution tensor to pandas DataFrame format expected by tsfresh, extracts features for each trajectory and state variable, then converts back to PyTorch tensor.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>Solution</code> <p>ODE solution with y tensor of shape (N, B, S) where N is time steps, B is batch size, and S is number of state variables.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Feature tensor of shape (B, F) where B is the batch size and F is the total number of features extracted by tsfresh across all state variables.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor","title":"pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor","text":"<p>               Bases: <code>FeatureExtractor</code></p> <p>Feature extractor using nolds for nonlinear dynamics analysis.</p> <p>Computes nonlinear dynamics features for each trajectory with multiprocessing parallelization. Uses tsfresh-style FCParameters configuration for specifying which features to extract and with what parameters.</p> <p>Available features (passed directly to nolds):     * <code>lyap_r</code>: Largest Lyapunov exponent (Rosenstein's algorithm)     * <code>lyap_e</code>: Largest Lyapunov exponent (Eckmann's algorithm)     * <code>sampen</code>: Sample entropy     * <code>hurst_rs</code>: Hurst exponent (R/S analysis)     * <code>corr_dim</code>: Correlation dimension     * <code>dfa</code>: Detrended fluctuation analysis     * <code>mfhurst_b</code>: Multifractal Hurst exponent (basic method)     * <code>mfhurst_dm</code>: Multifractal Hurst exponent (DM method)</p> <pre><code># Default: extract lyap_r and corr_dim from all states\nextractor = NoldsFeatureExtractor(time_steady=9.0)\n\n# Only extract Lyapunov exponents with custom parameters\nextractor = NoldsFeatureExtractor(\n    time_steady=9.0,\n    features={\"lyap_r\": [{\"emb_dim\": 15}]},\n)\n\n# Per-state configuration\nextractor = NoldsFeatureExtractor(\n    time_steady=9.0,\n    features=None,\n    features_per_state={\n        0: {\"lyap_r\": None},\n        1: {\"corr_dim\": [{\"emb_dim\": 10}]},\n    },\n)\n\n# Multiple parameter sets for same feature\nextractor = NoldsFeatureExtractor(\n    time_steady=9.0,\n    features={\n        \"lyap_r\": [\n            {\"emb_dim\": 5},\n            {\"emb_dim\": 10},\n        ],\n    },\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>time_steady</code> <code>float</code> <p>Time threshold for filtering transients. Default 0.0.</p> <code>0.0</code> <code>features</code> <code>NoldsFCParameters | None</code> <p>Feature configuration for all states. Can be: * NoldsFCParameters dict: Feature names mapped to parameter lists * None: Skip states not in features_per_state Default extracts both lyap_r and corr_dim with nolds defaults.</p> <code>None</code> <code>features_per_state</code> <code>dict[int, NoldsFCParameters | None] | None</code> <p>Optional dict mapping state indices to FCParameters. Overrides <code>features</code> for specified states. Use None to skip a state.</p> <code>None</code> <code>n_jobs</code> <code>int | None</code> <p>Number of worker processes. If None, uses all CPU cores.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If nolds library is not installed.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor-attributes","title":"Attributes","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor.feature_names","title":"feature_names  <code>property</code>","text":"<pre><code>feature_names: list[str]\n</code></pre> <p>Return the list of feature names in format 'state_X__feature__params'.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If extract_features has not been called yet.</p>"},{"location":"api/feature-extractors/#pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor-functions","title":"Functions","text":""},{"location":"api/feature-extractors/#pybasin.feature_extractors.nolds_feature_extractor.NoldsFeatureExtractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(solution: Solution) -&gt; torch.Tensor\n</code></pre> <p>Extract nolds features from an ODE solution.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>Solution</code> <p>ODE solution with shape (N, B, S).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Features tensor of shape (B, F) where F depends on configuration.</p>"},{"location":"api/feature-selector/","title":"Feature Selector","text":""},{"location":"api/feature-selector/#pybasin.feature_selector.default_feature_selector.DefaultFeatureSelector","title":"pybasin.feature_selector.default_feature_selector.DefaultFeatureSelector","text":"<p>               Bases: <code>Pipeline</code></p> <p>Feature selector combining variance threshold and correlation filtering.</p> <p>This class extends sklearn's Pipeline with two steps:</p> <ol> <li>VarianceThreshold: Removes features with variance below threshold</li> <li>CorrelationSelector: Removes highly correlated features (|corr| &gt; threshold)</li> </ol> <p>The correlation threshold uses absolute correlation values, meaning both positive and negative correlations above the threshold will trigger removal.</p> <p>As a Pipeline subclass, this implements the full sklearn transformer API: fit(), transform(), fit_transform(), get_params(), set_params(), etc.</p> <pre><code>selector = DefaultFeatureSelector(variance_threshold=0.01, correlation_threshold=0.95)\nfeatures_filtered = selector.fit_transform(features)\n</code></pre> <p>Attributes:</p> Name Type Description <code>variance_threshold</code> <code>float</code> <p>Minimum variance required to keep a feature.</p> <code>correlation_threshold</code> <code>float</code> <p>Maximum absolute correlation allowed between features. Features with |correlation| &gt; threshold will be removed.</p> <code>min_features</code> <code>int</code> <p>Minimum number of features to keep.</p>"},{"location":"api/feature-selector/#pybasin.feature_selector.default_feature_selector.DefaultFeatureSelector-functions","title":"Functions","text":""},{"location":"api/feature-selector/#pybasin.feature_selector.default_feature_selector.DefaultFeatureSelector.get_support","title":"get_support","text":"<pre><code>get_support(indices: bool = False) -&gt; np.ndarray\n</code></pre> <p>Get mask or indices of features that passed the filter.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>bool</code> <p>If True, returns indices. If False, returns boolean mask.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask or integer indices of selected features.</p>"},{"location":"api/feature-selector/#pybasin.feature_selector.correlation_selector.CorrelationSelector","title":"pybasin.feature_selector.correlation_selector.CorrelationSelector","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Scikit-learn transformer to remove highly correlated features.</p> <p>This transformer removes features with high pairwise correlations, keeping only one feature from each correlated group.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <code>float</code> <p>Correlation threshold. Features with absolute correlation above this value will be considered redundant.</p> <code>min_features</code> <code>int</code> <p>Minimum number of features to keep. If removing correlated features would result in fewer than this many, some correlated features are retained.</p> <code>support_</code> <p>Boolean mask of selected features.</p> <code>n_features_in_</code> <p>Number of input features.</p>"},{"location":"api/feature-selector/#pybasin.feature_selector.correlation_selector.CorrelationSelector-functions","title":"Functions","text":""},{"location":"api/feature-selector/#pybasin.feature_selector.correlation_selector.CorrelationSelector.fit","title":"fit","text":"<pre><code>fit(X: ndarray, y: ndarray | None = None)\n</code></pre> <p>Compute which features to keep.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | None</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Type Description <p>Fitted transformer.</p>"},{"location":"api/feature-selector/#pybasin.feature_selector.correlation_selector.CorrelationSelector.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Remove correlated features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Data with correlated features removed, shape (n_samples, n_features_out).</p>"},{"location":"api/feature-selector/#pybasin.feature_selector.correlation_selector.CorrelationSelector.get_support","title":"get_support","text":"<pre><code>get_support(indices: bool = False)\n</code></pre> <p>Get a mask or indices of selected features.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>bool</code> <p>If True, return feature indices. Otherwise, return boolean mask.</p> <code>False</code> <p>Returns:</p> Type Description <p>Boolean mask or integer indices of selected features.</p>"},{"location":"api/plotters/","title":"Plotters","text":""},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter","title":"pybasin.plotters.matplotlib_plotter.MatplotlibPlotter","text":""},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter-functions","title":"Functions","text":""},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.__init__","title":"__init__","text":"<pre><code>__init__(bse: BasinStabilityEstimator)\n</code></pre> <p>Initialize the Plotter with a BasinStabilityEstimator instance.</p> <p>Parameters:</p> Name Type Description Default <code>bse</code> <code>BasinStabilityEstimator</code> <p>An instance of BasinStabilityEstimator.</p> required"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_basin_stability_bars","title":"plot_basin_stability_bars","text":"<pre><code>plot_basin_stability_bars(ax: Axes | None = None)\n</code></pre> <p>Plot basin stability values as a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes | None</code> <p>Matplotlib axes to plot on. If None, creates a new figure.</p> <code>None</code>"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_state_space","title":"plot_state_space","text":"<pre><code>plot_state_space(ax: Axes | None = None)\n</code></pre> <p>Plot initial conditions in state space, colored by their attractor labels.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes | None</code> <p>Matplotlib axes to plot on. If None, creates a new figure.</p> <code>None</code>"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_feature_space","title":"plot_feature_space","text":"<pre><code>plot_feature_space(ax: Axes | None = None)\n</code></pre> <p>Plot feature space with classifier results.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes | None</code> <p>Matplotlib axes to plot on. If None, creates a new figure.</p> <code>None</code>"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_bse_results","title":"plot_bse_results","text":"<pre><code>plot_bse_results()\n</code></pre> <p>Generate diagnostic plots using the data stored in self.solution:     1. A bar plot of basin stability values.     2. A scatter plot of initial conditions (state space).     3. A scatter plot of the feature space with classifier results.     4. A placeholder plot for future use.</p> <p>This method combines the individual plotting functions into a 2x2 grid. For individual plots, use plot_basin_stability_bars(), plot_state_space(), or plot_feature_space() directly.</p>"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_templates_phase_space","title":"plot_templates_phase_space","text":"<pre><code>plot_templates_phase_space(\n    x_var: int = 0,\n    y_var: int = 1,\n    z_var: int | None = None,\n    time_range: tuple[float, float] = (700, 1000),\n) -&gt; Figure\n</code></pre> <p>Plot trajectories for the template initial conditions in 2D or 3D phase space.</p> <p>Creates a CPU copy of the solver with 10x the original n_steps for smoother visualization. Caching is disabled to avoid polluting the cache with visualization-specific integrations.</p> <p>Parameters:</p> Name Type Description Default <code>x_var</code> <code>int</code> <p>State variable index for x-axis.</p> <code>0</code> <code>y_var</code> <code>int</code> <p>State variable index for y-axis.</p> <code>1</code> <code>z_var</code> <code>int | None</code> <p>State variable index for z-axis (3D plot if provided).</p> <code>None</code> <code>time_range</code> <code>tuple[float, float]</code> <p>Time range (t_start, t_end) to plot.</p> <code>(700, 1000)</code>"},{"location":"api/plotters/#pybasin.plotters.matplotlib_plotter.MatplotlibPlotter.plot_templates_trajectories","title":"plot_templates_trajectories","text":"<pre><code>plot_templates_trajectories(\n    plotted_var: int,\n    y_limits: tuple[float, float]\n    | dict[str, tuple[float, float]]\n    | None = None,\n    x_limits: tuple[float, float]\n    | dict[str, tuple[float, float]]\n    | None = None,\n) -&gt; Figure\n</code></pre> <p>Plot template trajectories as vertically stacked subplots.</p> <p>Each trajectory gets its own row with independent y-axis scaling. This handles different amplitudes across attractors cleanly.</p> <p>Creates a CPU copy of the solver with 10x the original n_steps for smoother visualization. Caching is disabled to avoid polluting the cache with visualization-specific integrations.</p> <p>Parameters:</p> Name Type Description Default <code>plotted_var</code> <code>int</code> <p>Index of the state variable to plot.</p> required <code>y_limits</code> <code>tuple[float, float] | dict[str, tuple[float, float]] | None</code> <p>Y-axis limits. Tuple applies to all, dict maps label to (y_min, y_max).</p> <code>None</code> <code>x_limits</code> <code>tuple[float, float] | dict[str, tuple[float, float]] | None</code> <p>X-axis limits. Tuple applies to all, dict maps label to (x_min, x_max).</p> <code>None</code>"},{"location":"api/plotters/#pybasin.plotters.interactive_plotter.plotter.InteractivePlotter","title":"pybasin.plotters.interactive_plotter.plotter.InteractivePlotter","text":"<p>Interactive web-based plotter for basin stability visualization.</p> <p>Uses Dash with Mantine components for a modern UI and Plotly for interactive visualizations. Each page owns its controls, plot, and callbacks.</p> <p>Attributes:</p> Name Type Description <code>bse</code> <code>BasinStabilityEstimator</code> <p>BasinStabilityEstimator instance with computed results.</p> <code>state_labels</code> <p>Optional mapping of state indices to custom labels.</p> <code>app</code> <code>Dash | None</code> <p>Dash application instance.</p>"},{"location":"api/plotters/#pybasin.plotters.interactive_plotter.plotter.InteractivePlotter-functions","title":"Functions","text":""},{"location":"api/plotters/#pybasin.plotters.interactive_plotter.plotter.InteractivePlotter.__init__","title":"__init__","text":"<pre><code>__init__(\n    bse: BasinStabilityEstimator | BasinStabilityStudy,\n    state_labels: dict[int, str] | None = None,\n    options: InteractivePlotterOptions | None = None,\n)\n</code></pre> <p>Initialize the Plotter.</p> <p>Parameters:</p> Name Type Description Default <code>bse</code> <code>BasinStabilityEstimator | BasinStabilityStudy</code> <p>BasinStabilityEstimator or BasinStabilityStudy instance.</p> required <code>state_labels</code> <code>dict[int, str] | None</code> <p>Optional dict mapping state indices to labels, e.g., {0: \"\u03b8\", 1: \"\u03c9\"} for a pendulum system.</p> <code>None</code> <code>options</code> <code>InteractivePlotterOptions | None</code> <p>Optional configuration for default control values.</p> <code>None</code>"},{"location":"api/plotters/#pybasin.plotters.interactive_plotter.plotter.InteractivePlotter.run","title":"run","text":"<pre><code>run(port: int = 8050, debug: bool = False) -&gt; None\n</code></pre> <p>Launch the interactive plotter as a standalone Dash server.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port to run the server on (default: 8050).</p> <code>8050</code> <code>debug</code> <code>bool</code> <p>Enable Dash debug mode (default: False).</p> <code>False</code>"},{"location":"api/plotters/#pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter","title":"pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter","text":"<p>Matplotlib-based plotter for adaptive study basin stability results.</p> <p>Attributes:</p> Name Type Description <code>as_bse</code> <p>BasinStabilityStudy instance with computed results.</p>"},{"location":"api/plotters/#pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter-functions","title":"Functions","text":""},{"location":"api/plotters/#pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter.__init__","title":"__init__","text":"<pre><code>__init__(as_bse: BasinStabilityStudy)\n</code></pre> <p>Initialize the plotter with an BasinStabilityStudy instance.</p> <p>Parameters:</p> Name Type Description Default <code>as_bse</code> <code>BasinStabilityStudy</code> <p>An instance of BasinStabilityStudy.</p> required"},{"location":"api/plotters/#pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter.plot_basin_stability_variation","title":"plot_basin_stability_variation","text":"<pre><code>plot_basin_stability_variation(\n    interval: Literal[\"linear\", \"log\"] = \"linear\",\n    show: bool = True,\n)\n</code></pre> <p>Plot all basin stability values against parameter variation in a single plot.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Literal['linear', 'log']</code> <p>Indicates whether the x-axis should use a linear or logarithmic scale. Options:  - 'linear': Default linear scale. - 'log': Logarithmic scale, e.g., when using <code>2 * np.logspace(...)</code>.</p> <code>'linear'</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. If False, returns the figure without showing.</p> <code>True</code> <p>Returns:</p> Type Description <p>The matplotlib Figure object.</p>"},{"location":"api/plotters/#pybasin.matplotlib_study_plotter.MatplotlibStudyPlotter.plot_bifurcation_diagram","title":"plot_bifurcation_diagram","text":"<pre><code>plot_bifurcation_diagram(dof: list[int], show: bool = True)\n</code></pre> <p>Plot bifurcation diagram showing attractor locations over parameter variation.</p> <p>For each parameter value, the method extracts the bifurcation amplitudes (i.e. solution.bifurcation_amplitudes), selects the desired DOFs, applies k-means clustering and then plots the cluster centers as a function of the parameter.</p> <p>Unbounded trajectories are automatically filtered out before clustering, as bifurcation amplitudes are only computed for bounded trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>dof</code> <code>list[int]</code> <p>List of indices of the state variables (DOFs) to plot.</p> required <code>show</code> <code>bool</code> <p>Whether to display the plot. If False, returns the figure without showing.</p> <code>True</code> <p>Returns:</p> Type Description <p>The matplotlib Figure object.</p>"},{"location":"api/predictors/","title":"Predictors","text":""},{"location":"api/predictors/#pybasin.predictors.hdbscan_clusterer.HDBSCANClusterer","title":"pybasin.predictors.hdbscan_clusterer.HDBSCANClusterer","text":"<p>               Bases: <code>DisplayNameMixin</code>, <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>HDBSCAN clustering for basin stability analysis with optional auto-tuning and noise assignment (unsupervised learning).</p>"},{"location":"api/predictors/#pybasin.predictors.hdbscan_clusterer.HDBSCANClusterer-functions","title":"Functions","text":""},{"location":"api/predictors/#pybasin.predictors.hdbscan_clusterer.HDBSCANClusterer.__init__","title":"__init__","text":"<pre><code>__init__(\n    hdbscan: Any = None,\n    assign_noise: bool = False,\n    nearest_neighbors: NearestNeighbors | None = None,\n    auto_tune: bool = False,\n)\n</code></pre> <p>Initialize HDBSCAN clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>hdbscan</code> <code>Any</code> <p>A configured <code>sklearn.cluster.HDBSCAN</code> instance, or <code>None</code> to create a default one (<code>min_cluster_size=50</code>, <code>min_samples=10</code>).</p> <code>None</code> <code>assign_noise</code> <code>bool</code> <p>Whether to assign noise points to nearest clusters using KNN.</p> <code>False</code> <code>nearest_neighbors</code> <code>NearestNeighbors | None</code> <p>A configured <code>sklearn.neighbors.NearestNeighbors</code> instance for noise assignment, or <code>None</code> to create a default one (<code>n_neighbors=5</code>). Only used when <code>assign_noise=True</code>.</p> <code>None</code> <code>auto_tune</code> <code>bool</code> <p>Whether to automatically tune <code>min_cluster_size</code> using silhouette score. The tuned value overrides the one on the <code>hdbscan</code> instance.</p> <code>False</code>"},{"location":"api/predictors/#pybasin.predictors.hdbscan_clusterer.HDBSCANClusterer.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray, y: Any = None) -&gt; np.ndarray\n</code></pre> <p>Fit and predict labels using HDBSCAN clustering with optional noise assignment.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature array to cluster.</p> required <code>y</code> <code>Any</code> <p>Ignored (present for sklearn API compatibility).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Cluster labels.</p>"},{"location":"api/predictors/#pybasin.predictors.dbscan_clusterer.DBSCANClusterer","title":"pybasin.predictors.dbscan_clusterer.DBSCANClusterer","text":"<p>               Bases: <code>DisplayNameMixin</code>, <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>DBSCAN clustering for basin stability analysis with optional epsilon auto-tuning (unsupervised learning).</p> <p>When <code>auto_tune=True</code>, replicates the epsilon search from the MATLAB bSTAB <code>classify_solution.m</code> unsupervised branch:</p> <ol> <li>Precompute the pairwise Euclidean distance matrix.</li> <li>Build an epsilon grid from the feature ranges.</li> <li>For each candidate epsilon, run DBSCAN and record the minimum    per-sample silhouette score (worst-case cluster quality).</li> <li>Find the most prominent peak in the silhouette curve above a    height threshold.</li> <li>Fall back to the global maximum if no peak is found.</li> </ol>"},{"location":"api/predictors/#pybasin.predictors.dbscan_clusterer.DBSCANClusterer-functions","title":"Functions","text":""},{"location":"api/predictors/#pybasin.predictors.dbscan_clusterer.DBSCANClusterer.__init__","title":"__init__","text":"<pre><code>__init__(\n    dbscan: DBSCAN | None = None,\n    auto_tune: bool = False,\n    n_eps_grid: int = 200,\n    tune_sample_size: int = 2000,\n    min_peak_height: float = 0.9,\n    assign_noise: bool = False,\n    nearest_neighbors: NearestNeighbors | None = None,\n)\n</code></pre> <p>Initialize DBSCAN clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>dbscan</code> <code>DBSCAN | None</code> <p>A configured <code>sklearn.cluster.DBSCAN</code> instance, or <code>None</code> to create a default one (<code>eps=0.5</code>, <code>min_samples=10</code>). When <code>auto_tune=True</code>, the tuned epsilon overrides <code>dbscan.eps</code>.</p> <code>None</code> <code>auto_tune</code> <code>bool</code> <p>Whether to automatically find the optimal epsilon using silhouette-based peak analysis (MATLAB bSTAB algorithm).</p> <code>False</code> <code>n_eps_grid</code> <code>int</code> <p>Number of epsilon candidates to evaluate during auto-tuning.</p> <code>200</code> <code>tune_sample_size</code> <code>int</code> <p>Maximum number of samples to use during the epsilon search. If the dataset is larger, a random subsample is drawn to keep the search fast.</p> <code>2000</code> <code>min_peak_height</code> <code>float</code> <p>Minimum silhouette peak height for the peak finder during auto-tuning.</p> <code>0.9</code> <code>assign_noise</code> <code>bool</code> <p>Whether to assign noise points (-1) to the nearest cluster using KNN.</p> <code>False</code> <code>nearest_neighbors</code> <code>NearestNeighbors | None</code> <p>A configured <code>sklearn.neighbors.NearestNeighbors</code> instance for noise assignment, or <code>None</code> to create a default one (<code>n_neighbors=5</code>). Only used when <code>assign_noise=True</code>.</p> <code>None</code>"},{"location":"api/predictors/#pybasin.predictors.dbscan_clusterer.DBSCANClusterer.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray, y: Any = None) -&gt; np.ndarray\n</code></pre> <p>Fit and predict labels using DBSCAN clustering.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature array of shape <code>(n_samples, n_features)</code>.</p> required <code>y</code> <code>Any</code> <p>Ignored (present for sklearn API compatibility).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Cluster labels (<code>-1</code> for noise unless <code>assign_noise=True</code>).</p>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer","title":"pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer","text":"<p>               Bases: <code>DisplayNameMixin</code>, <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>Two-stage hierarchical clustering for dynamical systems.</p> <p>This clusterer uses physics-based heuristics to classify trajectories into attractor types (Stage 1) and then sub-classifies within each type (Stage 2).</p>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer--stage-1-attractor-type-classification","title":"Stage 1: Attractor Type Classification","text":"<p>Fixed Point (FP) Detection:     Heuristic: variance &lt; fp_variance_threshold</p> <pre><code>A trajectory is classified as converging to a fixed point if the variance\nof its steady-state values is extremely low. The threshold should be set\nbased on the expected numerical precision of your integration.\n\nIMPORTANT: If features are normalized/scaled (e.g., StandardScaler), the\nvariance values will be transformed. For normalized features with unit\nvariance, use a threshold relative to 1.0 (e.g., 1e-4). For unnormalized\nfeatures, use absolute thresholds based on your system's scale.\n</code></pre> <p>Limit Cycle (LC) Detection:     Heuristic: (periodicity_strength &gt; lc_periodicity_threshold AND                variance &lt; chaos_variance_threshold) OR has_drift</p> <pre><code>A trajectory is classified as a limit cycle if:\n1. It shows strong periodic behavior (high autocorrelation periodicity)\n   AND has bounded variance (not chaotic), OR\n2. It shows monotonic drift (rotating solutions like pendulum rotations)\n\nThe periodicity_strength comes from autocorrelation analysis and ranges\nfrom 0 (no periodicity) to 1 (perfect periodicity). Values above 0.5\ntypically indicate clear periodic behavior.\n</code></pre> <p>Chaos Detection:     Heuristic: NOT FP AND NOT LC (default fallback)</p> <pre><code>Trajectories that don't meet FP or LC criteria are classified as chaotic.\nHigh variance combined with low periodicity strength indicates chaos.\n</code></pre>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer--stage-2-sub-classification","title":"Stage 2: Sub-classification","text":"<p>Within each attractor type, trajectories are further clustered: - FP: Clustered by steady-state location (mean values) - LC: Hierarchically clustered by period number, then amplitude/mean - Chaos: Clustered by spatial mean location</p>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer--required-features","title":"Required Features","text":"<p>Feature names must follow the convention: state_X__feature_name</p> <p>Required base features:     - variance: Steady-state variance (FP detection)     - amplitude: Peak-to-peak amplitude (LC sub-classification)     - mean: Steady-state mean (FP/chaos sub-classification)     - linear_trend__attr_slope: Linear drift rate (rotating LC detection)     - autocorrelation_periodicity__output_strength: Periodicity measure [0-1]     - autocorrelation_periodicity__output_period: Detected period     - spectral_frequency_ratio: Ratio for period-n detection</p> <p>Note: This clusterer requires feature names to be set via set_feature_names() before calling fit_predict(). The BasinStabilityEstimator handles this automatically during the estimation process.</p>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer-functions","title":"Functions","text":""},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer.__init__","title":"__init__","text":"<pre><code>__init__(\n    drift_threshold: float = 0.1,\n    drift_fraction: float = 0.3,\n    tiers: list[str] | None = None,\n    fp_variance_threshold: float = 1e-06,\n    fp_sub_classifier: Any = None,\n    lc_periodicity_threshold: float = 0.5,\n    lc_sub_classifier: Any = None,\n    chaos_variance_threshold: float = 5.0,\n    chaos_sub_classifier: Any = None,\n)\n</code></pre> <p>Initialize the dynamical system clusterer.</p> <p>Parameters:</p> Name Type Description Default <code>drift_threshold</code> <code>float</code> <p>Minimum |slope| to consider a dimension as drifting. Drifting dimensions (e.g., pendulum angle during rotation) are excluded from variance/mean calculations for FP and chaos sub-classification to avoid spurious splits. Also used to detect rotating limit cycles. Units: [state_units / time_units]. Default: 0.1.</p> <code>0.1</code> <code>drift_fraction</code> <code>float</code> <p>Minimum fraction of trajectories with |slope| &gt; drift_threshold for a dimension to be flagged as drifting. Default: 0.3 (i.e., 30% of trajectories must show drift).</p> <code>0.3</code> <code>tiers</code> <code>list[str] | None</code> <p>List of attractor types to detect, in priority order. First matching tier wins. Options: \"FP\", \"LC\", \"chaos\". Default: [\"FP\", \"LC\", \"chaos\"].</p> <code>None</code> <code>fp_variance_threshold</code> <code>float</code> <p>Maximum variance to classify as fixed point. For unnormalized features, set based on expected steady-state fluctuations (e.g., 1e-6 for well-converged integrations). For normalized features (unit variance), use relative threshold (e.g., 1e-4 meaning 0.01% of typical variance). Default: 1e-6.</p> <code>1e-06</code> <code>fp_sub_classifier</code> <code>Any</code> <p>Custom sub-classifier for fixed points. Input: mean values per non-drifting dimension. Default: HDBSCAN with min_cluster_size=50.</p> <code>None</code> <code>lc_periodicity_threshold</code> <code>float</code> <p>Minimum periodicity strength [0-1] to classify as limit cycle. The periodicity strength measures how well the autocorrelation matches periodic behavior (0.0 = no periodic pattern, 0.3-0.5 = weak/noisy, 0.5-0.8 = clear periodic, 0.8-1.0 = strong/clean limit cycle). Default: 0.5.</p> <code>0.5</code> <code>lc_sub_classifier</code> <code>Any</code> <p>Custom sub-classifier for limit cycles. Input: [freq_ratio, amplitude, mean] features. Default: Hierarchical period-based clustering.</p> <code>None</code> <code>chaos_variance_threshold</code> <code>float</code> <p>Maximum variance for limit cycle. Trajectories with variance above this AND low periodicity are classified as chaotic. Set based on expected LC amplitude range. For normalized features, typical LC variance is ~0.5-2.0. Default: 5.0.</p> <code>5.0</code> <code>chaos_sub_classifier</code> <code>Any</code> <p>Custom sub-classifier for chaotic attractors. Input: mean values per dimension. Default: HDBSCAN with auto_tune=True.</p> <code>None</code>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer.needs_feature_names","title":"needs_feature_names","text":"<pre><code>needs_feature_names() -&gt; bool\n</code></pre> <p>This clusterer requires feature names to parse physics-based features.</p>"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer.set_feature_names","title":"set_feature_names","text":"<pre><code>set_feature_names(feature_names: list[str]) -&gt; None\n</code></pre> <p>Set feature names and build feature indices.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list[str]</code> <p>List of feature names matching the feature array columns.</p> required"},{"location":"api/predictors/#pybasin.predictors.dynamical_system_clusterer.DynamicalSystemClusterer.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X: ndarray, y: Any = None) -&gt; np.ndarray\n</code></pre> <p>Predict labels using two-stage hierarchical clustering.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature array of shape (n_samples, n_features).</p> required <code>y</code> <code>Any</code> <p>Ignored (present for sklearn API compatibility).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of predicted labels with format \"TYPE_subcluster\".</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If set_feature_names() was not called before prediction.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator","title":"pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator","text":"<p>               Bases: <code>DisplayNameMixin</code>, <code>MetaEstimatorMixin</code>, <code>BaseEstimator</code></p> <p>Meta-estimator for separately labeling unbounded trajectories.</p> <p>This meta-estimator wraps another estimator (classifier or clusterer) and handles unbounded trajectories separately. Unbounded trajectories are identified using a detector function and assigned a special label, while bounded trajectories are processed using the wrapped estimator.</p> <p>The API adapts to the wrapped estimator type (similar to sklearn.pipeline.Pipeline): - If estimator is a clusterer: provides fit(), fit_predict(), predict() - If estimator is a classifier: provides fit(), predict(), and potentially predict_proba()</p> <p>This is particularly useful in basin stability calculations where some trajectories may diverge to infinity (e.g., in the Lorenz system).</p> <pre><code>from pybasin.predictors import UnboundednessMetaEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nX, _ = make_blobs(n_samples=100, n_features=10, centers=3, random_state=42)\n# Add some \"unbounded\" samples with extreme values\nX[0, :] = 1e10\nX[1, :] = -1e10\nclf = UnboundednessMetaEstimator(KMeans(n_clusters=3, random_state=42))\nclf.fit(X)\nlabels = clf.predict(X)\nprint(f\"Unbounded samples: {np.sum(labels == 'unbounded')}\")\n</code></pre> <p>Notes:</p> <ul> <li>Only bounded samples are used to fit the base estimator</li> <li>The unbounded label is automatically tracked</li> <li>If all samples are unbounded, the estimator will only predict the unbounded label</li> <li>The estimator type validation ensures only classifiers or clusterers are accepted</li> </ul> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Any</code> <p>The base estimator to use for bounded trajectories. Must be a classifier or clusterer implementing <code>fit</code> and <code>predict</code> methods (or <code>fit_predict</code> for clustering).</p> required <code>unbounded_detector</code> <code>Callable[[ndarray], ndarray] | None</code> <p>Function to detect unbounded trajectories. Should take a feature array of shape (n_samples, n_features) and return a boolean array of shape (n_samples,) where True indicates unbounded. If None, uses the default detector which identifies:  - Trajectories with Inf/-Inf values (from JAX solver) - Trajectories with values at \u00b11e10 (from torch feature extractor)</p> <code>None</code> <code>unbounded_label</code> <code>int | str</code> <p>Label to assign to unbounded trajectories.</p> <code>'unbounded'</code> <p>Attributes:</p> Name Type Description <code>estimator_</code> <code>estimator object</code> <p>The fitted base estimator (only fitted on bounded samples).</p> <code>classes_</code> <code>ndarray of shape (n_classes,)</code> <p>The classes labels (only for classifiers), including the unbounded label.</p> <code>labels_</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each sample from the last fit operation (only for clusterers).</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during fit.</p> <code>bounded_mask_</code> <code>ndarray of shape (n_samples,)</code> <p>Boolean mask indicating which training samples were bounded.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator-functions","title":"Functions","text":""},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray, y: ndarray | None = None\n) -&gt; UnboundednessMetaEstimator\n</code></pre> <p>Fit the meta-estimator.</p> <p>Detects unbounded samples, then fits the base estimator only on bounded samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data.</p> required <code>y</code> <code>ndarray | None</code> <p>Target values. Only used if the base estimator is a classifier.</p> <code>None</code> <p>Returns:</p> Type Description <code>UnboundednessMetaEstimator</code> <p>Fitted estimator.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict labels for samples in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Samples to predict.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted labels.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(\n    X: ndarray, y: ndarray | None = None\n) -&gt; np.ndarray\n</code></pre> <p>Fit the meta-estimator and predict labels (for clusterers).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training data.</p> required <code>y</code> <code>ndarray | None</code> <p>Target values (ignored for clusterers, required for classifiers).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted labels.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.UnboundednessMetaEstimator.__sklearn_tags__","title":"__sklearn_tags__","text":"<pre><code>__sklearn_tags__() -&gt; Any\n</code></pre> <p>Provide sklearn tags based on the wrapped estimator type.</p> <p>The meta-estimator adapts its behavior based on the wrapped estimator, similar to Pipeline.</p>"},{"location":"api/predictors/#pybasin.predictors.unboundedness_meta_estimator.default_unbounded_detector","title":"pybasin.predictors.unboundedness_meta_estimator.default_unbounded_detector","text":"<pre><code>default_unbounded_detector(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Default unbounded trajectory detector.</p> <p>Detects unbounded trajectories based on: - NaN values (invalid/undefined trajectories) - Inf or -Inf values (from JAX solver) - Values at extreme bounds: 1e10 or -1e10 (from torch feature extractor with imputation)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Feature array of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array of shape (n_samples,) where True indicates unbounded.</p>"},{"location":"api/samplers/","title":"Samplers","text":""},{"location":"api/samplers/#pybasin.sampler.Sampler","title":"pybasin.sampler.Sampler","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sampling initial conditions using PyTorch.</p>"},{"location":"api/samplers/#pybasin.sampler.Sampler-functions","title":"Functions","text":""},{"location":"api/samplers/#pybasin.sampler.Sampler.__init__","title":"__init__","text":"<pre><code>__init__(\n    min_limits: list[float],\n    max_limits: list[float],\n    device: str | None = None,\n)\n</code></pre> <p>Initialize the sampler.</p> <p>Parameters:</p> Name Type Description Default <code>min_limits</code> <code>list[float]</code> <p>List of minimum values for each state.</p> required <code>max_limits</code> <code>list[float]</code> <p>List of maximum values for each state.</p> required <code>device</code> <code>str | None</code> <p>Device to use ('cuda', 'cpu', or None for auto-detect).</p> <code>None</code>"},{"location":"api/samplers/#pybasin.sampler.Sampler.sample","title":"sample  <code>abstractmethod</code>","text":"<pre><code>sample(n: int) -&gt; torch.Tensor\n</code></pre> <p>Generate n samples for the initial conditions.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Sampled initial conditions as a tensor of shape (n, state_dim).</p>"},{"location":"api/samplers/#pybasin.sampler.UniformRandomSampler","title":"pybasin.sampler.UniformRandomSampler","text":"<p>               Bases: <code>Sampler</code></p> <p>Generates random samples using a uniform distribution within the specified range.</p>"},{"location":"api/samplers/#pybasin.sampler.GridSampler","title":"pybasin.sampler.GridSampler","text":"<p>               Bases: <code>Sampler</code></p> <p>Generates evenly spaced samples in a grid pattern within the specified range.</p> <p>Handles fixed dimensions (where min == max) by only distributing grid points along varying dimensions. For example, with limits [-10, 10], [-20, 20], [0, 0] and n=20000, the grid uses n^(1/2) \u2248 142 points per varying dimension (x, y) and a single point for the fixed dimension (z), yielding 142 x 142 x 1 = 20164 unique samples instead of 28 x 28 x 28 = 21952 with many duplicates.</p>"},{"location":"api/samplers/#pybasin.sampler.GaussianSampler","title":"pybasin.sampler.GaussianSampler","text":"<p>               Bases: <code>Sampler</code></p> <p>Generates samples using a Gaussian distribution around the midpoint.</p>"},{"location":"api/solution/","title":"Solution","text":""},{"location":"api/solution/#pybasin.solution.Solution","title":"pybasin.solution.Solution","text":"<p>Solution: Represents the time integration result for a single initial condition.</p> <p>This class stores:</p> <ul> <li>The initial condition used for integration.</li> <li>The time series result from integration.</li> <li>Features extracted from the trajectory.</li> <li>Optional labels/classification for each trajectory.</li> <li>Optional model parameters that were used in the integration.</li> <li>Optional bifurcation amplitudes extracted from the trajectory.</li> </ul> <p>Attributes:</p> Name Type Description <code>initial_condition</code> <code>Tensor</code> <p>The initial condition used for integration (shape: B, S).</p> <code>time</code> <code>Tensor</code> <p>Time points of the solution (shape: N).</p> <code>y</code> <code>Tensor</code> <p>State values over time (shape: N, B, S).</p> <code>features</code> <code>Tensor | None</code> <p>Filtered features used for classification.</p> <code>extracted_features</code> <code>Tensor | None</code> <p>Original extracted features before filtering.</p> <code>extracted_feature_names</code> <code>list[str] | None</code> <p>Names of extracted features.</p> <code>feature_names</code> <code>list[str] | None</code> <p>Names of filtered features.</p> <code>labels</code> <code>ndarray | None</code> <p>Labels assigned to each solution in the batch.</p> <code>model_params</code> <code>dict[str, Any] | None</code> <p>Parameters of the ODE model.</p> <code>bifurcation_amplitudes</code> <code>Tensor | None</code> <p>Maximum absolute values along time dimension.</p>"},{"location":"api/solution/#pybasin.solution.Solution-functions","title":"Functions","text":""},{"location":"api/solution/#pybasin.solution.Solution.__init__","title":"__init__","text":"<pre><code>__init__(\n    initial_condition: Tensor,\n    time: Tensor,\n    y: Tensor,\n    features: Tensor | None = None,\n    labels: ndarray | None = None,\n    model_params: dict[str, float] | None = None,\n)\n</code></pre> <p>Initialize the Solution object.</p> <p>Parameters:</p> Name Type Description Default <code>initial_condition</code> <code>Tensor</code> <p>shape: (B, S) =&gt; B batches, S state variables</p> required <code>time</code> <code>Tensor</code> <p>shape: (N,) =&gt; N time points</p> required <code>y</code> <code>Tensor</code> <p>shape: (N, B, S) =&gt; N time points, B batches, S state variables</p> required <code>features</code> <code>Tensor | None</code> <p>Optional features describing the trajectory.</p> <code>None</code> <code>labels</code> <code>ndarray | None</code> <p>Optional classification labels for the solutions.</p> <code>None</code> <code>model_params</code> <code>dict[str, float] | None</code> <p>Optional dictionary of model parameters used in the simulation.</p> <code>None</code>"},{"location":"api/solution/#pybasin.solution.Solution.set_labels","title":"set_labels","text":"<pre><code>set_labels(labels: ndarray)\n</code></pre> <p>Assign a label to this solution.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>ndarray</code> <p>The label to assign from the classification results.</p> required"},{"location":"api/solution/#pybasin.solution.Solution.set_extracted_features","title":"set_extracted_features","text":"<pre><code>set_extracted_features(features: Tensor, names: list[str])\n</code></pre> <p>Store extracted features before filtering.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Extracted feature tensor.</p> required <code>names</code> <code>list[str]</code> <p>List of feature names.</p> required"},{"location":"api/solution/#pybasin.solution.Solution.set_features","title":"set_features","text":"<pre><code>set_features(\n    features: Tensor, names: list[str] | None = None\n)\n</code></pre> <p>Store features extracted from the trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>A feature vector describing the solution (typically filtered).</p> required <code>names</code> <code>list[str] | None</code> <p>Optional list of feature names (for filtered features).</p> <code>None</code>"},{"location":"api/solution/#pybasin.solution.Solution.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; dict[str, Any]\n</code></pre> <p>Return a summary of the solution.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with key information about the solution.</p>"},{"location":"api/solvers/","title":"Solvers","text":""},{"location":"api/solvers/#solver-protocol","title":"Solver Protocol","text":""},{"location":"api/solvers/#pybasin.protocols.SolverProtocol","title":"pybasin.protocols.SolverProtocol","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol defining the common interface for ODE solvers.</p> <p>Two implementations exist: Solver (PyTorch-based) and JaxSolver (JAX-based). Structural typing allows both to satisfy this protocol without explicit inheritance, though classes may inherit from it to declare conformance explicitly.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>device</code> <p>Device for output tensors.</p>"},{"location":"api/solvers/#pybasin.protocols.SolverProtocol-functions","title":"Functions","text":""},{"location":"api/solvers/#pybasin.protocols.SolverProtocol.__init__","title":"__init__","text":"<pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    method: Any = None,\n    rtol: float = 1e-08,\n    atol: float = 1e-06,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n) -&gt; None\n</code></pre> <p>Initialize the solver with integration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>time_span</code> <code>tuple[float, float]</code> <p>Tuple (t_start, t_end) defining the integration interval.</p> <code>(0, 1000)</code> <code>n_steps</code> <code>int</code> <p>Number of evaluation points.</p> <code>1000</code> <code>device</code> <code>str | None</code> <p>Device to use ('cuda', 'cpu', 'gpu', or None for auto-detect).</p> <code>None</code> <code>method</code> <code>Any</code> <p>Integration method (solver-specific).</p> <code>None</code> <code>rtol</code> <code>float</code> <p>Relative tolerance (used by adaptive-step methods only).</p> <code>1e-08</code> <code>atol</code> <code>float</code> <p>Absolute tolerance (used by adaptive-step methods only).</p> <code>1e-06</code> <code>cache_dir</code> <code>str | None</code> <p>Directory for caching integration results. Relative paths are resolved from the project root. <code>None</code> disables caching.</p> <code>DEFAULT_CACHE_DIR</code>"},{"location":"api/solvers/#pybasin.protocols.SolverProtocol.integrate","title":"integrate","text":"<pre><code>integrate(\n    ode_system: ODESystemProtocol, y0: Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Solve the ODE system and return the evaluation time points and solution.</p> <p>Parameters:</p> Name Type Description Default <code>ode_system</code> <code>ODESystemProtocol</code> <p>An instance of an ODE system (ODESystem or JaxODESystem).</p> required <code>y0</code> <code>Tensor</code> <p>Initial conditions with shape (batch, n_dims).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple (t_eval, y_values) where y_values has shape (n_steps, batch, n_dims).</p>"},{"location":"api/solvers/#pybasin.protocols.SolverProtocol.clone","title":"clone","text":"<pre><code>clone(\n    *,\n    device: str | None = None,\n    n_steps_factor: int = 1,\n    cache_dir: str | None | object = UNSET,\n) -&gt; SolverProtocol\n</code></pre> <p>Create a copy of this solver, optionally overriding device, resolution, or caching.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | None</code> <p>Target device ('cpu', 'cuda', 'gpu'). If None, keeps the current device.</p> <code>None</code> <code>n_steps_factor</code> <code>int</code> <p>Multiply the number of evaluation points by this factor (e.g. 10 for smoother plotting). Defaults to 1 (no change).</p> <code>1</code> <code>cache_dir</code> <code>str | None | object</code> <p>Override cache directory. Pass <code>None</code> to disable caching. If not provided, keeps the current setting.</p> <code>UNSET</code> <p>Returns:</p> Type Description <code>SolverProtocol</code> <p>New solver instance.</p>"},{"location":"api/solvers/#solver-implementations","title":"Solver Implementations","text":""},{"location":"api/solvers/#pybasin.solvers.jax_solver.JaxSolver","title":"pybasin.solvers.jax_solver.JaxSolver","text":"<p>               Bases: <code>SolverProtocol</code>, <code>DisplayNameMixin</code></p> <p>High-performance ODE solver using JAX and Diffrax for native JAX ODE systems.</p> <p>This solver is optimized for JaxODESystem instances and provides the fastest integration performance by avoiding any PyTorch callbacks. It uses JIT compilation and vmap for efficient batch processing.</p> <p>The interface is compatible with other solvers - it accepts PyTorch tensors and returns PyTorch tensors, but internally uses JAX for maximum performance.</p> <p>See also: Diffrax documentation</p> <p>Citation:</p> <pre><code>@phdthesis{kidger2021on,\n    title={{O}n {N}eural {D}ifferential {E}quations},\n    author={Patrick Kidger},\n    year={2021},\n    school={University of Oxford},\n}\n</code></pre> <p>Example usage:</p> <p>Overload 1 \u2014 generic API for standard ODEs:</p> <pre><code>from pybasin.jax_ode_system import JaxODESystem\nfrom pybasin.solvers import JaxSolver\nimport torch\n\nclass MyODE(JaxODESystem):\n    def ode(self, t, y):\n        return -y  # Simple decay\n    def get_str(self):\n        return \"decay\"\n\nsolver = JaxSolver(time_span=(0, 10), n_steps=100)\ny0 = torch.tensor([[1.0, 2.0]])  # batch=1, dims=2\nt, y = solver.integrate(MyODE({}), y0)\n</code></pre> <p>Overload 2 \u2014 direct Diffrax control via <code>solver_args</code>:</p> <p>Pass native Diffrax arguments directly to <code>diffeqsolve</code>. This is useful for SDEs, CDEs, or any advanced Diffrax configuration.</p> <p>.. note::</p> <p>When using <code>solver_args</code>, the integration time points are baked into    <code>saveat.ts</code> at construction time. The <code>n_steps_factor</code> parameter of    :meth:<code>clone</code> has no effect in this mode \u2014 the actual integration still    uses the original <code>saveat</code>.</p> <pre><code>from diffrax import Dopri5, ODETerm, PIDController, SaveAt\nimport jax.numpy as jnp\n\nsolver = JaxSolver(\n    solver_args={\n        \"terms\": ODETerm(lambda t, y, args: -y),\n        \"solver\": Dopri5(),\n        \"t0\": 0,\n        \"t1\": 10,\n        \"dt0\": 0.1,\n        \"saveat\": SaveAt(ts=jnp.linspace(0, 10, 100)),\n        \"stepsize_controller\": PIDController(rtol=1e-5, atol=1e-5),\n    },\n)\n</code></pre>"},{"location":"api/solvers/#pybasin.solvers.jax_solver.JaxSolver-functions","title":"Functions","text":""},{"location":"api/solvers/#pybasin.solvers.jax_solver.JaxSolver.__init__","title":"__init__","text":"<pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    method: Any | None = None,\n    rtol: float = 1e-08,\n    atol: float = 1e-06,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n    max_steps: int = DEFAULT_MAX_STEPS,\n    event_fn: Callable[[Any, Array, Any], Array]\n    | None = None,\n) -&gt; None\n</code></pre><pre><code>__init__(\n    *,\n    solver_args: dict[str, Any],\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n) -&gt; None\n</code></pre> <pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    method: Any | None = None,\n    rtol: float = 1e-08,\n    atol: float = 1e-06,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n    max_steps: int = DEFAULT_MAX_STEPS,\n    event_fn: Callable[[Any, Array, Any], Array]\n    | None = None,\n    *,\n    solver_args: dict[str, Any] | None = None,\n)\n</code></pre> <p>Initialize JaxSolver.</p> <p>Can be called in two ways:</p> <ol> <li>Generic API with named parameters for standard ODE integration:</li> </ol> <p><code>JaxSolver(time_span=(0, 10), n_steps=100, rtol=1e-8, ...)</code></p> <ol> <li>Direct Diffrax control via <code>solver_args</code> for full access to    <code>diffeqsolve</code> kwargs (SDEs, CDEs, custom step-size controllers, etc.):</li> </ol> <p><code>JaxSolver(solver_args={\"terms\": ..., \"solver\": ..., \"t0\": ..., ...})</code></p> <p>The two interfaces are mutually exclusive at the type level.</p> <p>Parameters:</p> Name Type Description Default <code>time_span</code> <code>tuple[float, float]</code> <p>Tuple (t_start, t_end) defining the integration interval.</p> <code>(0, 1000)</code> <code>n_steps</code> <code>int</code> <p>Number of evaluation points.</p> <code>1000</code> <code>device</code> <code>str | None</code> <p>Device to use ('cuda', 'gpu', 'cpu', or None for auto-detect).</p> <code>None</code> <code>method</code> <code>Any | None</code> <p>Diffrax solver instance (e.g., Dopri5(), Tsit5()). Defaults to Dopri5() if None.</p> <code>None</code> <code>rtol</code> <code>float</code> <p>Relative tolerance (used by adaptive-step methods only).</p> <code>1e-08</code> <code>atol</code> <code>float</code> <p>Absolute tolerance (used by adaptive-step methods only).</p> <code>1e-06</code> <code>max_steps</code> <code>int</code> <p>Maximum number of integrator steps.</p> <code>DEFAULT_MAX_STEPS</code> <code>cache_dir</code> <code>str | None</code> <p>Directory for caching integration results. Relative paths are resolved from the project root. <code>None</code> disables caching.</p> <code>DEFAULT_CACHE_DIR</code> <code>event_fn</code> <code>Callable[[Any, Array, Any], Array] | None</code> <p>Optional event function for early termination. Should return positive when integration should continue, negative/zero to stop. Signature: (t, y, args) -&gt; scalar Array.</p> <code>None</code> <code>solver_args</code> <code>dict[str, Any] | None</code> <p>Dict of kwargs passed directly to <code>diffrax.diffeqsolve()</code>. When provided, all other Diffrax-specific parameters are ignored. Must NOT include <code>y0</code> (provided per-trajectory via <code>integrate()</code>).</p> <code>None</code>"},{"location":"api/solvers/#pybasin.solvers.jax_solver.JaxSolver.clone","title":"clone","text":"<pre><code>clone(\n    *,\n    device: str | None = None,\n    n_steps_factor: int = 1,\n    cache_dir: str | None | object = UNSET,\n) -&gt; JaxSolver\n</code></pre> <p>Create a copy of this solver, optionally overriding device, resolution, or caching.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | None</code> <p>Target device ('cpu', 'cuda', 'gpu'). If None, keeps the current device.</p> <code>None</code> <code>n_steps_factor</code> <code>int</code> <p>Multiply the number of evaluation points by this factor. Ignored for <code>solver_args</code> mode (saveat is baked in at construction time).</p> <code>1</code> <code>cache_dir</code> <code>str | None | object</code> <p>Override cache directory. Pass <code>None</code> to disable caching. If not provided, keeps the current setting.</p> <code>UNSET</code> <p>Returns:</p> Type Description <code>JaxSolver</code> <p>New JaxSolver instance.</p>"},{"location":"api/solvers/#pybasin.solvers.jax_solver.JaxSolver.integrate","title":"integrate","text":"<pre><code>integrate(\n    ode_system: ODESystemProtocol, y0: Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Solve the ODE system and return the evaluation time points and solution.</p> <p>Parameters:</p> Name Type Description Default <code>ode_system</code> <code>ODESystemProtocol</code> <p>An instance of JaxODESystem.</p> required <code>y0</code> <code>Tensor</code> <p>Initial conditions as PyTorch tensor with shape (batch, n_dims).</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple (t_eval, y_values) as PyTorch tensors where y_values has shape (n_steps, batch, n_dims).</p>"},{"location":"api/solvers/#pybasin.solvers.TorchDiffEqSolver","title":"pybasin.solvers.TorchDiffEqSolver","text":"<p>               Bases: <code>Solver</code></p> <p>Differentiable ODE solver with full GPU support and O(1)-memory backpropagation.</p> <p>Uses the adjoint method for memory-efficient gradient computation through ODE solutions. Supports adaptive-step (dopri5, dopri8, bosh3) and fixed-step (euler, rk4) methods.</p> <p>See also: torchdiffeq GitHub</p> <p>Citation:</p> <pre><code>@misc{torchdiffeq,\n    author={Chen, Ricky T. Q.},\n    title={torchdiffeq},\n    year={2018},\n    url={https://github.com/rtqichen/torchdiffeq},\n}\n</code></pre>"},{"location":"api/solvers/#pybasin.solvers.TorchDiffEqSolver-functions","title":"Functions","text":""},{"location":"api/solvers/#pybasin.solvers.TorchDiffEqSolver.__init__","title":"__init__","text":"<pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    method: str = \"dopri5\",\n    rtol: float = 1e-08,\n    atol: float = 1e-06,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n)\n</code></pre> <p>Initialize TorchDiffEqSolver.</p> <p>Parameters:</p> Name Type Description Default <code>time_span</code> <code>tuple[float, float]</code> <p>Tuple (t_start, t_end) defining the integration interval.</p> <code>(0, 1000)</code> <code>n_steps</code> <code>int</code> <p>Number of evaluation points.</p> <code>1000</code> <code>device</code> <code>str | None</code> <p>Device to use ('cuda', 'cpu', or None for auto-detect).</p> <code>None</code> <code>method</code> <code>str</code> <p>Integration method from tordiffeq.odeint.</p> <code>'dopri5'</code> <code>rtol</code> <code>float</code> <p>Relative tolerance (used by adaptive-step methods only).</p> <code>1e-08</code> <code>atol</code> <code>float</code> <p>Absolute tolerance (used by adaptive-step methods only).</p> <code>1e-06</code> <code>cache_dir</code> <code>str | None</code> <p>Directory for caching integration results. <code>None</code> disables caching.</p> <code>DEFAULT_CACHE_DIR</code>"},{"location":"api/solvers/#pybasin.solvers.TorchDiffEqSolver.clone","title":"clone","text":"<pre><code>clone(\n    *,\n    device: str | None = None,\n    n_steps_factor: int = 1,\n    cache_dir: str | None | object = UNSET,\n) -&gt; TorchDiffEqSolver\n</code></pre> <p>Create a copy of this solver, optionally overriding device, resolution, or caching.</p>"},{"location":"api/solvers/#pybasin.solvers.TorchOdeSolver","title":"pybasin.solvers.TorchOdeSolver","text":"<p>               Bases: <code>Solver</code></p> <p>Parallel ODE solver with independent step sizes per batch element.</p> <p>Compatible with PyTorch's JIT compiler for performance optimization. Unlike other solvers, torchode can take different step sizes for each sample in a batch, avoiding performance traps for problems of varying stiffness.</p> <p>See also: torchode documentation</p> <p>Citation:</p> <pre><code>@inproceedings{lienen2022torchode,\n    title = {torchode: A Parallel {ODE} Solver for PyTorch},\n    author = {Marten Lienen and Stephan G{\"u}nnemann},\n    booktitle = {The Symbiosis of Deep Learning and Differential Equations II, NeurIPS},\n    year = {2022},\n    url = {https://openreview.net/forum?id=uiKVKTiUYB0}\n}\n</code></pre>"},{"location":"api/solvers/#pybasin.solvers.TorchOdeSolver-functions","title":"Functions","text":""},{"location":"api/solvers/#pybasin.solvers.TorchOdeSolver.__init__","title":"__init__","text":"<pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    method: str = \"dopri5\",\n    rtol: float = 1e-08,\n    atol: float = 1e-06,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n)\n</code></pre> <p>Initialize TorchOdeSolver.</p> <p>Parameters:</p> Name Type Description Default <code>time_span</code> <code>tuple[float, float]</code> <p>Tuple (t_start, t_end) defining the integration interval.</p> <code>(0, 1000)</code> <code>n_steps</code> <code>int</code> <p>Number of evaluation points.</p> <code>1000</code> <code>device</code> <code>str | None</code> <p>Device to use ('cuda', 'cpu', or None for auto-detect).</p> <code>None</code> <code>method</code> <code>str</code> <p>Integration method ('dopri5', 'tsit5', 'euler', 'heun').</p> <code>'dopri5'</code> <code>rtol</code> <code>float</code> <p>Relative tolerance (used by adaptive-step methods only).</p> <code>1e-08</code> <code>atol</code> <code>float</code> <p>Absolute tolerance (used by adaptive-step methods only).</p> <code>1e-06</code> <code>cache_dir</code> <code>str | None</code> <p>Directory for caching integration results. <code>None</code> disables caching.</p> <code>DEFAULT_CACHE_DIR</code>"},{"location":"api/solvers/#pybasin.solvers.TorchOdeSolver.clone","title":"clone","text":"<pre><code>clone(\n    *,\n    device: str | None = None,\n    n_steps_factor: int = 1,\n    cache_dir: str | None | object = UNSET,\n) -&gt; TorchOdeSolver\n</code></pre> <p>Create a copy of this solver, optionally overriding device, resolution, or caching.</p>"},{"location":"api/solvers/#pybasin.solvers.ScipyParallelSolver","title":"pybasin.solvers.ScipyParallelSolver","text":"<p>               Bases: <code>Solver</code></p> <p>ODE solver using sklearn's parallel processing with scipy's solve_ivp.</p> <p>Uses multiprocessing (loky backend) to solve multiple initial conditions in parallel. Each worker solves one trajectory at a time using scipy's solve_ivp.</p> <p>See also: scipy.integrate.solve_ivp</p>"},{"location":"api/solvers/#pybasin.solvers.ScipyParallelSolver-functions","title":"Functions","text":""},{"location":"api/solvers/#pybasin.solvers.ScipyParallelSolver.__init__","title":"__init__","text":"<pre><code>__init__(\n    time_span: tuple[float, float] = (0, 1000),\n    n_steps: int = 1000,\n    device: str | None = None,\n    n_jobs: int = -1,\n    method: str = \"RK45\",\n    rtol: float = 1e-06,\n    atol: float = 1e-08,\n    max_step: float | None = None,\n    cache_dir: str | None = DEFAULT_CACHE_DIR,\n)\n</code></pre> <p>Initialize ScipyParallelSolver.</p> <p>Parameters:</p> Name Type Description Default <code>time_span</code> <code>tuple[float, float]</code> <p>Integration interval (t_start, t_end).</p> <code>(0, 1000)</code> <code>n_steps</code> <code>int</code> <p>Number of evaluation points.</p> <code>1000</code> <code>device</code> <code>str | None</code> <p>Device to use (only 'cpu' supported).</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs (-1 for all CPUs).</p> <code>-1</code> <code>method</code> <code>str</code> <p>Integration method ('RK45', 'RK23', 'DOP853', 'Radau', 'BDF', 'LSODA', etc).</p> <code>'RK45'</code> <code>rtol</code> <code>float</code> <p>Relative tolerance (used by adaptive-step methods only).</p> <code>1e-06</code> <code>atol</code> <code>float</code> <p>Absolute tolerance (used by adaptive-step methods only).</p> <code>1e-08</code> <code>max_step</code> <code>float | None</code> <p>Maximum step size for the solver.</p> <code>None</code> <code>cache_dir</code> <code>str | None</code> <p>Directory for caching integration results. <code>None</code> disables caching.</p> <code>DEFAULT_CACHE_DIR</code>"},{"location":"api/solvers/#pybasin.solvers.ScipyParallelSolver.clone","title":"clone","text":"<pre><code>clone(\n    *,\n    device: str | None = None,\n    n_steps_factor: int = 1,\n    cache_dir: str | None | object = UNSET,\n) -&gt; ScipyParallelSolver\n</code></pre> <p>Create a copy of this solver, optionally overriding device, resolution, or caching.</p> <p>Note: ScipyParallelSolver only supports CPU, so the device is always CPU.</p>"},{"location":"api/template-integrator/","title":"TemplateIntegrator","text":"<p>For supervised classification workflows, templates are managed by the <code>TemplateIntegrator</code> class.</p> <p>See the Predictors User Guide for usage examples.</p>"},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator","title":"pybasin.template_integrator.TemplateIntegrator","text":"<p>Integrates template initial conditions and extracts training data for supervised classifiers.</p> <p>Handles the ODE integration of known attractor templates and feature extraction.</p> <p>Attributes:</p> Name Type Description <code>template_y0</code> <p>Template initial conditions.</p> <code>labels</code> <p>Ground truth labels for each template.</p> <code>ode_params</code> <p>ODE parameters for template integration (may differ from main study).</p> <code>solver</code> <p>Optional dedicated solver for template integration.</p> <code>solution</code> <code>Solution | None</code> <p>Populated after :meth:<code>integrate</code> is called.</p>"},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator-attributes","title":"Attributes","text":""},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator.has_dedicated_solver","title":"has_dedicated_solver  <code>property</code>","text":"<pre><code>has_dedicated_solver: bool\n</code></pre> <p>Check if a dedicated solver was provided for template integration.</p>"},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator-functions","title":"Functions","text":""},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator.__init__","title":"__init__","text":"<pre><code>__init__(\n    template_y0: list[list[float]],\n    labels: list[str],\n    ode_params: Mapping[str, Any],\n    solver: SolverProtocol | None = None,\n)\n</code></pre> <p>Initialize the template integrator.</p> <p>Parameters:</p> Name Type Description Default <code>template_y0</code> <code>list[list[float]]</code> <p>Template initial conditions as a list of lists (e.g., <code>[[0.5, 0.0], [2.7, 0.0]]</code>). Will be converted to tensor with appropriate device during integration.</p> required <code>labels</code> <code>list[str]</code> <p>Ground truth labels for template conditions.</p> required <code>ode_params</code> <code>Mapping[str, Any]</code> <p>ODE parameters mapping (dict or TypedDict with numeric values).</p> required <code>solver</code> <code>SolverProtocol | None</code> <p>Optional dedicated solver for template integration. If provided, this solver will be used instead of the main solver (useful for CPU-based template integration when templates are few).</p> <code>None</code>"},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator.integrate","title":"integrate","text":"<pre><code>integrate(\n    solver: SolverProtocol | None,\n    ode_system: ODESystemProtocol,\n) -&gt; None\n</code></pre> <p>Integrate ODE for template initial conditions.</p> <p>If no dedicated solver was provided at init, automatically creates a CPU variant of the passed solver for better performance with small batch sizes.</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <code>SolverProtocol | None</code> <p>Fallback solver if none was provided at init. Can be None if a solver was provided during initialization.</p> required <code>ode_system</code> <code>ODESystemProtocol</code> <p>ODE system to integrate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If no solver is available.</p>"},{"location":"api/template-integrator/#pybasin.template_integrator.TemplateIntegrator.get_training_data","title":"get_training_data","text":"<pre><code>get_training_data(\n    feature_extractor: FeatureExtractor,\n    feature_selector: Any | None = None,\n) -&gt; tuple[np.ndarray, list[str]]\n</code></pre> <p>Extract features from integrated templates and return training data.</p> <p>Must call :meth:<code>integrate</code> first to populate <code>self.solution</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_extractor</code> <code>FeatureExtractor</code> <p>Feature extractor (already fitted on main data).</p> required <code>feature_selector</code> <code>Any | None</code> <p>Optional feature selector (already fitted on main data). If provided, applies the same filtering to template features.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, list[str]]</code> <p>Tuple of <code>(X_train, y_labels)</code> ready for <code>classifier.fit()</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If :meth:<code>integrate</code> was not called first.</p> <code>ValueError</code> <p>If filtering removes all template features.</p>"},{"location":"api/torch-feature-calculators/","title":"Torch Feature Calculators","text":"<p>Experimental</p> <p>These feature calculators are experimental reimplementations of tsfresh in pure PyTorch. Individual implementations have not been deeply validated against tsfresh for correctness in all cases. Results are close but not identical to tsfresh.</p> <p>All feature functions follow a consistent tensor shape convention:</p> <ul> <li>Input: <code>(N, B, S)</code> where <code>N</code> = timesteps, <code>B</code> = batch size, <code>S</code> = state variables</li> <li>Output: <code>(B, S)</code> for scalar features, or <code>(K, B, S)</code> for multi-valued features where <code>K</code> is the number of values</li> </ul> <p>Features are computed along the time dimension (<code>dim=0</code>), preserving batch and state dimensions. Functions suffixed with <code>_batched</code> compute several parameter variations in a single pass and return shape <code>(K, B, S)</code>.</p>"},{"location":"api/torch-feature-calculators/#statistical","title":"Statistical","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical","title":"pybasin.ts_torch.calculators.torch_features_statistical","text":"<p>Statistical feature calculators for time series.</p> <p>All feature functions follow a consistent tensor shape convention: - Input: (N, B, S) where N=timesteps, B=batch size, S=state variables - Output: (B, S) for scalar features, or (K, B, S) for multi-valued features where K is the number of values</p> <p>Features are computed along the time dimension (dim=0), preserving batch and state dimensions.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.sum_values","title":"sum_values","text":"<pre><code>sum_values(x: Tensor) -&gt; Tensor\n</code></pre> <p>Sum of all values along the time dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the sum across all N timesteps.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.median","title":"median","text":"<pre><code>median(x: Tensor) -&gt; Tensor\n</code></pre> <p>Median of the time series along the time dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the median value across all N timesteps.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.mean","title":"mean","text":"<pre><code>mean(x: Tensor) -&gt; Tensor\n</code></pre> <p>Mean of the time series along the time dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the mean value across all N timesteps.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.length","title":"length","text":"<pre><code>length(x: Tensor) -&gt; Tensor\n</code></pre> <p>Length of the time series (number of timesteps).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) where all values equal N (the number of timesteps).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.standard_deviation","title":"standard_deviation","text":"<pre><code>standard_deviation(x: Tensor) -&gt; Tensor\n</code></pre> <p>Standard deviation (population, ddof=0) along the time dimension.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the standard deviation across all N timesteps.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.variance","title":"variance","text":"<pre><code>variance(x: Tensor) -&gt; Tensor\n</code></pre> <p>Variance (population, ddof=0).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.root_mean_square","title":"root_mean_square","text":"<pre><code>root_mean_square(x: Tensor) -&gt; Tensor\n</code></pre> <p>Root mean square value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.maximum","title":"maximum","text":"<pre><code>maximum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Maximum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.absolute_maximum","title":"absolute_maximum","text":"<pre><code>absolute_maximum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Maximum absolute value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.minimum","title":"minimum","text":"<pre><code>minimum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Minimum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.delta","title":"delta","text":"<pre><code>delta(x: Tensor) -&gt; Tensor\n</code></pre> <p>Absolute difference between max and mean.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.log_delta","title":"log_delta","text":"<pre><code>log_delta(x: Tensor) -&gt; Tensor\n</code></pre> <p>Log of delta (with epsilon for stability).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.abs_energy","title":"abs_energy","text":"<pre><code>abs_energy(x: Tensor) -&gt; Tensor\n</code></pre> <p>Absolute energy (sum of squared values).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.kurtosis","title":"kurtosis","text":"<pre><code>kurtosis(x: Tensor) -&gt; Tensor\n</code></pre> <p>Fisher's kurtosis (excess kurtosis, bias-corrected).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.skewness","title":"skewness","text":"<pre><code>skewness(x: Tensor) -&gt; Tensor\n</code></pre> <p>Fisher's skewness (bias-corrected).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.quantile","title":"quantile","text":"<pre><code>quantile(x: Tensor, q: float) -&gt; Tensor\n</code></pre> <p>Q-quantile of the time series.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.variation_coefficient","title":"variation_coefficient","text":"<pre><code>variation_coefficient(x: Tensor) -&gt; Tensor\n</code></pre> <p>Coefficient of variation (std / mean).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.mean_n_absolute_max","title":"mean_n_absolute_max","text":"<pre><code>mean_n_absolute_max(\n    x: Tensor, number_of_maxima: int = 1\n) -&gt; Tensor\n</code></pre> <p>Mean of n largest absolute values (optimized with topk).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.ratio_beyond_r_sigma","title":"ratio_beyond_r_sigma","text":"<pre><code>ratio_beyond_r_sigma(x: Tensor, r: float = 1.0) -&gt; Tensor\n</code></pre> <p>Ratio of values beyond r standard deviations.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.symmetry_looking","title":"symmetry_looking","text":"<pre><code>symmetry_looking(x: Tensor, r: float = 0.1) -&gt; Tensor\n</code></pre> <p>Check if distribution looks symmetric.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.quantile_batched","title":"quantile_batched","text":"<pre><code>quantile_batched(x: Tensor, qs: list[float]) -&gt; Tensor\n</code></pre> <p>Compute multiple quantiles at once.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>qs</code> <code>list[float]</code> <p>List of quantile values (0.0 to 1.0) to compute.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (len(qs), B, S) containing the requested quantiles. The first dimension corresponds to the different quantile values in the same order as the input list.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.large_standard_deviation_batched","title":"large_standard_deviation_batched","text":"<pre><code>large_standard_deviation_batched(\n    x: Tensor, rs: list[float]\n) -&gt; Tensor\n</code></pre> <p>Check if std &gt; r * range for multiple r values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     rs: List of r threshold values</p> <p>Returns:     Tensor of shape (len(rs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.symmetry_looking_batched","title":"symmetry_looking_batched","text":"<pre><code>symmetry_looking_batched(\n    x: Tensor, rs: list[float]\n) -&gt; Tensor\n</code></pre> <p>Check if distribution looks symmetric for multiple r values.</p> <p>Args:     x: Input tensor of shape (N, B, S)     rs: List of r threshold values</p> <p>Returns:     Tensor of shape (len(rs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.ratio_beyond_r_sigma_batched","title":"ratio_beyond_r_sigma_batched","text":"<pre><code>ratio_beyond_r_sigma_batched(\n    x: Tensor, rs: list[float]\n) -&gt; Tensor\n</code></pre> <p>Compute ratio of values beyond r standard deviations for multiple r.</p> <p>Args:     x: Input tensor of shape (N, B, S)     rs: List of r multiplier values</p> <p>Returns:     Tensor of shape (len(rs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.mean_n_absolute_max_batched","title":"mean_n_absolute_max_batched","text":"<pre><code>mean_n_absolute_max_batched(\n    x: Tensor, ns: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute mean_n_absolute_max for multiple n values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     ns: List of number_of_maxima values</p> <p>Returns:     Tensor of shape (len(ns), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_statistical.amplitude","title":"amplitude","text":"<pre><code>amplitude(x: Tensor) -&gt; Tensor\n</code></pre> <p>Peak-to-peak amplitude (max - min) of the time series.</p> <p>Useful for distinguishing limit cycles with different oscillation amplitudes.</p> <p>Args:     x: Input tensor of shape (N, B, S) where N is timesteps, B is batch, S is states.</p> <p>Returns:     Tensor of shape (B, S) with the amplitude for each batch/state.</p>"},{"location":"api/torch-feature-calculators/#change-difference","title":"Change / Difference","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change","title":"pybasin.ts_torch.calculators.torch_features_change","text":"<p>Change and difference-based feature calculators for time series.</p> <p>All feature functions follow a consistent tensor shape convention: - Input: (N, B, S) where N=timesteps, B=batch size, S=state variables - Output: (B, S) for scalar features, or (K, B, S) for multi-valued features where K is the number of values</p> <p>Features are computed along the time dimension (dim=0), preserving batch and state dimensions.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.absolute_sum_of_changes","title":"absolute_sum_of_changes","text":"<pre><code>absolute_sum_of_changes(x: Tensor) -&gt; Tensor\n</code></pre> <p>Sum of absolute differences between consecutive values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the sum of |x[i+1] - x[i]| for all consecutive pairs.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.mean_abs_change","title":"mean_abs_change","text":"<pre><code>mean_abs_change(x: Tensor) -&gt; Tensor\n</code></pre> <p>Mean of absolute differences between consecutive values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the mean of |x[i+1] - x[i]| across all consecutive pairs.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.mean_change","title":"mean_change","text":"<pre><code>mean_change(x: Tensor) -&gt; Tensor\n</code></pre> <p>Mean change: (x[-1] - x[0]) / (n - 1).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.mean_second_derivative_central","title":"mean_second_derivative_central","text":"<pre><code>mean_second_derivative_central(x: Tensor) -&gt; Tensor\n</code></pre> <p>Mean of second derivative (central difference): (x[-1] - x[-2] - x[1] + x[0]) / (2 * (n-2)).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.change_quantiles","title":"change_quantiles","text":"<pre><code>change_quantiles(\n    x: Tensor,\n    ql: float,\n    qh: float,\n    isabs: bool = True,\n    f_agg: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Statistics of changes within quantile corridor.</p> <p>Computes statistics of consecutive value changes where both values fall within the [ql, qh] quantile range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>ql</code> <code>float</code> <p>Lower quantile (0.0 to 1.0) defining the corridor.</p> required <code>qh</code> <code>float</code> <p>Upper quantile (0.0 to 1.0), must be &gt; ql.</p> required <code>isabs</code> <code>bool</code> <p>If True, use absolute differences. Default is True.</p> <code>True</code> <code>f_agg</code> <code>str</code> <p>Aggregation function, \"mean\" or \"var\". Default is \"mean\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the requested statistic (mean or variance) of changes within the quantile corridor.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_change.change_quantiles_batched","title":"change_quantiles_batched","text":"<pre><code>change_quantiles_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute change_quantiles for multiple parameter combinations using vmap.</p> <p>This function pre-computes all unique quantiles once, then uses vmap to efficiently process all parameter combinations in a single kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>params</code> <code>list[dict]</code> <p>List of parameter dicts, each with keys: - \"ql\": float (lower quantile, 0.0 to 1.0) - \"qh\": float (upper quantile, 0.0 to 1.0) - \"isabs\": bool (whether to use absolute differences) - \"f_agg\": str (\"mean\" or \"var\")</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (len(params), B, S) containing the results for each parameter combination. The first dimension corresponds to the different parameter sets in the same order as the input list.  Example: params = [ {\"ql\": 0.0, \"qh\": 0.2, \"isabs\": True, \"f_agg\": \"mean\"}, {\"ql\": 0.0, \"qh\": 0.2, \"isabs\": True, \"f_agg\": \"var\"}, {\"ql\": 0.0, \"qh\": 0.2, \"isabs\": False, \"f_agg\": \"mean\"}, ... ] result = change_quantiles_batched(x, params)  # shape: (80, B, S)</p>"},{"location":"api/torch-feature-calculators/#counting","title":"Counting","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count","title":"pybasin.ts_torch.calculators.torch_features_count","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_above","title":"count_above","text":"<pre><code>count_above(x: Tensor, t: float) -&gt; Tensor\n</code></pre> <p>Percentage of values above threshold t.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_above_mean","title":"count_above_mean","text":"<pre><code>count_above_mean(x: Tensor) -&gt; Tensor\n</code></pre> <p>Count of values above mean.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_below","title":"count_below","text":"<pre><code>count_below(x: Tensor, t: float) -&gt; Tensor\n</code></pre> <p>Percentage of values below threshold t.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_below_mean","title":"count_below_mean","text":"<pre><code>count_below_mean(x: Tensor) -&gt; Tensor\n</code></pre> <p>Count of values below mean.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_in_range","title":"count_in_range","text":"<pre><code>count_in_range(\n    x: Tensor, min_val: float, max_val: float\n) -&gt; Tensor\n</code></pre> <p>Count of values in range [min_val, max_val].</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.count_value","title":"count_value","text":"<pre><code>count_value(x: Tensor, value: float) -&gt; Tensor\n</code></pre> <p>Count of specific value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.value_count_batched","title":"value_count_batched","text":"<pre><code>value_count_batched(\n    x: Tensor, values: list[float]\n) -&gt; Tensor\n</code></pre> <p>Compute value_count for multiple values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     values: List of values to count</p> <p>Returns:     Tensor of shape (len(values), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_count.range_count_batched","title":"range_count_batched","text":"<pre><code>range_count_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute range_count for multiple (min_val, max_val) pairs at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     params: List of parameter dicts, each with keys \"min_val\" and \"max_val\"</p> <p>Returns:     Tensor of shape (len(params), B, S)</p>"},{"location":"api/torch-feature-calculators/#boolean","title":"Boolean","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean","title":"pybasin.ts_torch.calculators.torch_features_boolean","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean.has_duplicate","title":"has_duplicate","text":"<pre><code>has_duplicate(x: Tensor) -&gt; Tensor\n</code></pre> <p>Check if any value occurs more than once (optimized with sorting).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean.has_duplicate_max","title":"has_duplicate_max","text":"<pre><code>has_duplicate_max(x: Tensor) -&gt; Tensor\n</code></pre> <p>Check if maximum value occurs more than once.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean.has_duplicate_min","title":"has_duplicate_min","text":"<pre><code>has_duplicate_min(x: Tensor) -&gt; Tensor\n</code></pre> <p>Check if minimum value occurs more than once.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean.has_variance_larger_than_standard_deviation","title":"has_variance_larger_than_standard_deviation","text":"<pre><code>has_variance_larger_than_standard_deviation(\n    x: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Check if variance &gt; standard deviation (equivalent to std &gt; 1).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_boolean.has_large_standard_deviation","title":"has_large_standard_deviation","text":"<pre><code>has_large_standard_deviation(\n    x: Tensor, r: float = 0.25\n) -&gt; Tensor\n</code></pre> <p>Check if std &gt; r * range.</p>"},{"location":"api/torch-feature-calculators/#location","title":"Location","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location","title":"pybasin.ts_torch.calculators.torch_features_location","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.first_location_of_maximum","title":"first_location_of_maximum","text":"<pre><code>first_location_of_maximum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Relative first location of maximum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.first_location_of_minimum","title":"first_location_of_minimum","text":"<pre><code>first_location_of_minimum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Relative first location of minimum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.last_location_of_maximum","title":"last_location_of_maximum","text":"<pre><code>last_location_of_maximum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Relative last location of maximum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.last_location_of_minimum","title":"last_location_of_minimum","text":"<pre><code>last_location_of_minimum(x: Tensor) -&gt; Tensor\n</code></pre> <p>Relative last location of minimum value.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.index_mass_quantile","title":"index_mass_quantile","text":"<pre><code>index_mass_quantile(x: Tensor, q: float) -&gt; Tensor\n</code></pre> <p>Index where q% of cumulative mass is reached.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_location.index_mass_quantile_batched","title":"index_mass_quantile_batched","text":"<pre><code>index_mass_quantile_batched(\n    x: Tensor, qs: list[float]\n) -&gt; Tensor\n</code></pre> <p>Compute index_mass_quantile for multiple q values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     qs: List of quantile values</p> <p>Returns:     Tensor of shape (len(qs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pattern-streak","title":"Pattern / Streak","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern","title":"pybasin.ts_torch.calculators.torch_features_pattern","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.longest_strike_above_mean","title":"longest_strike_above_mean","text":"<pre><code>longest_strike_above_mean(x: Tensor) -&gt; Tensor\n</code></pre> <p>Longest consecutive sequence above mean.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.longest_strike_below_mean","title":"longest_strike_below_mean","text":"<pre><code>longest_strike_below_mean(x: Tensor) -&gt; Tensor\n</code></pre> <p>Longest consecutive sequence below mean.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.number_crossing_m","title":"number_crossing_m","text":"<pre><code>number_crossing_m(x: Tensor, m: float) -&gt; Tensor\n</code></pre> <p>Number of crossings of level m.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.number_peaks","title":"number_peaks","text":"<pre><code>number_peaks(x: Tensor, n: int) -&gt; Tensor\n</code></pre> <p>Count peaks with support n on each side (vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.number_cwt_peaks","title":"number_cwt_peaks","text":"<pre><code>number_cwt_peaks(x: Tensor, max_width: int = 5) -&gt; Tensor\n</code></pre> <p>Count peaks detected via CWT-like multi-scale analysis (optimized).</p> <p>Uses integer accumulation and precomputed masks to minimize allocations and avoid unnecessary dtype conversions during the loop.</p> <p>Input x: (N, B, S) -&gt; returns (B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.number_peaks_batched","title":"number_peaks_batched","text":"<pre><code>number_peaks_batched(x: Tensor, ns: list[int]) -&gt; Tensor\n</code></pre> <p>Compute number_peaks for multiple n values at once.</p> <p>Uses max pooling for each unique n value, computing peaks efficiently.</p> <p>Args:     x: Input tensor of shape (N, B, S)     ns: List of n values (support on each side)</p> <p>Returns:     Tensor of shape (len(ns), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_pattern.number_crossing_m_batched","title":"number_crossing_m_batched","text":"<pre><code>number_crossing_m_batched(\n    x: Tensor, ms: list[float]\n) -&gt; Tensor\n</code></pre> <p>Compute number_crossing_m for multiple m values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     ms: List of threshold values m</p> <p>Returns:     Tensor of shape (len(ms), B, S)</p>"},{"location":"api/torch-feature-calculators/#autocorrelation","title":"Autocorrelation","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation","title":"pybasin.ts_torch.calculators.torch_features_autocorrelation","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.autocorrelation","title":"autocorrelation","text":"<pre><code>autocorrelation(x: Tensor, lag: int) -&gt; Tensor\n</code></pre> <p>Autocorrelation at given lag.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.partial_autocorrelation","title":"partial_autocorrelation","text":"<pre><code>partial_autocorrelation(x: Tensor, lag: int) -&gt; Tensor\n</code></pre> <p>Partial autocorrelation at given lag using Durbin-Levinson (fully vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.agg_autocorrelation","title":"agg_autocorrelation","text":"<pre><code>agg_autocorrelation(\n    x: Tensor, maxlag: int = 40, f_agg: str = \"mean\"\n) -&gt; Tensor\n</code></pre> <p>Aggregated autocorrelation over lags 1 to maxlag (FFT-optimized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.autocorrelation_batched","title":"autocorrelation_batched","text":"<pre><code>autocorrelation_batched(\n    x: Tensor, lags: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute autocorrelation for multiple lags at once using FFT.</p> <p>Args:     x: Input tensor of shape (N, B, S)     lags: List of lag values to compute</p> <p>Returns:     Tensor of shape (len(lags), B, S) with autocorrelation at each lag</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.partial_autocorrelation_batched","title":"partial_autocorrelation_batched","text":"<pre><code>partial_autocorrelation_batched(\n    x: Tensor, lags: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute partial autocorrelation for multiple lags at once.</p> <p>Computes Durbin-Levinson algorithm once for max(lags), then extracts requested lag values. Stores PACF values separately from AR coefficients since the algorithm modifies AR coefficients in-place.</p> <p>Args:     x: Input tensor of shape (N, B, S)     lags: List of lag values to compute</p> <p>Returns:     Tensor of shape (len(lags), B, S) with PACF at each lag</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.agg_autocorrelation_batched","title":"agg_autocorrelation_batched","text":"<pre><code>agg_autocorrelation_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute agg_autocorrelation for multiple (maxlag, f_agg) combinations at once.</p> <p>Groups by maxlag to minimize FFT computations.</p> <p>Args:     x: Input tensor of shape (N, B, S)     params: List of parameter dicts, each with keys:         - \"maxlag\": int (defaults to 40 if not specified)         - \"f_agg\": str (\"mean\", \"median\", \"var\")</p> <p>Returns:     Tensor of shape (len(params), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_autocorrelation.autocorrelation_periodicity","title":"autocorrelation_periodicity","text":"<pre><code>autocorrelation_periodicity(\n    x: Tensor,\n    min_lag: int = 2,\n    peak_threshold: float = 0.3,\n    output: str = \"strength\",\n) -&gt; Tensor\n</code></pre> <p>Compute autocorrelation-based periodicity measures.</p> <p>TODO: Support returning multiple outputs (K, B, S) instead of requiring separate calls.       This would require updating torch_feature_extractor.py and torch_feature_processors.py       to handle 3D output tensors properly.</p> <p>Returns either the periodicity strength (height of first significant autocorrelation peak) or the period estimate (lag of that peak). This is useful for detecting limit cycles vs chaos vs fixed points.</p> <p>Uses FFT for efficient autocorrelation computation and local_maxima_1d for robust peak detection.</p> <p>Args:     x: Input tensor of shape (N, B, S) where N is timesteps, B is batch, S is states.     min_lag: Minimum lag to search for peaks (to skip lag-0 peak). Default 2.     peak_threshold: Minimum autocorrelation value to consider as a peak. Default 0.3.     output: Which value to return - \"strength\" or \"period\". Default \"strength\".</p> <p>Returns:     Tensor of shape (B, S) with either periodicity strength or period estimate.</p>"},{"location":"api/torch-feature-calculators/#entropy-complexity","title":"Entropy / Complexity","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity","title":"pybasin.ts_torch.calculators.torch_features_entropy_complexity","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.permutation_entropy","title":"permutation_entropy","text":"<pre><code>permutation_entropy(\n    x: Tensor, tau: int = 1, dimension: int = 3\n) -&gt; Tensor\n</code></pre> <p>Permutation entropy (fully vectorized GPU implementation).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.binned_entropy","title":"binned_entropy","text":"<pre><code>binned_entropy(x: Tensor, max_bins: int = 10) -&gt; Tensor\n</code></pre> <p>Entropy of binned distribution (vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.fourier_entropy","title":"fourier_entropy","text":"<pre><code>fourier_entropy(x: Tensor, bins: int = 10) -&gt; Tensor\n</code></pre> <p>Entropy of the power spectral density.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.fourier_entropy_batched","title":"fourier_entropy_batched","text":"<pre><code>fourier_entropy_batched(\n    x: Tensor, bins_list: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute fourier_entropy for multiple bins values at once.</p> <p>Computes FFT and PSD once, then returns entropy for each bins value. Note: The bins parameter is not actually used in the tsfresh implementation (it's always spectral entropy), so this returns the same value for all bins.</p> <p>Args:     x: Input tensor of shape (N, B, S)     bins_list: List of bins values (not actually used in computation)</p> <p>Returns:     Tensor of shape (len(bins_list), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.lempel_ziv_complexity","title":"lempel_ziv_complexity","text":"<pre><code>lempel_ziv_complexity(x: Tensor, bins: int = 2) -&gt; Tensor\n</code></pre> <p>Lempel-Ziv complexity approximation (optimized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.cid_ce","title":"cid_ce","text":"<pre><code>cid_ce(x: Tensor, normalize: bool = True) -&gt; Tensor\n</code></pre> <p>Complexity-invariant distance.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.approximate_entropy","title":"approximate_entropy","text":"<pre><code>approximate_entropy(\n    x: Tensor, m: int = 2, r: float = 0.3\n) -&gt; Tensor\n</code></pre> <p>Approximate entropy of the time series.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_entropy_complexity.sample_entropy","title":"sample_entropy","text":"<pre><code>sample_entropy(\n    x: Tensor, m: int = 2, r: float = 0.3\n) -&gt; Tensor\n</code></pre> <p>Sample entropy of the time series.</p>"},{"location":"api/torch-feature-calculators/#frequency-domain","title":"Frequency Domain","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency","title":"pybasin.ts_torch.calculators.torch_features_frequency","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.fft_coefficient","title":"fft_coefficient","text":"<pre><code>fft_coefficient(\n    x: Tensor, coeff: int = 0, attr: str = \"abs\"\n) -&gt; Tensor\n</code></pre> <p>FFT coefficient attributes.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.fft_aggregated","title":"fft_aggregated","text":"<pre><code>fft_aggregated(\n    x: Tensor, aggtype: str = \"centroid\"\n) -&gt; Tensor\n</code></pre> <p>Aggregated FFT spectral statistics.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.spkt_welch_density","title":"spkt_welch_density","text":"<pre><code>spkt_welch_density(x: Tensor, coeff: int = 0) -&gt; Tensor\n</code></pre> <p>Simplified Welch power spectral density at coefficient.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.cwt_coefficients","title":"cwt_coefficients","text":"<pre><code>cwt_coefficients(\n    x: Tensor,\n    widths: tuple[int, ...] = (2,),\n    coeff: int = 0,\n    w: int = 2,\n) -&gt; Tensor\n</code></pre> <p>CWT coefficients using Ricker wavelet (vectorized).</p> <p>This matches tsfresh's cwt_coefficients interface: - widths: tuple of scale values to compute CWT for - coeff: coefficient index to extract from the convolution result - w: which width from the widths tuple to use for the result</p> <p>Note: This implementation uses a direct Ricker wavelet convolution which differs from tsfresh's pywt.cwt in normalization. Results have the same sign but different scaling. This is acceptable for feature extraction where relative patterns matter.</p> <p>Args:     x: Input tensor of shape (N, B, S)     widths: Tuple of wavelet width (scale) parameters     coeff: Coefficient index to extract     w: Which width from widths to use (must be in widths)</p> <p>Returns:     Tensor of shape (B, S) with the CWT coefficient</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.fft_coefficient_batched","title":"fft_coefficient_batched","text":"<pre><code>fft_coefficient_batched(\n    x: Tensor, coeffs: list[int], attr: str = \"abs\"\n) -&gt; Tensor\n</code></pre> <p>Compute FFT coefficients for multiple indices at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     coeffs: List of coefficient indices to extract     attr: Attribute to extract (\"real\", \"imag\", \"abs\", \"angle\")</p> <p>Returns:     Tensor of shape (len(coeffs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.fft_coefficient_all_attrs_batched","title":"fft_coefficient_all_attrs_batched","text":"<pre><code>fft_coefficient_all_attrs_batched(\n    x: Tensor, coeffs: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute all FFT attributes for multiple coefficients at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     coeffs: List of coefficient indices</p> <p>Returns:     Tensor of shape (len(coeffs) * 4, B, S) ordered as:     [coeff0_real, coeff0_imag, coeff0_abs, coeff0_angle, coeff1_real, ...]</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.fft_aggregated_batched","title":"fft_aggregated_batched","text":"<pre><code>fft_aggregated_batched(\n    x: Tensor, aggtypes: list[str]\n) -&gt; Tensor\n</code></pre> <p>Compute fft_aggregated for all aggregation types at once.</p> <p>Computes FFT and PSD once, then returns all requested aggregations.</p> <p>Args:     x: Input tensor of shape (N, B, S)     aggtypes: List of aggregation types (\"centroid\", \"variance\", \"skew\", \"kurtosis\")</p> <p>Returns:     Tensor of shape (len(aggtypes), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.spkt_welch_density_batched","title":"spkt_welch_density_batched","text":"<pre><code>spkt_welch_density_batched(\n    x: Tensor, coeffs: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute spkt_welch_density for multiple coefficient indices at once.</p> <p>Computes Welch PSD once, then extracts multiple coefficients.</p> <p>Args:     x: Input tensor of shape (N, B, S)     coeffs: List of coefficient indices to extract</p> <p>Returns:     Tensor of shape (len(coeffs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.cwt_coefficients_batched","title":"cwt_coefficients_batched","text":"<pre><code>cwt_coefficients_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute CWT coefficients for all parameter combinations at once (GPU-optimized).</p> <p>This function computes CWT once per unique width value, then uses vectorized advanced indexing to extract all requested coefficients in a single operation. The extraction step is fully vectorized with no Python loops.</p> <p>Note: This implementation uses a direct Ricker wavelet convolution which differs from tsfresh's pywt.cwt in normalization. Results have the same sign but different scaling.</p> <p>Args:     x: Input tensor of shape (N, B, S)     params: List of parameter dicts, each with keys:         - \"widths\": tuple of int (e.g., (2, 5, 10, 20))         - \"coeff\": int (coefficient index to extract)         - \"w\": int (which width from widths to use)</p> <p>Returns:     Tensor of shape (len(params), B, S) with CWT coefficient for each param set</p> <p>Example:     params = [         {\"widths\": (2, 5, 10, 20), \"coeff\": c, \"w\": w}         for c in range(15) for w in (2, 5, 10, 20)     ]     result = cwt_coefficients_batched(x, params)  # shape: (60, B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_frequency.spectral_frequency_ratio","title":"spectral_frequency_ratio","text":"<pre><code>spectral_frequency_ratio(x: Tensor) -&gt; Tensor\n</code></pre> <p>Compute the ratio of 2nd to 1st dominant frequency.</p> <p>This feature is critical for distinguishing period-doubling bifurcations: - Period-1 limit cycle: ratio \u2248 2.0 (2nd peak is harmonic at 2f) - Period-2 limit cycle: ratio \u2248 0.5 (2nd peak is subharmonic at f/2) - Period-3 limit cycle: ratio \u2248 0.33 (2nd peak is subharmonic at f/3)</p> <p>The function finds the two highest peaks in the power spectrum and returns the ratio of the 2nd dominant frequency to the 1st dominant frequency.</p> <p>Args:     x: Input tensor of shape (N, B, S) where N is timesteps, B is batch, S is states.</p> <p>Returns:     Tensor of shape (B, S) with the frequency ratio. Returns 0 if only one peak found.</p>"},{"location":"api/torch-feature-calculators/#trend-regression","title":"Trend / Regression","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend","title":"pybasin.ts_torch.calculators.torch_features_trend","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.linear_trend","title":"linear_trend","text":"<pre><code>linear_trend(x: Tensor, attr: str = 'slope') -&gt; Tensor\n</code></pre> <p>Linear regression trend attributes.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.linear_trend_timewise","title":"linear_trend_timewise","text":"<pre><code>linear_trend_timewise(\n    x: Tensor, attr: str = \"slope\"\n) -&gt; Tensor\n</code></pre> <p>Linear trend (same as linear_trend for our use case).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.agg_linear_trend","title":"agg_linear_trend","text":"<pre><code>agg_linear_trend(\n    x: Tensor,\n    chunk_size: int = 10,\n    f_agg: str = \"mean\",\n    attr: str = \"slope\",\n) -&gt; Tensor\n</code></pre> <p>Linear trend on aggregated chunks.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.ar_coefficient","title":"ar_coefficient","text":"<pre><code>ar_coefficient(\n    x: Tensor, k: int = 1, coeff: int = 0\n) -&gt; Tensor\n</code></pre> <p>AR model coefficients using Yule-Walker (optimized with FFT autocorrelation).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.augmented_dickey_fuller","title":"augmented_dickey_fuller","text":"<pre><code>augmented_dickey_fuller(\n    x: Tensor, attr: str = \"teststat\"\n) -&gt; Tensor\n</code></pre> <p>Simplified Augmented Dickey-Fuller test (vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.linear_trend_batched","title":"linear_trend_batched","text":"<pre><code>linear_trend_batched(x: Tensor, attrs: list[str]) -&gt; Tensor\n</code></pre> <p>Compute linear regression trend for multiple attributes at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     attrs: List of attributes (\"slope\", \"intercept\", \"rvalue\", \"pvalue\", \"stderr\")</p> <p>Returns:     Tensor of shape (len(attrs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.agg_linear_trend_batched","title":"agg_linear_trend_batched","text":"<pre><code>agg_linear_trend_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute agg_linear_trend for multiple parameter combinations at once.</p> <p>Groups by (chunk_size, f_agg) to minimize redundant chunk aggregation, then computes all 4 trend attributes at once per group.</p> <p>Args:     x: Input tensor of shape (N, B, S)     params: List of parameter dicts, each with keys:         - \"chunk_size\": int (chunk size for aggregation, e.g., 5, 10, 50)         - \"f_agg\": str (\"mean\", \"var\", \"min\", \"max\")         - \"attr\": str (\"slope\", \"intercept\", \"rvalue\", \"stderr\")</p> <p>Returns:     Tensor of shape (len(params), B, S)</p> <p>Example:     params = [         {\"chunk_size\": 5, \"f_agg\": \"mean\", \"attr\": \"slope\"},         {\"chunk_size\": 5, \"f_agg\": \"mean\", \"attr\": \"intercept\"},         {\"chunk_size\": 5, \"f_agg\": \"var\", \"attr\": \"slope\"},         ...     ]     result = agg_linear_trend_batched(x, params)  # shape: (48, B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.ar_coefficient_batched","title":"ar_coefficient_batched","text":"<pre><code>ar_coefficient_batched(\n    x: Tensor, k: int, coeffs: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute AR model coefficients for multiple coeff indices at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     k: AR model order     coeffs: List of coefficient indices to return</p> <p>Returns:     Tensor of shape (len(coeffs), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_trend.augmented_dickey_fuller_batched","title":"augmented_dickey_fuller_batched","text":"<pre><code>augmented_dickey_fuller_batched(\n    x: Tensor, attrs: list[str]\n) -&gt; Tensor\n</code></pre> <p>Compute augmented_dickey_fuller for multiple attributes at once.</p> <p>Computes ADF test once and returns all requested attributes.</p> <p>Args:     x: Input tensor of shape (N, B, S)     attrs: List of attributes (\"teststat\", \"pvalue\", \"usedlag\")</p> <p>Returns:     Tensor of shape (len(attrs), B, S)</p>"},{"location":"api/torch-feature-calculators/#reoccurrence","title":"Reoccurrence","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance","title":"pybasin.ts_torch.calculators.torch_features_reocurrance","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance.percentage_of_reoccurring_datapoints_to_all_datapoints","title":"percentage_of_reoccurring_datapoints_to_all_datapoints","text":"<pre><code>percentage_of_reoccurring_datapoints_to_all_datapoints(\n    x: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Percentage of unique values that appear more than once (fully vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance.percentage_of_reoccurring_values_to_all_values","title":"percentage_of_reoccurring_values_to_all_values","text":"<pre><code>percentage_of_reoccurring_values_to_all_values(\n    x: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Percentage of datapoints that are reoccurring (optimized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance.sum_of_reoccurring_data_points","title":"sum_of_reoccurring_data_points","text":"<pre><code>sum_of_reoccurring_data_points(x: Tensor) -&gt; Tensor\n</code></pre> <p>Sum of values that appear more than once (optimized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance.sum_of_reoccurring_values","title":"sum_of_reoccurring_values","text":"<pre><code>sum_of_reoccurring_values(x: Tensor) -&gt; Tensor\n</code></pre> <p>Sum of unique values that appear more than once (optimized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_reocurrance.ratio_value_number_to_time_series_length","title":"ratio_value_number_to_time_series_length","text":"<pre><code>ratio_value_number_to_time_series_length(\n    x: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Ratio of unique values to length (optimized).</p>"},{"location":"api/torch-feature-calculators/#advanced","title":"Advanced","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced","title":"pybasin.ts_torch.calculators.torch_features_advanced","text":"<p>Advanced feature calculators that don't fit cleanly into other categories.</p> <p>These features use specialized algorithms or test unique properties: - benford_correlation: Tests first-digit distribution - c3: Non-linearity measure using triple products - energy_ratio_by_chunks: Temporal energy distribution - time_reversal_asymmetry_statistic: Temporal asymmetry measure</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.benford_correlation","title":"benford_correlation","text":"<pre><code>benford_correlation(x: Tensor) -&gt; Tensor\n</code></pre> <p>Correlation with Benford's law distribution (vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.c3","title":"c3","text":"<pre><code>c3(x: Tensor, lag: int) -&gt; Tensor\n</code></pre> <p>Non-linearity measure: mean(x[t] * x[t+lag] * x[t+2*lag]).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.energy_ratio_by_chunks","title":"energy_ratio_by_chunks","text":"<pre><code>energy_ratio_by_chunks(\n    x: Tensor,\n    num_segments: int = 10,\n    segment_focus: int = 0,\n) -&gt; Tensor\n</code></pre> <p>Energy ratio of a segment.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.time_reversal_asymmetry_statistic","title":"time_reversal_asymmetry_statistic","text":"<pre><code>time_reversal_asymmetry_statistic(\n    x: Tensor, lag: int\n) -&gt; Tensor\n</code></pre> <p>Time reversal asymmetry statistic.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.energy_ratio_by_chunks_batched","title":"energy_ratio_by_chunks_batched","text":"<pre><code>energy_ratio_by_chunks_batched(\n    x: Tensor, num_segments: int, segment_focuses: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute energy ratio for multiple segment focuses at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     num_segments: Number of segments to divide the series into     segment_focuses: List of segment indices to focus on</p> <p>Returns:     Tensor of shape (len(segment_focuses), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.c3_batched","title":"c3_batched","text":"<pre><code>c3_batched(x: Tensor, lags: list[int]) -&gt; Tensor\n</code></pre> <p>Compute c3 for multiple lag values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     lags: List of lag values</p> <p>Returns:     Tensor of shape (len(lags), B, S)</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_advanced.time_reversal_asymmetry_statistic_batched","title":"time_reversal_asymmetry_statistic_batched","text":"<pre><code>time_reversal_asymmetry_statistic_batched(\n    x: Tensor, lags: list[int]\n) -&gt; Tensor\n</code></pre> <p>Compute time_reversal_asymmetry_statistic for multiple lag values at once.</p> <p>Args:     x: Input tensor of shape (N, B, S)     lags: List of lag values</p> <p>Returns:     Tensor of shape (len(lags), B, S)</p>"},{"location":"api/torch-feature-calculators/#dynamical-systems","title":"Dynamical Systems","text":"<p>options: show_root_heading: false heading_level: 3</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical","title":"pybasin.ts_torch.calculators.torch_features_dynamical","text":"<p>Dynamical systems feature calculators for time series.</p> <p>All feature functions follow a consistent tensor shape convention: - Input: (N, B, S) where N=timesteps, B=batch size, S=state variables - Output: (B, S) for scalar features, or (B, S, K) for multi-valued features where K is the number of values</p> <p>Features are computed along the time dimension (dim=0), preserving batch and state dimensions.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical-functions","title":"Functions","text":""},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.lyapunov_r","title":"lyapunov_r","text":"<pre><code>lyapunov_r(\n    x: Tensor,\n    emb_dim: int = 10,\n    lag: int = 1,\n    trajectory_len: int = 20,\n    tau: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute largest Lyapunov exponent using Rosenstein algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>emb_dim</code> <code>int</code> <p>Embedding dimension for phase space reconstruction. Default is 10.</p> <code>10</code> <code>lag</code> <code>int</code> <p>Lag for delay embedding. Default is 1.</p> <code>1</code> <code>trajectory_len</code> <code>int</code> <p>Number of steps to follow divergence. Default is 20.</p> <code>20</code> <code>tau</code> <code>float</code> <p>Time step size for normalization. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the largest Lyapunov exponent for each state of each batch.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.lyapunov_e","title":"lyapunov_e","text":"<pre><code>lyapunov_e(\n    x: Tensor,\n    emb_dim: int = 10,\n    matrix_dim: int = 4,\n    min_nb: int = 8,\n    min_tsep: int = 0,\n    tau: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute multiple Lyapunov exponents using Eckmann algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>emb_dim</code> <code>int</code> <p>Embedding dimension for phase space reconstruction. Default is 10.</p> <code>10</code> <code>matrix_dim</code> <code>int</code> <p>Matrix dimension for Jacobian estimation (number of exponents to compute). Default is 4.</p> <code>4</code> <code>min_nb</code> <code>int</code> <p>Minimal number of neighbors required. Default is 8.</p> <code>8</code> <code>min_tsep</code> <code>int</code> <p>Minimal temporal separation between neighbors. Default is 0.</p> <code>0</code> <code>tau</code> <code>float</code> <p>Time step size for normalization. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S, matrix_dim) containing the Lyapunov exponents. The third dimension contains matrix_dim exponents sorted from largest to smallest.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.correlation_dimension","title":"correlation_dimension","text":"<pre><code>correlation_dimension(\n    x: Tensor,\n    emb_dim: int = 4,\n    lag: int = 1,\n    n_rvals: int = 50,\n) -&gt; Tensor\n</code></pre> <p>Compute correlation dimension using Grassberger-Procaccia algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input time series tensor of shape (N, B, S) where N=timesteps, B=batch size, S=states.</p> required <code>emb_dim</code> <code>int</code> <p>Embedding dimension for phase space reconstruction. Default is 4.</p> <code>4</code> <code>lag</code> <code>int</code> <p>Lag for delay embedding. Default is 1.</p> <code>1</code> <code>n_rvals</code> <code>int</code> <p>Number of radius values to use in correlation integral. Default is 50.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of shape (B, S) containing the correlation dimension for each state of each batch.</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.friedrich_coefficients","title":"friedrich_coefficients","text":"<pre><code>friedrich_coefficients(\n    x: Tensor, m: int = 3, r: float = 30.0, coeff: int = 0\n) -&gt; Tensor\n</code></pre> <p>Coefficients of polynomial fit to velocity vs position (fully batch vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.max_langevin_fixed_point","title":"max_langevin_fixed_point","text":"<pre><code>max_langevin_fixed_point(\n    x: Tensor, r: float = 3, m: int = 30\n) -&gt; Tensor\n</code></pre> <p>Maximum fixed point of Langevin model (fully batch vectorized).</p>"},{"location":"api/torch-feature-calculators/#pybasin.ts_torch.calculators.torch_features_dynamical.friedrich_coefficients_batched","title":"friedrich_coefficients_batched","text":"<pre><code>friedrich_coefficients_batched(\n    x: Tensor, params: list[dict]\n) -&gt; Tensor\n</code></pre> <p>Compute friedrich_coefficients for multiple coeff values at once.</p> <p>Groups by (m, r) combinations and computes polynomial fit once per group, then extracts all requested coefficients.</p> <p>Args:     x: Input tensor of shape (N, B, S)     params: List of parameter dicts, each with keys:         - \"m\": int (polynomial degree)         - \"r\": float (not used in computation, kept for API compatibility)         - \"coeff\": int (coefficient index to extract)</p> <p>Returns:     Tensor of shape (len(params), B, S)</p>"},{"location":"benchmarks/basin-stability-estimator/","title":"Basin Stability Estimator","text":""},{"location":"benchmarks/basin-stability-estimator/#interactive-flame-graph","title":"Interactive Flame Graph","text":"<p>View the profiling results in speedscope:</p> <p>Open in speedscope</p>"},{"location":"benchmarks/basin-stability-estimator/#example-run","title":"Example Run","text":"<p>The pendulum case study with 10,000 initial conditions using pyBasin defaults:</p> <pre><code>BASIN STABILITY ESTIMATION COMPLETE\nTotal time: 17.3210s\nTiming Breakdown:\n  1. Sampling:             0.0629s  (  0.4%)\n  2. Integration:         12.1686s  ( 70.3%)\n  3. Solution/Amps:        0.0571s  (  0.3%)\n  4. Features:             0.4379s  (  2.5%)\n  5. Filtering:            0.0047s  (  0.0%)\n  6. Classification:       4.5817s  ( 26.5%)\n  7. BS Computation:       0.0073s  (  0.0%)\n</code></pre>"},{"location":"benchmarks/basin-stability-estimator/#expensive-steps","title":"Expensive Steps","text":"<p>The three most computationally expensive steps are:</p> <ol> <li> <p>ODE Integration (~70%) \u2014 Solving the differential equations for all initial conditions. Uses JAX/Diffrax by default with GPU acceleration.</p> </li> <li> <p>Classification (~26%) \u2014 HDBSCAN clustering with auto-tuning enabled, followed by KMeans to assign noise points to the nearest cluster.</p> </li> <li> <p>Feature Extraction (~2.5%) \u2014 Extracts time series features from trajectories. The default <code>TorchFeatureExtractor</code> uses these statistical features: <code>median</code>, <code>mean</code>, <code>standard_deviation</code>, <code>variance</code>, <code>root_mean_square</code>, <code>maximum</code>, <code>absolute_maximum</code>, <code>minimum</code>, <code>delta</code>, <code>log_delta</code>.</p> </li> </ol> <p>Feature Complexity</p> <p>More complex features (e.g., entropy, autocorrelation, frequency domain) can significantly increase extraction time. The default minimal set is chosen for speed while maintaining classification accuracy.</p>"},{"location":"benchmarks/basin-stability-estimator/#profiling-setup","title":"Profiling Setup","text":"<p>The profile was generated using Austin, a frame stack sampler for CPython.</p> <p>The pendulum case study is run using pyBasin defaults\u2014only the ODE system and area of interest (sampler bounds) are defined. All other components (solver, feature extractor, predictor) use their default configurations.</p> <p>To generate a new profile:</p> <pre><code>./scripts/generate_profiling.sh\n</code></pre> <p>This runs the pendulum case study and outputs <code>profile.speedscope.json</code> for visualization in speedscope.</p>"},{"location":"benchmarks/end-to-end/","title":"End-to-End Performance","text":"<p>This benchmark compares the full basin stability estimation pipeline across MATLAB and Python implementations.</p>"},{"location":"benchmarks/end-to-end/#methodology","title":"Methodology","text":"<p>All implementations use the same:</p> <ul> <li>ODE system: Damped driven pendulum</li> <li>Parameters: <code>\u03b1=0.1</code>, <code>T=0.5</code>, <code>K=1.0</code></li> <li>Integration: <code>t_span=(0, 1000)</code>, <code>rtol=1e-8</code>, <code>atol=1e-6</code></li> <li>Sample sizes: 100 to 100,000 initial conditions</li> </ul>"},{"location":"benchmarks/end-to-end/#implementations-compared","title":"Implementations Compared","text":"Implementation Platform Parallelization MATLAB bSTAB-M CPU MATLAB <code>parfor</code> pyBasin + JAX CPU Vectorized (<code>vmap</code>) pyBasin + JAX CUDA GPU Vectorized (<code>vmap</code>)"},{"location":"benchmarks/end-to-end/#results","title":"Results","text":""},{"location":"benchmarks/end-to-end/#performance-comparison","title":"Performance Comparison","text":"N MATLAB (s) Python CPU (s) Python CUDA (s) CPU vs MATLAB GPU vs MATLAB 100 0.76 1.30 12.86 0.6x 0.1x 200 1.02 1.50 12.87 0.7x 0.1x 500 1.90 1.62 12.98 1.2x 0.1x 1,000 3.27 2.00 12.05 1.6x 0.3x 2,000 6.29 2.72 12.32 2.3x 0.5x 5,000 15.90 5.73 12.82 2.8x 1.2x 10,000 31.01 10.52 12.64 2.9x 2.5x 20,000 62.73 20.94 12.27 3.0x 5.1x 50,000 153.04 30.07 12.40 5.1x 12.3x 100,000 309.07 62.94 12.57 4.9x 24.6x"},{"location":"benchmarks/end-to-end/#scaling-analysis","title":"Scaling Analysis","text":"Implementation Scaling Exponent \u03b1 R\u00b2 MATLAB O(N) 0.90 \u00b1 0.06 0.992 Python CPU O(N^0.59) 0.59 \u00b1 0.10 0.942 Python CUDA O(1) -0.00 \u00b1 0.01 0.168"},{"location":"benchmarks/end-to-end/#comparison-plot","title":"Comparison Plot","text":""},{"location":"benchmarks/end-to-end/#scaling-plot-log-log","title":"Scaling Plot (Log-Log)","text":""},{"location":"benchmarks/end-to-end/#key-findings","title":"Key Findings","text":"<ol> <li>Python CPU becomes 3-5\u00d7 faster than MATLAB for N &gt; 5,000</li> <li>Python CUDA achieves near-constant time (~12s) regardless of N due to GPU parallelization</li> <li>At N=100,000: GPU is ~25\u00d7 faster than MATLAB (as long as data fits in GPU memory)</li> </ol>"},{"location":"benchmarks/end-to-end/#hardware","title":"Hardware","text":"<p>Benchmarks run on:</p> <ul> <li>CPU: Intel Core Ultra 9 275HX</li> <li>GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU (12 GB VRAM)</li> </ul>"},{"location":"benchmarks/feature-extraction/","title":"Feature Extraction Benchmarks","text":"<p>Documentation in Progress</p> <p>This page is under construction.</p>"},{"location":"benchmarks/feature-extraction/#implementations-compared","title":"Implementations Compared","text":"<ul> <li>tsfresh (reference, CPU)</li> <li>TorchFeatureExtractor (CPU parallel)</li> <li>TorchFeatureExtractor (CUDA GPU)</li> </ul>"},{"location":"benchmarks/feature-extraction/#results","title":"Results","text":"Backend Mode Device 10k batches time tsfresh parallel cpu 34,465 ms PyTorch parallel cpu 1,734 ms PyTorch sequential cpu 3,464 ms PyTorch gpu cuda 7,702 ms"},{"location":"benchmarks/feature-extraction/#key-finding","title":"Key Finding","text":"<p>PyTorch CPU parallel is ~20x faster than tsfresh.</p>"},{"location":"benchmarks/feature-extraction/#feature-accuracy","title":"Feature Accuracy","text":"<p>All features validated against tsfresh reference values.</p>"},{"location":"benchmarks/overview/","title":"Benchmarks Overview","text":"<p>This section documents pyBasin's performance characteristics and compares it against the original MATLAB implementation (bSTAB-M).</p>"},{"location":"benchmarks/overview/#test-hardware","title":"Test Hardware","text":"<ul> <li>CPU: Intel Core Ultra 9 275HX</li> <li>GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU (12 GB VRAM)</li> </ul>"},{"location":"benchmarks/overview/#key-findings","title":"Key Findings","text":"<ul> <li>GPU delivers massive speedups at scale: At N=100k samples, pyBasin on GPU (Diffrax) is ~25\u00d7 faster than MATLAB for end-to-end basin stability estimation (12s vs 309s)</li> <li>Near-constant GPU time: JAX/Diffrax on CUDA maintains ~11-12s integration time regardless of sample size, enabling large-scale studies without linear time scaling</li> <li>CPU competitive for smaller workloads: pyBasin CPU (Diffrax) is ~1.2\u00d7 faster than MATLAB at N=5k and scales to 3-5\u00d7 faster at N&gt;5k</li> <li>GPU overhead at small N: For sample sizes below ~10k, CPU solvers outperform GPU due to data transfer and kernel launch overhead</li> <li>JAX/Diffrax is the recommended solver: Best performance on both CPU and GPU, plus unique support for per-trajectory event-based termination (critical for unbounded systems)</li> <li>Integration dominates runtime: ODE integration accounts for ~70% of total estimation time, classification ~26%, and feature extraction ~2.5%. Solver selection has the most impact on performance.</li> </ul>"},{"location":"benchmarks/overview/#benchmark-pages","title":"Benchmark Pages","text":""},{"location":"benchmarks/overview/#basin-stability-estimator","title":"Basin Stability Estimator","text":"<p>Detailed breakdown of the full estimation pipeline showing how time is distributed across each step (sampling, integration, feature extraction, classification). Includes an interactive flame graph for profiling analysis.</p>"},{"location":"benchmarks/overview/#end-to-end-performance","title":"End-to-End Performance","text":"<p>Compares the complete basin stability estimation workflow between pyBasin and MATLAB bSTAB-M across different sample sizes. Demonstrates pyBasin's scalability advantage, especially on GPU.</p>"},{"location":"benchmarks/overview/#solver-comparison","title":"Solver Comparison","text":"<p>Evaluates different ODE solver backends (JAX/Diffrax, PyTorch/torchdiffeq, SciPy) across CPU and GPU. Shows how JAX achieves near-constant integration time on GPU regardless of sample size.</p>"},{"location":"benchmarks/overview/#feature-extraction","title":"Feature Extraction","text":"<p>Compares feature extraction performance between pyBasin's PyTorch-based implementation and tsfresh. Analyzes the trade-offs between feature complexity and extraction speed.</p>"},{"location":"benchmarks/solvers/","title":"Solver Comparison","text":"<p>This benchmark compares ODE solver performance across different Python backends and MATLAB.</p>"},{"location":"benchmarks/solvers/#test-configuration","title":"Test Configuration","text":"<ul> <li>ODE: Driven damped pendulum</li> <li>t_span: (0, 1000)</li> <li>Tolerances: rtol=1e-8, atol=1e-6</li> <li>Sample sizes: 5,000 / 10,000 / 100,000 initial conditions</li> </ul>"},{"location":"benchmarks/solvers/#solvers-tested","title":"Solvers Tested","text":"Solver Backend Devices Method MATLAB ode45 MATLAB CPU Dormand-Prince 5(4) JAX/Diffrax JAX CPU, CUDA Dormand-Prince 5(4) torchdiffeq PyTorch CPU, CUDA Dormand-Prince 5(4) torchode PyTorch CUDA Dormand-Prince 5(4) <p>TorchOde Performance Issues</p> <p>TorchOde was excluded from CPU benchmarks due to severe performance issues observed in previous runs. Additionally, it performs very poorly at larger N values (e.g., ~1133s at N=100k vs ~11s for JAX/Diffrax), indicating it is not properly optimized for GPU batch processing in this use case.</p>"},{"location":"benchmarks/solvers/#results-by-sample-size","title":"Results by Sample Size","text":""},{"location":"benchmarks/solvers/#n-5000","title":"N = 5,000","text":"Solver Device Time (s) Std Dev vs MATLAB JAX/Diffrax CPU 4.80 \u00b10.33 1.21x MATLAB ode45 CPU 5.83 \u00b10.72 1.00x torchdiffeq CPU 7.36 \u00b10.35 0.79x JAX/Diffrax CUDA 12.30 \u00b10.30 0.47x torchode CUDA 29.13 \u00b10.27 0.20x torchdiffeq CUDA 36.90 \u00b10.14 0.16x"},{"location":"benchmarks/solvers/#n-10000","title":"N = 10,000","text":"Solver Device Time (s) Std Dev vs MATLAB torchdiffeq CPU 9.22 \u00b10.19 1.26x JAX/Diffrax CPU 9.59 \u00b10.25 1.21x MATLAB ode45 CPU 11.64 \u00b10.40 1.00x JAX/Diffrax CUDA 12.44 \u00b10.36 0.94x torchode CUDA 30.14 \u00b10.31 0.39x torchdiffeq CUDA 35.94 \u00b11.24 0.32x"},{"location":"benchmarks/solvers/#n-100000","title":"N = 100,000","text":"Solver Device Time (s) Std Dev vs MATLAB JAX/Diffrax CUDA 12.04 \u00b10.38 8.49x torchdiffeq CUDA 31.24 \u00b10.54 3.27x torchdiffeq CPU 31.92 \u00b11.80 3.20x JAX/Diffrax CPU 60.67 \u00b13.01 1.69x MATLAB ode45 CPU 102.26 \u00b11.59 1.00x torchode CUDA 364.54 \u00b1122.05 0.28x"},{"location":"benchmarks/solvers/#comparison-plots","title":"Comparison Plots","text":""},{"location":"benchmarks/solvers/#n-5000_1","title":"N = 5,000","text":""},{"location":"benchmarks/solvers/#n-10000_1","title":"N = 10,000","text":""},{"location":"benchmarks/solvers/#n-100000_1","title":"N = 100,000","text":"<p>Note: torchode (CUDA) time is divided by 3 in the N=100,000 plot to improve readability.</p> <p>Note: the first round of torchode tooks 2x longer than any following run inflating the average. The real average duration is around 310 seconds</p>"},{"location":"benchmarks/solvers/#key-findings","title":"Key Findings","text":"<ol> <li>JAX/Diffrax (CPU) is the fastest option for small to medium N (5k-10k samples)</li> <li>JAX/Diffrax (CUDA) achieves near-constant time (~11.5s) regardless of N, making it 8.9x faster than MATLAB at N=100k</li> <li>torchdiffeq scales reasonably well on both CPU and CUDA</li> <li>GPU acceleration only provides significant benefit at large sample sizes (N \u2265 100k)</li> <li>At small N, GPU overhead makes CPU solvers faster</li> </ol>"},{"location":"benchmarks/solvers/#recommendations","title":"Recommendations","text":"<p>JAX/Diffrax should be the default solver for pyBasin. When a GPU is available, it delivers unmatched performance with near-constant integration time regardless of sample size\u2014making it the clear choice for any workload.</p> <p>Additionally, JAX/Diffrax is the only solver that supports event-based termination with individual trajectory stopping. This is critical for systems with unbounded trajectories (e.g., Lorenz \"broken butterfly\"), where some initial conditions diverge to infinity. With JAX events, each trajectory stops independently when it exceeds a threshold, while bounded trajectories continue integrating. Other solvers either stop all trajectories simultaneously or require workarounds like zero masking. See the Handling Unbounded Trajectories guide for details.</p> <p>For CPU-only systems, the choice depends on scale. At smaller sample sizes (N \u2264 10k), JAX/Diffrax on CPU is the fastest option. However, at larger scales (N = 100k), torchdiffeq on CPU offers a meaningful ~2x improvement over JAX/Diffrax CPU (32s vs 64s) . While this difference is negligible for single runs, it becomes significant for parameter studies that require evaluating many ODE configurations. A study testing 50 parameter combinations at N=100k would save roughly 26 minutes by using torchdiffeq instead of JAX/Diffrax on CPU\u2014though JAX/Diffrax on GPU would complete the same workload in just 10 minutes.</p>"},{"location":"benchmarks/solvers/#hardware","title":"Hardware","text":"<p>Benchmarks run on:</p> <ul> <li>CPU: Intel Core Ultra 9 275HX</li> <li>GPU: NVIDIA GeForce RTX 5070 Ti Laptop GPU (12 GB VRAM)</li> </ul>"},{"location":"case-studies/duffing/","title":"Duffing Oscillator","text":""},{"location":"case-studies/duffing/#system-description","title":"System Description","text":"<p>Duffing oscillator with cubic nonlinearity:</p> \\[ \\begin{aligned} \\dot{x} &amp;= v \\\\ \\dot{v} &amp;= -\\delta v - k_3 x^3 + A \\cos(t) \\end{aligned} \\]"},{"location":"case-studies/duffing/#system-parameters","title":"System Parameters","text":"Parameter Symbol Value Damping coefficient \\(\\delta\\) 0.08 Cubic stiffness \\(k_3\\) 1.0 Forcing amplitude \\(A\\) 0.2"},{"location":"case-studies/duffing/#sampling","title":"Sampling","text":"<ul> <li>Dimension: \\(D = 2\\)</li> <li>Sample size: \\(N = 10000\\)</li> <li>Distribution: \\(\\rho\\) = Uniform</li> <li>Region of interest: \\(\\mathcal{Q}(x, v) : [-1, 1] \\times [-0.5, 1]\\)</li> </ul>"},{"location":"case-studies/duffing/#solver","title":"Solver","text":"Setting Value Method Dopri5 (Diffrax) Time span \\([0, 1000]\\) Steps 5000 (\\(f_s\\) = 5 Hz) Relative tolerance 1e-08 Absolute tolerance 1e-06"},{"location":"case-studies/duffing/#feature-extraction","title":"Feature Extraction","text":"<p>Maximum and standard deviation of position:</p> <ul> <li>States: \\(x\\) (state 0)</li> <li>Formula: \\([\\max(x), \\sigma(x)]\\)</li> <li>Transient cutoff: \\(t^* = 900.0\\)</li> </ul>"},{"location":"case-studies/duffing/#clustering","title":"Clustering","text":"<ul> <li>Method: k-NN (k=1)</li> <li>Template ICs:</li> <li>y1: \\([-0.21, 0.02]\\) \u2014 Period-1 limit cycle (small amplitude)</li> <li>y2: \\([1.05, 0.77]\\) \u2014 Period-1 limit cycle (large amplitude)</li> <li>y3: \\([-0.67, 0.02]\\) \u2014 Period-2 limit cycle</li> <li>y4: \\([-0.46, 0.30]\\) \u2014 Period-2 limit cycle (symmetric)</li> <li>y5: \\([-0.43, 0.12]\\) \u2014 Period-3 limit cycle</li> </ul>"},{"location":"case-studies/duffing/#reproduction-code","title":"Reproduction Code","text":""},{"location":"case-studies/duffing/#setup","title":"Setup","text":"<pre><code>def setup_duffing_oscillator_system() -&gt; SetupProperties:\n    n = 10000\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up Duffing oscillator system on device: {device}\")\n\n    params: DuffingParams = {\"delta\": 0.08, \"k3\": 1, \"A\": 0.2}\n    ode_system = DuffingJaxODE(params)\n\n    sampler = UniformRandomSampler(min_limits=[-1, -0.5], max_limits=[1, 1], device=device)\n\n    solver = JaxSolver(\n        time_span=(0, 1000),\n        n_steps=5000,\n        device=device,\n        rtol=1e-8,\n        atol=1e-6,\n        cache_dir=\".pybasin_cache/duffing\",\n    )\n\n    feature_extractor = TorchFeatureExtractor(\n        time_steady=900.0,\n        normalize=False,\n        features=None,\n        features_per_state={\n            0: {\"maximum\": None, \"standard_deviation\": None},\n        },\n    )\n\n    classifier_initial_conditions = [\n        [-0.21, 0.02],\n        [1.05, 0.77],\n        [-0.67, 0.02],\n        [-0.46, 0.30],\n        [-0.43, 0.12],\n    ]\n\n    classifier_labels = [\n        \"y1\",\n        \"y2\",\n        \"y3\",\n        \"y4\",\n        \"y5\",\n    ]\n\n    knn = KNeighborsClassifier(n_neighbors=1)\n\n    template_integrator = TemplateIntegrator(\n        template_y0=classifier_initial_conditions,\n        labels=classifier_labels,\n        ode_params=params,\n    )\n\n    return {\n        \"n\": n,\n        \"ode_system\": ode_system,\n        \"sampler\": sampler,\n        \"solver\": solver,\n        \"feature_extractor\": feature_extractor,\n        \"estimator\": knn,\n        \"template_integrator\": template_integrator,\n    }\n</code></pre>"},{"location":"case-studies/duffing/#main-estimation","title":"Main Estimation","text":"<pre><code>def main():\n    setup = setup_duffing_oscillator_system()\n\n    bse = BasinStabilityEstimator(\n        n=setup[\"n\"],\n        ode_system=setup[\"ode_system\"],\n        sampler=setup[\"sampler\"],\n        solver=setup.get(\"solver\"),\n        feature_extractor=setup.get(\"feature_extractor\"),\n        predictor=setup.get(\"estimator\"),\n        template_integrator=setup.get(\"template_integrator\"),\n        save_to=\"results\",\n        feature_selector=None,\n    )\n\n    bse.estimate_bs()\n\n    return bse\n</code></pre>"},{"location":"case-studies/duffing/#case-1-baseline-results-supervised","title":"Case 1: Baseline Results (Supervised)","text":""},{"location":"case-studies/duffing/#comparison-with-matlab-bstab","title":"Comparison with MATLAB bSTAB","text":"<p>Overall Classification Quality:</p> <ul> <li>Macro F1-score: 0.9980</li> <li>Matthews Correlation Coefficient: 0.9977</li> </ul> Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE F1 y1 0.2023 \u00b1 0.0040 0.2027 \u00b1 0.0040 0.9965 y2 0.4950 \u00b1 0.0050 0.4950 \u00b1 0.0050 1.0000 y3 0.0288 \u00b1 0.0017 0.0288 \u00b1 0.0017 1.0000 y4 0.0257 \u00b1 0.0016 0.0257 \u00b1 0.0016 0.9961 y5 0.2482 \u00b1 0.0043 0.2478 \u00b1 0.0043 0.9972"},{"location":"case-studies/duffing/#visualizations","title":"Visualizations","text":""},{"location":"case-studies/duffing/#basin-stability","title":"Basin Stability","text":""},{"location":"case-studies/duffing/#state-space","title":"State Space","text":""},{"location":"case-studies/duffing/#feature-space","title":"Feature Space","text":""},{"location":"case-studies/duffing/#template-trajectories","title":"Template Trajectories","text":""},{"location":"case-studies/duffing/#template-phase-space","title":"Template Phase Space","text":""},{"location":"case-studies/duffing/#case-2-unsupervised-clustering-with-template-relabeling","title":"Case 2: Unsupervised Clustering with Template Relabeling","text":"<p>This case demonstrates unsupervised attractor discovery using DBSCAN clustering, followed by relabeling using KNN template matching to assign meaningful attractor names.</p>"},{"location":"case-studies/duffing/#comparison-with-matlab-bstab_1","title":"Comparison with MATLAB bSTAB","text":"<p>Cluster Quality Metrics:</p> <ul> <li>Clusters found: 5 (expected: 5)</li> <li>Overall agreement: 100.0%</li> <li>Adjusted Rand Index: 1.0000</li> <li>Macro F1-score: 0.9980</li> <li>Matthews Correlation Coefficient: 0.9977</li> </ul> Attractor DBSCAN Purity pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE F1 y1 0 100.0% 0.2023 \u00b1 0.0040 0.2027 \u00b1 0.0040 0.9965 y2 1 100.0% 0.4950 \u00b1 0.0050 0.4950 \u00b1 0.0050 1.0000 y3 4 100.0% 0.0288 \u00b1 0.0017 0.0288 \u00b1 0.0017 1.0000 y4 2 100.0% 0.0257 \u00b1 0.0016 0.0257 \u00b1 0.0016 0.9961 y5 3 100.0% 0.2482 \u00b1 0.0043 0.2478 \u00b1 0.0043 0.9972"},{"location":"case-studies/duffing/#visualizations_1","title":"Visualizations","text":""},{"location":"case-studies/duffing/#basin-stability_1","title":"Basin Stability","text":""},{"location":"case-studies/duffing/#state-space_1","title":"State Space","text":""},{"location":"case-studies/duffing/#feature-space_1","title":"Feature Space","text":""},{"location":"case-studies/friction/","title":"Friction Oscillator","text":""},{"location":"case-studies/friction/#system-description","title":"System Description","text":"<p>Self-excited friction oscillator with Stribeck friction characteristic:</p> \\[ \\begin{aligned} \\dot{x} &amp;= v \\\\ \\dot{v} &amp;= -2\\xi v - \\mu(v_{\\text{rel}}) \\cdot \\text{sgn}(v_{\\text{rel}}) \\end{aligned} \\] <p>where \\(v_{\\text{rel}} = v - v_d\\) and the friction characteristic is:</p> \\[ \\mu(v_{\\text{rel}}) = \\mu_d + (\\mu_{sd} - 1)\\mu_d \\exp\\left(-\\frac{|v_{\\text{rel}}|}{v_0}\\right) + \\mu_v |v_{\\text{rel}}| \\]"},{"location":"case-studies/friction/#system-parameters","title":"System Parameters","text":"Parameter Symbol Value Driving velocity \\(v_d\\) 1.5 Damping ratio \\(\\xi\\) 0.05 Static/dynamic ratio \\(\\mu_{sd}\\) 2.0 Dynamic friction \\(\\mu_d\\) 0.5 Viscous friction \\(\\mu_v\\) 0.0 Reference velocity \\(v_0\\) 0.5"},{"location":"case-studies/friction/#sampling","title":"Sampling","text":"<ul> <li>Dimension: \\(D = 2\\)</li> <li>Sample size: \\(N = 5000\\)</li> <li>Distribution: \\(\\rho\\) = Uniform</li> <li>Region of interest: \\(\\mathcal{Q}(x, v) : [-2, 2] \\times [0, 2]\\)</li> </ul>"},{"location":"case-studies/friction/#solver","title":"Solver","text":"Setting Value Method Dopri5 (Diffrax) Time span \\([0, 500]\\) Steps 500 (\\(f_s\\) = 1 Hz) Relative tolerance 1e-08 Absolute tolerance 1e-06 <p>Implementation detail</p> <p>The JAX implementation uses a \\(\\tanh\\) approximation for the sign function with \\(k_{\\text{smooth}} = 200\\), while MATLAB uses hard switching.</p>"},{"location":"case-studies/friction/#feature-extraction","title":"Feature Extraction","text":"<p>Maximum absolute velocity after transient:</p> <ul> <li>States: \\(v\\) (state 1)</li> <li>Formula: \\(\\max(|v|)\\), threshold = 0.2 (FP if \\(\\leq 0.2\\), LC otherwise)</li> <li>Transient cutoff: \\(t^* = 400.0\\)</li> </ul>"},{"location":"case-studies/friction/#clustering","title":"Clustering","text":"<ul> <li>Method: k-NN (k=1)</li> <li>Template ICs:</li> <li>FP: \\([1.0, 1.0]\\) \u2014 Fixed point (stable fixed point of steady sliding)</li> <li>LC: \\([2.0, 2.0]\\) \u2014 Limit cycle (stick-slip oscillation)</li> </ul>"},{"location":"case-studies/friction/#reproduction-code","title":"Reproduction Code","text":""},{"location":"case-studies/friction/#setup","title":"Setup","text":"<pre><code>def setup_friction_system() -&gt; SetupProperties:\n    n = 5000\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up friction system on device: {device}\")\n\n    params: FrictionParams = {\n        \"v_d\": 1.5,  # Driving velocity\n        \"xi\": 0.05,  # Damping ratio\n        \"musd\": 2.0,  # Ratio static to dynamic friction coefficient\n        \"mud\": 0.5,  # Dynamic coefficient of friction\n        \"muv\": 0.0,  # Linear strengthening parameter\n        \"v0\": 0.5,  # Reference velocity for exponential decay\n    }\n\n    ode_system = FrictionJaxODE(params)\n\n    sampler = UniformRandomSampler(\n        min_limits=[-2.0, 0.0],\n        max_limits=[2.0, 2.0],\n        device=device,\n    )\n\n    solver = JaxSolver(\n        time_span=(0, 500),\n        n_steps=500,\n        device=device,\n        rtol=1e-8,\n        atol=1e-6,\n        cache_dir=\".pybasin_cache/friction\",\n    )\n\n    feature_extractor = FrictionFeatureExtractor(time_steady=400)\n\n    classifier_initial_conditions = [\n        [1.0, 1.0],\n        [2.0, 2.0],\n    ]\n\n    classifier_labels = [\"FP\", \"LC\"]\n\n    knn = KNeighborsClassifier(n_neighbors=1)\n\n    template_integrator = TemplateIntegrator(\n        template_y0=classifier_initial_conditions,\n        labels=classifier_labels,\n        ode_params=params,\n    )\n\n    return {\n        \"n\": n,\n        \"ode_system\": ode_system,\n        \"sampler\": sampler,\n        \"solver\": solver,\n        \"feature_extractor\": feature_extractor,\n        \"estimator\": knn,\n        \"template_integrator\": template_integrator,\n    }\n</code></pre>"},{"location":"case-studies/friction/#main-estimation","title":"Main Estimation","text":"<pre><code>def main():\n    props = setup_friction_system()\n\n    bse = BasinStabilityEstimator(\n        n=props[\"n\"],\n        ode_system=props[\"ode_system\"],\n        sampler=props[\"sampler\"],\n        solver=props.get(\"solver\"),\n        feature_extractor=props.get(\"feature_extractor\"),\n        predictor=props.get(\"estimator\"),\n        template_integrator=props.get(\"template_integrator\"),\n        save_to=\"results_friction\",\n        feature_selector=None,\n    )\n\n    bse.estimate_bs()\n\n    return bse\n</code></pre>"},{"location":"case-studies/friction/#case-1-baseline-results","title":"Case 1: Baseline Results","text":""},{"location":"case-studies/friction/#comparison-with-matlab-bstab","title":"Comparison with MATLAB bSTAB","text":"<p>Overall Classification Quality:</p> <ul> <li>Macro F1-score: 1.0000</li> <li>Matthews Correlation Coefficient: 1.0000</li> </ul> Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE F1 FP 0.3120 \u00b1 0.0066 0.3120 \u00b1 0.0066 1.0000 LC 0.6880 \u00b1 0.0066 0.6880 \u00b1 0.0066 1.0000"},{"location":"case-studies/friction/#visualizations","title":"Visualizations","text":""},{"location":"case-studies/friction/#basin-stability","title":"Basin Stability","text":""},{"location":"case-studies/friction/#state-space","title":"State Space","text":""},{"location":"case-studies/friction/#feature-space","title":"Feature Space","text":""},{"location":"case-studies/friction/#template-trajectories","title":"Template Trajectories","text":""},{"location":"case-studies/friction/#case-2-v_d-parameter-sweep","title":"Case 2: v_d Parameter Sweep","text":""},{"location":"case-studies/friction/#comparison-with-matlab-bstab_1","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 0.9991</p> <p>The average excludes cases where there is only a single attractor and the basin stability values are the same since MCC is 0 for single class cases, and would therefore drop the average.</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 0.8 LC 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 0.875 LC 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 0.95 LC 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 1.025 LC 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 1.1 LC 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 1.175 FP 0.0128 \u00b1 0.0016 0.0126 \u00b1 0.0016 0.9921 LC 0.9872 \u00b1 0.0016 0.9874 \u00b1 0.0016 1.25 FP 0.0838 \u00b1 0.0039 0.0838 \u00b1 0.0039 1.0000 LC 0.9162 \u00b1 0.0039 0.9162 \u00b1 0.0039 1.325 FP 0.1378 \u00b1 0.0049 0.1378 \u00b1 0.0049 1.0000 LC 0.8622 \u00b1 0.0049 0.8622 \u00b1 0.0049 1.4 FP 0.2110 \u00b1 0.0058 0.2110 \u00b1 0.0058 1.0000 LC 0.7890 \u00b1 0.0058 0.7890 \u00b1 0.0058 1.475 FP 0.2824 \u00b1 0.0064 0.2824 \u00b1 0.0064 1.0000 LC 0.7176 \u00b1 0.0064 0.7176 \u00b1 0.0064 1.55 FP 0.3636 \u00b1 0.0068 0.3636 \u00b1 0.0068 1.0000 LC 0.6364 \u00b1 0.0068 0.6364 \u00b1 0.0068 1.625 FP 0.4336 \u00b1 0.0070 0.4336 \u00b1 0.0070 1.0000 LC 0.5664 \u00b1 0.0070 0.5664 \u00b1 0.0070 1.7 FP 0.4912 \u00b1 0.0071 0.4912 \u00b1 0.0071 1.0000 LC 0.5088 \u00b1 0.0071 0.5088 \u00b1 0.0071 1.775 FP 0.5552 \u00b1 0.0070 0.5552 \u00b1 0.0070 1.0000 LC 0.4448 \u00b1 0.0070 0.4448 \u00b1 0.0070 1.85 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 1.925 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 2 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 2.075 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 2.15 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 2.225 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000"},{"location":"case-studies/friction/#visualizations_1","title":"Visualizations","text":""},{"location":"case-studies/friction/#basin-stability-variation","title":"Basin Stability Variation","text":""},{"location":"case-studies/friction/#bifurcation-diagram","title":"Bifurcation Diagram","text":""},{"location":"case-studies/lorenz/","title":"Lorenz System","text":""},{"location":"case-studies/lorenz/#system-description","title":"System Description","text":"<p>Lorenz system:</p> \\[ \\begin{aligned} \\dot{x} &amp;= \\sigma(y - x) \\\\ \\dot{y} &amp;= rx - xz - y \\\\ \\dot{z} &amp;= xy - bz \\end{aligned} \\]"},{"location":"case-studies/lorenz/#system-parameters","title":"System Parameters","text":"Parameter Symbol Value Prandtl number \\(\\sigma\\) 0.12 Rayleigh number \\(r\\) 0.0 Geometric factor \\(b\\) -0.6"},{"location":"case-studies/lorenz/#sampling","title":"Sampling","text":"<ul> <li>Dimension: \\(D = 3\\)</li> <li>Sample size: \\(N = 20000\\)</li> <li>Distribution: \\(\\rho\\) = Uniform</li> <li>Region of interest: \\(\\mathcal{Q}(x, y, z) : [-10, 10] \\times [-20, 20] \\times [0]\\)</li> </ul>"},{"location":"case-studies/lorenz/#solver","title":"Solver","text":"Setting Value Method Dopri5 (Diffrax) Time span \\([0, 1000]\\) Steps 4000 (\\(f_s\\) = 4 Hz) Relative tolerance 1e-08 Absolute tolerance 1e-06 Event function Divergence at \\(\\vert y \\vert &gt; 200\\)"},{"location":"case-studies/lorenz/#feature-extraction","title":"Feature Extraction","text":"<p>Mean of \\(x\\) coordinate after transient:</p> <ul> <li>States: \\(x\\) (state 0)</li> <li>Formula: \\(\\bar{x} = \\text{mean}(x_{t &gt; t^*})\\)</li> <li>Transient cutoff: \\(t^* = 900.0\\)</li> </ul>"},{"location":"case-studies/lorenz/#clustering","title":"Clustering","text":"<ul> <li>Method: k-NN (k=1)</li> <li>Template ICs:</li> <li>chaotic attractor 1: \\([0.8, -3.0, 0.0]\\) \u2014 Positive wing chaotic attractor</li> <li>chaotic attractor 2: \\([-0.8, 3.0, 0.0]\\) \u2014 Negative wing chaotic attractor</li> <li>unbounded: \\([10.0, 50.0, 0.0]\\) \u2014 Diverging trajectories</li> </ul> <p>Key Feature</p> <p>Demonstrates unboundedness detection with <code>event_fn</code>.</p>"},{"location":"case-studies/lorenz/#reproduction-code","title":"Reproduction Code","text":""},{"location":"case-studies/lorenz/#setup","title":"Setup","text":"<pre><code>def setup_lorenz_system() -&gt; SetupProperties:\n    n = 20_000\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up Lorenz system on device: {device}\")\n\n    params: LorenzParams = {\"sigma\": 0.12, \"r\": 0.0, \"b\": -0.6}\n\n    ode_system = LorenzJaxODE(params)\n\n    sampler = UniformRandomSampler(\n        min_limits=[-10.0, -20.0, 0.0], max_limits=[10.0, 20.0, 0.0], device=device\n    )\n\n    solver = JaxSolver(\n        time_span=(0, 1000),\n        n_steps=4000,\n        device=device,\n        rtol=1e-8,\n        atol=1e-6,\n        cache_dir=\".pybasin_cache/lorenz\",\n        event_fn=lorenz_stop_event,\n    )\n\n    feature_extractor = JaxFeatureExtractor(\n        time_steady=900.0,\n        normalize=False,\n        features_per_state={\n            0: {\"mean\": None},\n            1: None,\n            2: None,\n        },\n    )\n\n    classifier_initial_conditions = [\n        [0.8, -3.0, 0.0],\n        [-0.8, 3.0, 0.0],\n        [10.0, 50.0, 0.0],\n    ]\n\n    classifier_labels = [\"chaotic attractor 1\", \"chaotic attractor 2\", \"unbounded\"]\n\n    knn = KNeighborsClassifier(n_neighbors=1)\n\n    template_integrator = TemplateIntegrator(\n        template_y0=classifier_initial_conditions,\n        labels=classifier_labels,\n        ode_params=params,\n    )\n\n    return {\n        \"n\": n,\n        \"ode_system\": ode_system,\n        \"sampler\": sampler,\n        \"solver\": solver,\n        \"feature_extractor\": feature_extractor,\n        \"estimator\": knn,\n        \"template_integrator\": template_integrator,\n    }\n</code></pre>"},{"location":"case-studies/lorenz/#main-estimation","title":"Main Estimation","text":"<pre><code>def main():\n    props = setup_lorenz_system()\n\n    bse = BasinStabilityEstimator(\n        n=props[\"n\"],\n        ode_system=props[\"ode_system\"],\n        sampler=props[\"sampler\"],\n        solver=props.get(\"solver\"),\n        feature_extractor=props.get(\"feature_extractor\"),\n        predictor=props.get(\"estimator\"),\n        template_integrator=props.get(\"template_integrator\"),\n        save_to=\"results_case1\",\n        # feature_selector=None,\n    )\n\n    basin_stability = bse.estimate_bs()\n    print(\"Basin Stability:\", basin_stability)\n\n    # bse.save()\n\n    return bse\n</code></pre>"},{"location":"case-studies/lorenz/#case-1-baseline-results","title":"Case 1: Baseline Results","text":""},{"location":"case-studies/lorenz/#comparison-with-matlab-bstab","title":"Comparison with MATLAB bSTAB","text":"<p>Overall Classification Quality:</p> <ul> <li>Macro F1-score: 0.9991</li> <li>Matthews Correlation Coefficient: 0.9985</li> </ul> Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE F1 chaotic attractor 1 0.0894 \u00b1 0.0020 0.0894 \u00b1 0.0020 0.9989 chaotic attractor 2 0.0875 \u00b1 0.0020 0.0874 \u00b1 0.0020 0.9986 unbounded 0.8231 \u00b1 0.0027 0.8232 \u00b1 0.0027 0.9997"},{"location":"case-studies/lorenz/#visualizations","title":"Visualizations","text":""},{"location":"case-studies/lorenz/#basin-stability","title":"Basin Stability","text":""},{"location":"case-studies/lorenz/#state-space","title":"State Space","text":""},{"location":"case-studies/lorenz/#feature-space","title":"Feature Space","text":""},{"location":"case-studies/lorenz/#template-phase-space","title":"Template Phase Space","text":""},{"location":"case-studies/lorenz/#case-2-sigma-parameter-sweep","title":"Case 2: Sigma Parameter Sweep","text":""},{"location":"case-studies/lorenz/#comparison-with-matlab-bstab_1","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 0.9999</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 0.12 chaotic attractor 1 0.0844 \u00b1 0.0020 0.0843 \u00b1 0.0020 0.9982 chaotic attractor 2 0.0862 \u00b1 0.0020 0.0859 \u00b1 0.0020 unbounded 0.8294 \u00b1 0.0027 0.8298 \u00b1 0.0027 0.1225 chaotic attractor 1 0.1019 \u00b1 0.0021 0.1019 \u00b1 0.0021 1.0000 chaotic attractor 2 0.1047 \u00b1 0.0022 0.1047 \u00b1 0.0022 unbounded 0.7933 \u00b1 0.0029 0.7933 \u00b1 0.0029 0.125 chaotic attractor 1 0.1122 \u00b1 0.0022 0.1122 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1139 \u00b1 0.0022 0.1139 \u00b1 0.0022 unbounded 0.7740 \u00b1 0.0030 0.7740 \u00b1 0.0030 0.1275 chaotic attractor 1 0.1140 \u00b1 0.0022 0.1140 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1163 \u00b1 0.0023 0.1163 \u00b1 0.0023 unbounded 0.7697 \u00b1 0.0030 0.7697 \u00b1 0.0030 0.13 chaotic attractor 1 0.1148 \u00b1 0.0023 0.1148 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1138 \u00b1 0.0022 0.1138 \u00b1 0.0022 unbounded 0.7714 \u00b1 0.0030 0.7714 \u00b1 0.0030 0.1325 chaotic attractor 1 0.1080 \u00b1 0.0022 0.1080 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1127 \u00b1 0.0022 0.1127 \u00b1 0.0022 unbounded 0.7794 \u00b1 0.0029 0.7794 \u00b1 0.0029 0.135 chaotic attractor 1 0.1105 \u00b1 0.0022 0.1105 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1106 \u00b1 0.0022 0.1106 \u00b1 0.0022 unbounded 0.7789 \u00b1 0.0029 0.7789 \u00b1 0.0029 0.1375 chaotic attractor 1 0.1118 \u00b1 0.0022 0.1118 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1126 \u00b1 0.0022 0.1126 \u00b1 0.0022 unbounded 0.7756 \u00b1 0.0029 0.7756 \u00b1 0.0029 0.14 chaotic attractor 1 0.1123 \u00b1 0.0022 0.1123 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1103 \u00b1 0.0022 0.1103 \u00b1 0.0022 unbounded 0.7775 \u00b1 0.0029 0.7775 \u00b1 0.0029 0.1425 chaotic attractor 1 0.1119 \u00b1 0.0022 0.1119 \u00b1 0.0022 0.9999 chaotic attractor 2 0.1092 \u00b1 0.0022 0.1092 \u00b1 0.0022 unbounded 0.7789 \u00b1 0.0029 0.7789 \u00b1 0.0029 0.145 chaotic attractor 1 0.1143 \u00b1 0.0022 0.1143 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1093 \u00b1 0.0022 0.1093 \u00b1 0.0022 unbounded 0.7764 \u00b1 0.0029 0.7764 \u00b1 0.0029 0.1475 chaotic attractor 1 0.1134 \u00b1 0.0022 0.1134 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1122 \u00b1 0.0022 0.1122 \u00b1 0.0022 unbounded 0.7743 \u00b1 0.0030 0.7743 \u00b1 0.0030 0.15 chaotic attractor 1 0.1139 \u00b1 0.0022 0.1139 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1081 \u00b1 0.0022 0.1081 \u00b1 0.0022 unbounded 0.7780 \u00b1 0.0029 0.7780 \u00b1 0.0029 0.1525 chaotic attractor 1 0.1155 \u00b1 0.0023 0.1155 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1116 \u00b1 0.0022 0.1116 \u00b1 0.0022 unbounded 0.7729 \u00b1 0.0030 0.7729 \u00b1 0.0030 0.155 chaotic attractor 1 0.1112 \u00b1 0.0022 0.1112 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1163 \u00b1 0.0023 0.1163 \u00b1 0.0023 unbounded 0.7724 \u00b1 0.0030 0.7724 \u00b1 0.0030 0.1575 chaotic attractor 1 0.1116 \u00b1 0.0022 0.1116 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1092 \u00b1 0.0022 0.1092 \u00b1 0.0022 unbounded 0.7792 \u00b1 0.0029 0.7792 \u00b1 0.0029 0.16 chaotic attractor 1 0.1142 \u00b1 0.0022 0.1142 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1114 \u00b1 0.0022 0.1114 \u00b1 0.0022 unbounded 0.7744 \u00b1 0.0030 0.7744 \u00b1 0.0030 0.1625 chaotic attractor 1 0.1131 \u00b1 0.0022 0.1131 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1157 \u00b1 0.0023 0.1157 \u00b1 0.0023 unbounded 0.7712 \u00b1 0.0030 0.7712 \u00b1 0.0030 0.165 chaotic attractor 1 0.1114 \u00b1 0.0022 0.1114 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1121 \u00b1 0.0022 0.1121 \u00b1 0.0022 unbounded 0.7765 \u00b1 0.0029 0.7765 \u00b1 0.0029 0.1675 chaotic attractor 1 0.1123 \u00b1 0.0022 0.1123 \u00b1 0.0022 0.9999 chaotic attractor 2 0.1193 \u00b1 0.0023 0.1194 \u00b1 0.0023 unbounded 0.7683 \u00b1 0.0030 0.7683 \u00b1 0.0030 0.17 chaotic attractor 1 0.1136 \u00b1 0.0022 0.1136 \u00b1 0.0022 1.0000 chaotic attractor 2 0.1182 \u00b1 0.0023 0.1182 \u00b1 0.0023 unbounded 0.7682 \u00b1 0.0030 0.7682 \u00b1 0.0030 0.1725 chaotic attractor 1 0.1201 \u00b1 0.0023 0.1201 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1171 \u00b1 0.0023 0.1171 \u00b1 0.0023 unbounded 0.7629 \u00b1 0.0030 0.7629 \u00b1 0.0030 0.175 chaotic attractor 1 0.1179 \u00b1 0.0023 0.1179 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1118 \u00b1 0.0022 0.1118 \u00b1 0.0022 unbounded 0.7702 \u00b1 0.0030 0.7702 \u00b1 0.0030 0.1775 chaotic attractor 1 0.1162 \u00b1 0.0023 0.1162 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1181 \u00b1 0.0023 0.1181 \u00b1 0.0023 unbounded 0.7657 \u00b1 0.0030 0.7657 \u00b1 0.0030 0.18 chaotic attractor 1 0.1172 \u00b1 0.0023 0.1172 \u00b1 0.0023 1.0000 chaotic attractor 2 0.1220 \u00b1 0.0023 0.1220 \u00b1 0.0023 unbounded 0.7609 \u00b1 0.0030 0.7609 \u00b1 0.0030"},{"location":"case-studies/lorenz/#visualizations_1","title":"Visualizations","text":""},{"location":"case-studies/lorenz/#basin-stability-variation","title":"Basin Stability Variation","text":""},{"location":"case-studies/lorenz/#bifurcation-diagram","title":"Bifurcation Diagram","text":""},{"location":"case-studies/lorenz/#case-3-solver-rtol-convergence-study","title":"Case 3: Solver rtol Convergence Study","text":"<p>This hyperparameter study demonstrates the effect of ODE solver relative tolerance on basin stability estimation. Coarse tolerances (rtol=1e-3) produce inaccurate results, while finer tolerances converge to consistent values.</p>"},{"location":"case-studies/lorenz/#comparison-with-matlab-bstab_2","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 0.9024</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 1.0e-03 chaotic attractor 1 0.0236 \u00b1 0.0011 0.0895 \u00b1 0.0020 0.4478 chaotic attractor 2 0.0211 \u00b1 0.0010 0.0858 \u00b1 0.0020 unbounded 0.9553 \u00b1 0.0015 0.8246 \u00b1 0.0027 1.0e-04 chaotic attractor 1 0.0871 \u00b1 0.0020 0.0874 \u00b1 0.0020 0.9771 chaotic attractor 2 0.0859 \u00b1 0.0020 0.0862 \u00b1 0.0020 unbounded 0.8270 \u00b1 0.0027 0.8264 \u00b1 0.0027 1.0e-05 chaotic attractor 1 0.0871 \u00b1 0.0020 0.0871 \u00b1 0.0020 0.9952 chaotic attractor 2 0.0855 \u00b1 0.0020 0.0850 \u00b1 0.0020 unbounded 0.8274 \u00b1 0.0027 0.8279 \u00b1 0.0027 1.0e-06 chaotic attractor 1 0.0871 \u00b1 0.0020 0.0872 \u00b1 0.0020 0.9984 chaotic attractor 2 0.0887 \u00b1 0.0020 0.0887 \u00b1 0.0020 unbounded 0.8242 \u00b1 0.0027 0.8242 \u00b1 0.0027 1.0e-07 chaotic attractor 1 0.0860 \u00b1 0.0020 0.0862 \u00b1 0.0020 0.9979 chaotic attractor 2 0.0882 \u00b1 0.0020 0.0882 \u00b1 0.0020 unbounded 0.8258 \u00b1 0.0027 0.8256 \u00b1 0.0027 1.0e-08 chaotic attractor 1 0.0876 \u00b1 0.0020 0.0874 \u00b1 0.0020 0.9984 chaotic attractor 2 0.0872 \u00b1 0.0020 0.0871 \u00b1 0.0020 unbounded 0.8252 \u00b1 0.0027 0.8255 \u00b1 0.0027"},{"location":"case-studies/lorenz/#visualizations_2","title":"Visualizations","text":""},{"location":"case-studies/lorenz/#basin-stability-variation_1","title":"Basin Stability Variation","text":""},{"location":"case-studies/lorenz/#bifurcation-diagram_1","title":"Bifurcation Diagram","text":""},{"location":"case-studies/lorenz/#case-4-sample-size-convergence-study","title":"Case 4: Sample Size Convergence Study","text":"<p>This hyperparameter study varies the number of initial conditions \\(N\\) from 200 to 20,000 (using \\(2 \\times \\text{logspace}(2, 4, 50)\\)) to assess how basin stability estimates converge as sample size increases. The relative standard error decreases as \\(\\text{SE}/\\mathcal{S}_{\\mathcal{B}} \\sim 1/\\sqrt{N}\\).</p>"},{"location":"case-studies/lorenz/#comparison-with-matlab-bstab_3","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 0.9981</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 200 chaotic attractor 1 0.1000 \u00b1 0.0212 0.1000 \u00b1 0.0212 1.0000 chaotic attractor 2 0.0750 \u00b1 0.0186 0.0750 \u00b1 0.0186 unbounded 0.8250 \u00b1 0.0269 0.8250 \u00b1 0.0269 219.7082 chaotic attractor 1 0.1136 \u00b1 0.0214 0.1091 \u00b1 0.0210 0.9844 chaotic attractor 2 0.0545 \u00b1 0.0153 0.0545 \u00b1 0.0153 unbounded 0.8318 \u00b1 0.0252 0.8364 \u00b1 0.0249 241.3585 chaotic attractor 1 0.0537 \u00b1 0.0145 0.0537 \u00b1 0.0145 1.0000 chaotic attractor 2 0.0785 \u00b1 0.0173 0.0785 \u00b1 0.0173 unbounded 0.8678 \u00b1 0.0218 0.8678 \u00b1 0.0218 265.1423 chaotic attractor 1 0.0639 \u00b1 0.0150 0.0639 \u00b1 0.0150 1.0000 chaotic attractor 2 0.1053 \u00b1 0.0188 0.1053 \u00b1 0.0188 unbounded 0.8308 \u00b1 0.0230 0.8308 \u00b1 0.0230 291.2697 chaotic attractor 1 0.0788 \u00b1 0.0158 0.0788 \u00b1 0.0158 1.0000 chaotic attractor 2 0.0822 \u00b1 0.0161 0.0822 \u00b1 0.0161 unbounded 0.8390 \u00b1 0.0215 0.8390 \u00b1 0.0215 319.9717 chaotic attractor 1 0.1000 \u00b1 0.0168 0.1000 \u00b1 0.0168 1.0000 chaotic attractor 2 0.0969 \u00b1 0.0165 0.0969 \u00b1 0.0165 unbounded 0.8031 \u00b1 0.0222 0.8031 \u00b1 0.0222 351.5021 chaotic attractor 1 0.0938 \u00b1 0.0155 0.0938 \u00b1 0.0155 1.0000 chaotic attractor 2 0.0909 \u00b1 0.0153 0.0909 \u00b1 0.0153 unbounded 0.8153 \u00b1 0.0207 0.8153 \u00b1 0.0207 386.1395 chaotic attractor 1 0.0879 \u00b1 0.0144 0.0879 \u00b1 0.0144 1.0000 chaotic attractor 2 0.0801 \u00b1 0.0138 0.0801 \u00b1 0.0138 unbounded 0.8320 \u00b1 0.0190 0.8320 \u00b1 0.0190 424.1902 chaotic attractor 1 0.0918 \u00b1 0.0140 0.0918 \u00b1 0.0140 1.0000 chaotic attractor 2 0.0871 \u00b1 0.0137 0.0871 \u00b1 0.0137 unbounded 0.8212 \u00b1 0.0186 0.8212 \u00b1 0.0186 465.9904 chaotic attractor 1 0.0901 \u00b1 0.0133 0.0901 \u00b1 0.0133 1.0000 chaotic attractor 2 0.0966 \u00b1 0.0137 0.0966 \u00b1 0.0137 unbounded 0.8133 \u00b1 0.0181 0.8133 \u00b1 0.0181 511.9096 chaotic attractor 1 0.0859 \u00b1 0.0124 0.0859 \u00b1 0.0124 1.0000 chaotic attractor 2 0.0781 \u00b1 0.0119 0.0781 \u00b1 0.0119 unbounded 0.8359 \u00b1 0.0164 0.8359 \u00b1 0.0164 562.3537 chaotic attractor 1 0.0764 \u00b1 0.0112 0.0764 \u00b1 0.0112 1.0000 chaotic attractor 2 0.0764 \u00b1 0.0112 0.0764 \u00b1 0.0112 unbounded 0.8472 \u00b1 0.0152 0.8472 \u00b1 0.0152 617.7687 chaotic attractor 1 0.0971 \u00b1 0.0119 0.0971 \u00b1 0.0119 1.0000 chaotic attractor 2 0.0761 \u00b1 0.0107 0.0761 \u00b1 0.0107 unbounded 0.8269 \u00b1 0.0152 0.8269 \u00b1 0.0152 678.6444 chaotic attractor 1 0.0781 \u00b1 0.0103 0.0781 \u00b1 0.0103 1.0000 chaotic attractor 2 0.0810 \u00b1 0.0105 0.0810 \u00b1 0.0105 unbounded 0.8409 \u00b1 0.0140 0.8409 \u00b1 0.0140 745.5187 chaotic attractor 1 0.0979 \u00b1 0.0109 0.0979 \u00b1 0.0109 1.0000 chaotic attractor 2 0.1046 \u00b1 0.0112 0.1046 \u00b1 0.0112 unbounded 0.7976 \u00b1 0.0147 0.7976 \u00b1 0.0147 818.983 chaotic attractor 1 0.0696 \u00b1 0.0089 0.0696 \u00b1 0.0089 1.0000 chaotic attractor 2 0.0891 \u00b1 0.0100 0.0891 \u00b1 0.0100 unbounded 0.8413 \u00b1 0.0128 0.8413 \u00b1 0.0128 899.6865 chaotic attractor 1 0.0878 \u00b1 0.0094 0.0878 \u00b1 0.0094 1.0000 chaotic attractor 2 0.0844 \u00b1 0.0093 0.0844 \u00b1 0.0093 unbounded 0.8278 \u00b1 0.0126 0.8278 \u00b1 0.0126 988.3427 chaotic attractor 1 0.0890 \u00b1 0.0091 0.0890 \u00b1 0.0091 1.0000 chaotic attractor 2 0.0839 \u00b1 0.0088 0.0839 \u00b1 0.0088 unbounded 0.8271 \u00b1 0.0120 0.8271 \u00b1 0.0120 1085.7351 chaotic attractor 1 0.0829 \u00b1 0.0084 0.0838 \u00b1 0.0084 0.9912 chaotic attractor 2 0.0994 \u00b1 0.0091 0.0976 \u00b1 0.0090 unbounded 0.8177 \u00b1 0.0117 0.8186 \u00b1 0.0117 1192.7247 chaotic attractor 1 0.0930 \u00b1 0.0084 0.0922 \u00b1 0.0084 0.9972 chaotic attractor 2 0.0805 \u00b1 0.0079 0.0805 \u00b1 0.0079 unbounded 0.8265 \u00b1 0.0110 0.8273 \u00b1 0.0109 1310.2571 chaotic attractor 1 0.0847 \u00b1 0.0077 0.0847 \u00b1 0.0077 0.9975 chaotic attractor 2 0.0923 \u00b1 0.0080 0.0931 \u00b1 0.0080 unbounded 0.8230 \u00b1 0.0105 0.8223 \u00b1 0.0106 1439.3713 chaotic attractor 1 0.0764 \u00b1 0.0070 0.0764 \u00b1 0.0070 1.0000 chaotic attractor 2 0.0875 \u00b1 0.0074 0.0875 \u00b1 0.0074 unbounded 0.8361 \u00b1 0.0098 0.8361 \u00b1 0.0098 1581.2086 chaotic attractor 1 0.0853 \u00b1 0.0070 0.0853 \u00b1 0.0070 0.9979 chaotic attractor 2 0.0904 \u00b1 0.0072 0.0898 \u00b1 0.0072 unbounded 0.8243 \u00b1 0.0096 0.8249 \u00b1 0.0096 1737.0227 chaotic attractor 1 0.0829 \u00b1 0.0066 0.0829 \u00b1 0.0066 1.0000 chaotic attractor 2 0.0748 \u00b1 0.0063 0.0748 \u00b1 0.0063 unbounded 0.8423 \u00b1 0.0087 0.8423 \u00b1 0.0087 1908.191 chaotic attractor 1 0.0849 \u00b1 0.0064 0.0838 \u00b1 0.0063 0.9963 chaotic attractor 2 0.0765 \u00b1 0.0061 0.0765 \u00b1 0.0061 unbounded 0.8387 \u00b1 0.0084 0.8397 \u00b1 0.0084 2096.2263 chaotic attractor 1 0.0930 \u00b1 0.0063 0.0925 \u00b1 0.0063 0.9985 chaotic attractor 2 0.0897 \u00b1 0.0062 0.0897 \u00b1 0.0062 unbounded 0.8174 \u00b1 0.0084 0.8178 \u00b1 0.0084 2302.7908 chaotic attractor 1 0.0821 \u00b1 0.0057 0.0821 \u00b1 0.0057 0.9986 chaotic attractor 2 0.0925 \u00b1 0.0060 0.0921 \u00b1 0.0060 unbounded 0.8254 \u00b1 0.0079 0.8259 \u00b1 0.0079 2529.7104 chaotic attractor 1 0.0870 \u00b1 0.0056 0.0870 \u00b1 0.0056 0.9986 chaotic attractor 2 0.0798 \u00b1 0.0054 0.0802 \u00b1 0.0054 unbounded 0.8332 \u00b1 0.0074 0.8328 \u00b1 0.0074 2778.991 chaotic attractor 1 0.0831 \u00b1 0.0052 0.0831 \u00b1 0.0052 0.9976 chaotic attractor 2 0.0892 \u00b1 0.0054 0.0892 \u00b1 0.0054 unbounded 0.8276 \u00b1 0.0072 0.8276 \u00b1 0.0072 3052.8359 chaotic attractor 1 0.0868 \u00b1 0.0051 0.0868 \u00b1 0.0051 0.9967 chaotic attractor 2 0.0809 \u00b1 0.0049 0.0812 \u00b1 0.0049 unbounded 0.8323 \u00b1 0.0068 0.8320 \u00b1 0.0068 3353.6659 chaotic attractor 1 0.0829 \u00b1 0.0048 0.0832 \u00b1 0.0048 0.9980 chaotic attractor 2 0.0847 \u00b1 0.0048 0.0844 \u00b1 0.0048 unbounded 0.8324 \u00b1 0.0064 0.8324 \u00b1 0.0064 3684.1399 chaotic attractor 1 0.0939 \u00b1 0.0048 0.0936 \u00b1 0.0048 0.9937 chaotic attractor 2 0.0803 \u00b1 0.0045 0.0803 \u00b1 0.0045 unbounded 0.8258 \u00b1 0.0062 0.8261 \u00b1 0.0062 4047.1793 chaotic attractor 1 0.0862 \u00b1 0.0044 0.0855 \u00b1 0.0044 0.9968 chaotic attractor 2 0.0931 \u00b1 0.0046 0.0929 \u00b1 0.0046 unbounded 0.8207 \u00b1 0.0060 0.8216 \u00b1 0.0060 4445.993 chaotic attractor 1 0.0823 \u00b1 0.0041 0.0823 \u00b1 0.0041 0.9977 chaotic attractor 2 0.0888 \u00b1 0.0043 0.0886 \u00b1 0.0043 unbounded 0.8288 \u00b1 0.0056 0.8291 \u00b1 0.0056 4884.1062 chaotic attractor 1 0.0911 \u00b1 0.0041 0.0911 \u00b1 0.0041 0.9980 chaotic attractor 2 0.0839 \u00b1 0.0040 0.0837 \u00b1 0.0040 unbounded 0.8250 \u00b1 0.0054 0.8252 \u00b1 0.0054 5365.3916 chaotic attractor 1 0.0840 \u00b1 0.0038 0.0839 \u00b1 0.0038 0.9982 chaotic attractor 2 0.0906 \u00b1 0.0039 0.0902 \u00b1 0.0039 unbounded 0.8254 \u00b1 0.0052 0.8259 \u00b1 0.0052 5894.1034 chaotic attractor 1 0.0875 \u00b1 0.0037 0.0875 \u00b1 0.0037 0.9994 chaotic attractor 2 0.0896 \u00b1 0.0037 0.0897 \u00b1 0.0037 unbounded 0.8229 \u00b1 0.0050 0.8227 \u00b1 0.0050 6474.9151 chaotic attractor 1 0.0853 \u00b1 0.0035 0.0854 \u00b1 0.0035 0.9979 chaotic attractor 2 0.0854 \u00b1 0.0035 0.0849 \u00b1 0.0035 unbounded 0.8293 \u00b1 0.0047 0.8297 \u00b1 0.0047 7112.9606 chaotic attractor 1 0.0842 \u00b1 0.0033 0.0845 \u00b1 0.0033 0.9962 chaotic attractor 2 0.0879 \u00b1 0.0034 0.0876 \u00b1 0.0034 unbounded 0.8279 \u00b1 0.0045 0.8279 \u00b1 0.0045 7813.8799 chaotic attractor 1 0.0898 \u00b1 0.0032 0.0898 \u00b1 0.0032 0.9980 chaotic attractor 2 0.0907 \u00b1 0.0032 0.0906 \u00b1 0.0032 unbounded 0.8194 \u00b1 0.0044 0.8196 \u00b1 0.0044 8583.8685 chaotic attractor 1 0.0905 \u00b1 0.0031 0.0906 \u00b1 0.0031 0.9988 chaotic attractor 2 0.0833 \u00b1 0.0030 0.0833 \u00b1 0.0030 unbounded 0.8262 \u00b1 0.0041 0.8261 \u00b1 0.0041 9429.7327 chaotic attractor 1 0.0871 \u00b1 0.0029 0.0871 \u00b1 0.0029 0.9982 chaotic attractor 2 0.0852 \u00b1 0.0029 0.0848 \u00b1 0.0029 unbounded 0.8278 \u00b1 0.0039 0.8281 \u00b1 0.0039 10358.9494 chaotic attractor 1 0.0849 \u00b1 0.0027 0.0851 \u00b1 0.0027 0.9974 chaotic attractor 2 0.0870 \u00b1 0.0028 0.0873 \u00b1 0.0028 unbounded 0.8282 \u00b1 0.0037 0.8276 \u00b1 0.0037 11379.7321 chaotic attractor 1 0.0852 \u00b1 0.0026 0.0853 \u00b1 0.0026 0.9967 chaotic attractor 2 0.0823 \u00b1 0.0026 0.0827 \u00b1 0.0026 unbounded 0.8324 \u00b1 0.0035 0.8320 \u00b1 0.0035 12501.1039 chaotic attractor 1 0.0893 \u00b1 0.0026 0.0894 \u00b1 0.0026 0.9966 chaotic attractor 2 0.0891 \u00b1 0.0025 0.0893 \u00b1 0.0026 unbounded 0.8215 \u00b1 0.0034 0.8213 \u00b1 0.0034 13732.9769 chaotic attractor 1 0.0891 \u00b1 0.0024 0.0891 \u00b1 0.0024 0.9988 chaotic attractor 2 0.0869 \u00b1 0.0024 0.0869 \u00b1 0.0024 unbounded 0.8241 \u00b1 0.0032 0.8240 \u00b1 0.0032 15086.2401 chaotic attractor 1 0.0852 \u00b1 0.0023 0.0851 \u00b1 0.0023 0.9975 chaotic attractor 2 0.0848 \u00b1 0.0023 0.0848 \u00b1 0.0023 unbounded 0.8300 \u00b1 0.0031 0.8301 \u00b1 0.0031 16572.8555 chaotic attractor 1 0.0876 \u00b1 0.0022 0.0875 \u00b1 0.0022 0.9976 chaotic attractor 2 0.0863 \u00b1 0.0022 0.0863 \u00b1 0.0022 unbounded 0.8262 \u00b1 0.0029 0.8262 \u00b1 0.0029 18205.9636 chaotic attractor 1 0.0857 \u00b1 0.0021 0.0855 \u00b1 0.0021 0.9978 chaotic attractor 2 0.0889 \u00b1 0.0021 0.0888 \u00b1 0.0021 unbounded 0.8254 \u00b1 0.0028 0.8257 \u00b1 0.0028 20000 chaotic attractor 1 0.0890 \u00b1 0.0020 0.0890 \u00b1 0.0020 0.9974 chaotic attractor 2 0.0867 \u00b1 0.0020 0.0866 \u00b1 0.0020 unbounded 0.8244 \u00b1 0.0027 0.8245 \u00b1 0.0027"},{"location":"case-studies/lorenz/#visualizations_3","title":"Visualizations","text":""},{"location":"case-studies/lorenz/#basin-stability-variation_2","title":"Basin Stability Variation","text":""},{"location":"case-studies/lorenz/#bifurcation-diagram_2","title":"Bifurcation Diagram","text":""},{"location":"case-studies/overview/","title":"Case Studies Overview","text":"<p>Documented here are the case studies that validate pyBasin against its original MATLAB counterpart, bSTAB. Each study targets a specific dynamical system and compares the two implementations on identical initial conditions.</p>"},{"location":"case-studies/overview/#classification-quality-metrics","title":"Classification Quality Metrics","text":"<p>To assess whether pyBasin reproduces the MATLAB bSTAB results, we compare predicted attractor labels from pyBasin with ground truth labels produced by bSTAB. Both tools classify trajectories into discrete attractor categories (e.g., \"FP\", \"LC\", \"chaos\"), which makes it possible to apply standard classification metrics directly.</p>"},{"location":"case-studies/overview/#methodology","title":"Methodology","text":"<p>Given that attractor labels from two independent implementations are compared, we treat the problem as a classification task and evaluate agreement using established metrics.</p> <p>Concretely, each test case proceeds as follows:</p> <ol> <li>Load the exact initial conditions exported from MATLAB ground truth CSV files</li> <li>Classify those initial conditions with pyBasin</li> <li>Match the resulting labels against the MATLAB ground truth</li> <li>Evaluate the classification metrics described below</li> </ol>"},{"location":"case-studies/overview/#metrics-used","title":"Metrics Used","text":""},{"location":"case-studies/overview/#1-f1-score-per-class","title":"1. F1-Score (Per Class)","text":"<p>Per-class classification quality is measured by the F1-score:</p> \\[F1 = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}\\] <p>where TP = true positives, FP = false positives, FN = false negatives for that class.</p> <p>Range: [0, 1], where 1.0 = perfect classification for that class</p>"},{"location":"case-studies/overview/#2-macro-f1-score-overall","title":"2. Macro F1-Score (Overall)","text":"<p>Averaging the per-class F1-scores yields the macro F1-score, which captures overall classification quality without weighting by class frequency:</p> \\[\\text{Macro F1} = \\frac{1}{K} \\sum_{k=1}^{K} F1_k\\] <p>where \\(K\\) is the number of classes (attractor types).</p> <p>Range: [0, 1], where 1.0 = perfect classification across all classes</p>"},{"location":"case-studies/overview/#3-matthews-correlation-coefficient-mcc","title":"3. Matthews Correlation Coefficient (MCC)","text":"<p>As a global measure of prediction\u2013ground truth correlation, we also report the Matthews correlation coefficient.</p> <p>For binary classification:</p> \\[\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\\] <p>For multiclass classification, scikit-learn implements a generalization based on the confusion matrix \\(C\\):</p> \\[\\text{MCC} = \\frac{c \\cdot s - \\sum_k p_k \\cdot t_k}{\\sqrt{(s^2 - \\sum_k p_k^2)(s^2 - \\sum_k t_k^2)}}\\] <p>where:</p> <ul> <li>\\(t_k = \\sum_i^K C_{ik}\\) \u2014 the number of times class \\(k\\) truly occurred</li> <li>\\(p_k = \\sum_i^K C_{ki}\\) \u2014 the number of times class \\(k\\) was predicted</li> <li>\\(c = \\sum_k^K C_{kk}\\) \u2014 the total number of samples correctly predicted</li> <li>\\(s = \\sum_i^K \\sum_j^K C_{ij}\\) \u2014 the total number of samples</li> </ul> <p>Range: [-1, 1], where:</p> <ul> <li>+1 = perfect prediction</li> <li>0 = random prediction</li> <li>-1 = complete disagreement</li> </ul> <p>Because basin stability problems often feature one dominant attractor, MCC is well-suited here; it remains informative even under class imbalance.</p>"},{"location":"case-studies/overview/#quality-thresholds","title":"Quality Thresholds","text":"Metric Excellent Good Acceptable Poor F1 \u2265 0.95 \u2265 0.90 \u2265 0.80 &lt; 0.80 Macro F1 \u2265 0.95 \u2265 0.90 \u2265 0.80 &lt; 0.80 MCC \u2265 0.90 \u2265 0.80 \u2265 0.70 &lt; 0.70 <p>Scores in the Excellent or Good range confirm that pyBasin faithfully reproduces the MATLAB behaviour. Acceptable scores point to minor discrepancies\u2014often numerical precision or boundary-case effects. Poor scores warrant further investigation.</p>"},{"location":"case-studies/overview/#reading-comparison-tables","title":"Reading Comparison Tables","text":"<p>Each case study contains a comparison table with the following columns:</p> <ul> <li>Attractor: The attractor type (e.g., \"FP\", \"LC\", \"chaos\")</li> <li>pyBasin BS +/- SE: Basin stability and standard error from the Python implementation</li> <li>bSTAB BS +/- SE: Corresponding values from the MATLAB reference</li> <li>F1: Per-class F1-score</li> </ul> <p>Above each table, a summary line reports the macro F1-score and the MCC for that comparison.</p>"},{"location":"case-studies/overview/#purpose","title":"Purpose","text":"<p>These case studies fulfil several roles at once. First, they provide a systematic validation path: comparing pyBasin results against MATLAB bSTAB on the same initial conditions establishes correctness. Beyond validation, they double as usage examples for different classes of dynamical systems. They also produce the figures and numerical tables needed for documentation. Finally, they serve as performance benchmarks.</p>"},{"location":"case-studies/overview/#available-case-studies","title":"Available Case Studies","text":""},{"location":"case-studies/overview/#duffing-oscillator","title":"Duffing Oscillator","text":"<p>A periodically forced Duffing oscillator that exhibits multistability with five coexisting limit cycle attractors.</p> <p>Key Features:</p> <ul> <li>Five coexisting limit cycle attractors</li> <li>Supervised vs. unsupervised classification comparison</li> <li>Feature extraction based on maximum amplitude and standard deviation</li> </ul> <p>Reference: Thomson, J. M. T., &amp; Stewart, H. B. (2002). Nonlinear dynamics and chaos (2nd ed.). Wiley. (See p. 9, Fig. 1.9)</p> <p>Files: <code>case_studies/duffing_oscillator/</code></p>"},{"location":"case-studies/overview/#lorenz-system","title":"Lorenz System","text":"<p>A modified Lorenz system in which two chaotic attractors coexist alongside unbounded trajectories.</p> <p>Key Features:</p> <ul> <li>Two coexisting chaotic attractors and unbounded solutions</li> <li>Parameter sweep over \u03c3</li> <li>Sample size (N) convergence study</li> <li>Sensitivity analysis of integrator tolerances (rtol/atol)</li> </ul> <p>Reference: Li, C., &amp; Sprott, J. C. (2014). Multistability in the Lorenz system: A broken butterfly. International Journal of Bifurcation and Chaos, 24(10), Article 1450131. https://doi.org/10.1142/S0218127414501314</p> <p>Files: <code>case_studies/lorenz/</code></p>"},{"location":"case-studies/overview/#pendulum","title":"Pendulum","text":"<p>A periodically forced pendulum, studied under several different forcing parameter configurations.</p> <p>Key Features:</p> <ul> <li>Multiple parameter cases</li> <li>Fixed point and limit cycle attractors</li> <li>Supervised classification</li> </ul> <p>Reference: Menck, P., Heitzig, J., Marwan, N., &amp; Kurths, J. (2013). How basin stability complements the linear-stability paradigm. Nature Physics, 9, 89\u201392. https://doi.org/10.1038/nphys2516</p> <p>Files: <code>case_studies/pendulum/</code></p>"},{"location":"case-studies/overview/#friction-system","title":"Friction System","text":"<p>A mechanical oscillator subject to dry friction, resulting in non-smooth dynamics and coexisting fixed point and limit cycle attractors.</p> <p>Key Features:</p> <ul> <li>Fixed point and limit cycle attractors</li> <li>Non-smooth dynamics due to friction</li> <li>Parameter sweep over the driving velocity \\(v_d\\)</li> </ul> <p>Reference: Stender, M., Hoffmann, N., &amp; Papangelo, A. (2020). The basin stability of bi-stable friction-excited oscillators. Lubricants, 8(12), Article 105. https://doi.org/10.3390/lubricants8120105</p> <p>Files: <code>case_studies/friction/</code></p>"},{"location":"case-studies/overview/#rossler-network","title":"R\u00f6ssler Network","text":"<p>A network of coupled R\u00f6ssler oscillators, used to study synchronization and its basin stability.</p> <p>Key Features:</p> <ul> <li>Coupled oscillator dynamics</li> <li>Synchronization analysis</li> <li>Network-level basin stability estimation</li> </ul> <p>Reference: Menck, P., Heitzig, J., Marwan, N., &amp; Kurths, J. (2013). How basin stability complements the linear-stability paradigm. Nature Physics, 9, 89\u201392. https://doi.org/10.1038/nphys2516</p> <p>Files: <code>case_studies/rossler_network/</code></p>"},{"location":"case-studies/overview/#running-case-studies","title":"Running Case Studies","text":"<p>From the project root, individual case studies can be executed as follows:</p> <pre><code># Navigate to project root\ncd /path/to/pyBasinWorkspace\n\n# Run a specific case study\nuv run python -m case_studies.duffing_oscillator.main_duffing_oscillator_supervised\nuv run python -m case_studies.lorenz.main_lorenz\nuv run python -m case_studies.pendulum.main_pendulum_case1\n</code></pre>"},{"location":"case-studies/overview/#integration-tests","title":"Integration Tests","text":"<p>Each case study has a corresponding integration test that automatically checks correctness:</p> <pre><code># Run all integration tests\nuv run pytest tests/integration/\n\n# Run specific case study test\nuv run pytest tests/integration/test_duffing.py\n</code></pre>"},{"location":"case-studies/overview/#generated-artifacts","title":"Generated Artifacts","text":"<p>Outputs are written to two locations:</p> <ul> <li>Figures: <code>docs/assets/</code> \u2014 plots and visualisations</li> <li>Results: <code>artifacts/results/</code> \u2014 numerical data (JSON, CSV)</li> </ul> <p>Passing the <code>--generate-artifacts</code> flag to the test runner regenerates these outputs:</p> <pre><code># Generate artifacts for all case studies\nuv run pytest tests/integration/ --generate-artifacts\n\n# Generate artifacts for a specific case study\nuv run pytest tests/integration/test_duffing.py --generate-artifacts\n</code></pre>"},{"location":"case-studies/overview/#contributing-new-case-studies","title":"Contributing New Case Studies","text":"<p>Adding a new case study involves the following steps:</p> <ol> <li>Create a directory under <code>case_studies/</code></li> <li>Implement the ODE system and feature extractor</li> <li>Write a main script that runs the analysis</li> <li>Add a matching integration test</li> <li>Document the study in this section</li> </ol> <p>See the Contributing Guide for further details.</p>"},{"location":"case-studies/pendulum/","title":"Pendulum","text":""},{"location":"case-studies/pendulum/#system-description","title":"System Description","text":"<p>Driven damped pendulum:</p> \\[ \\begin{aligned} \\dot{\\theta} &amp;= \\omega \\\\ \\dot{\\omega} &amp;= -\\alpha \\omega - K \\sin(\\theta) + T \\end{aligned} \\]"},{"location":"case-studies/pendulum/#system-parameters","title":"System Parameters","text":"Parameter Symbol Value Damping coefficient \\(\\alpha\\) 0.1 Torque \\(T\\) 0.5 Stiffness \\(K\\) 1.0"},{"location":"case-studies/pendulum/#sampling","title":"Sampling","text":"<ul> <li>Dimension: \\(D = 2\\)</li> <li>Sample size: \\(N = 10000\\)</li> <li>Distribution: \\(\\rho\\) = Uniform</li> <li>Region of interest: \\(\\mathcal{Q}(\\theta, \\omega) : [\\psi - \\pi, \\psi + \\pi] \\times [-10, 10]\\) where \\(\\psi = \\arcsin(T/K)\\)</li> </ul>"},{"location":"case-studies/pendulum/#solver","title":"Solver","text":"Setting Value Method Dopri5 (Diffrax) Time span \\([0, 1000]\\) Steps 1000 (\\(f_s\\) = 1 Hz) Relative tolerance 1e-08 Absolute tolerance 1e-06"},{"location":"case-studies/pendulum/#feature-extraction","title":"Feature Extraction","text":"<p>Log-delta feature on angular velocity:</p> <ul> <li>States: \\(\\omega\\) (state 1)</li> <li>Formula: \\(\\Delta = \\log_{10}(|\\max(\\omega) - \\text{mean}(\\omega)| + \\epsilon)\\)</li> <li>Transient cutoff: \\(t^* = 950.0\\)</li> </ul>"},{"location":"case-studies/pendulum/#clustering","title":"Clustering","text":"<ul> <li>Method: k-NN (k=1)</li> <li>Template ICs:</li> <li>FP: \\([0.4, 0.0]\\) \u2014 Fixed point (stable equilibrium)</li> <li>LC: \\([2.7, 0.0]\\) \u2014 Limit cycle (rotational motion)</li> </ul>"},{"location":"case-studies/pendulum/#reproduction-code","title":"Reproduction Code","text":""},{"location":"case-studies/pendulum/#setup","title":"Setup","text":"<pre><code>def setup_pendulum_system() -&gt; SetupProperties:\n    n = 10000\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up pendulum system on device: {device}\")\n\n    params: PendulumParams = {\"alpha\": 0.1, \"T\": 0.5, \"K\": 1.0}\n\n    ode_system = PendulumJaxODE(params)\n\n    sampler = UniformRandomSampler(\n        min_limits=[-np.pi + np.arcsin(params[\"T\"] / params[\"K\"]), -10.0],\n        max_limits=[np.pi + np.arcsin(params[\"T\"] / params[\"K\"]), 10.0],\n        device=device,\n    )\n\n    solver = JaxSolver(\n        time_span=(0, 1000),\n        n_steps=1000,\n        device=device,\n        rtol=1e-8,\n        atol=1e-6,\n        cache_dir=\".pybasin_cache/pendulum\",\n    )\n\n    feature_extractor = TorchFeatureExtractor(\n        time_steady=950.0,\n        features=None,\n        features_per_state={\n            1: {\"log_delta\": None},\n        },\n        normalize=False,\n    )\n\n    template_y0 = [\n        [0.4, 0.0],\n        [2.7, 0.0],\n    ]\n    classifier_labels = [\"FP\", \"LC\"]\n\n    knn = KNeighborsClassifier(n_neighbors=1)\n\n    template_integrator = TemplateIntegrator(\n        template_y0=template_y0,\n        labels=classifier_labels,\n        ode_params=params,\n    )\n\n    return {\n        \"n\": n,\n        \"ode_system\": ode_system,\n        \"sampler\": sampler,\n        \"solver\": solver,\n        \"feature_extractor\": feature_extractor,\n        \"estimator\": knn,\n        \"template_integrator\": template_integrator,\n    }\n</code></pre>"},{"location":"case-studies/pendulum/#main-estimation","title":"Main Estimation","text":"<pre><code>def main():\n    props = setup_pendulum_system()\n\n    bse = BasinStabilityEstimator(\n        n=props[\"n\"],\n        ode_system=props[\"ode_system\"],\n        sampler=props[\"sampler\"],\n        solver=props.get(\"solver\"),\n        feature_extractor=props.get(\"feature_extractor\"),\n        predictor=props.get(\"estimator\"),\n        template_integrator=props.get(\"template_integrator\"),\n        save_to=\"results_case1\",\n        feature_selector=None,\n    )\n\n    bse.estimate_bs()\n\n    return bse\n</code></pre>"},{"location":"case-studies/pendulum/#case-1-baseline-results","title":"Case 1: Baseline Results","text":""},{"location":"case-studies/pendulum/#comparison-with-matlab-bstab","title":"Comparison with MATLAB bSTAB","text":"<p>Overall Classification Quality:</p> <ul> <li>Macro F1-score: 1.0000</li> <li>Matthews Correlation Coefficient: 1.0000</li> </ul> Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE F1 FP 0.1520 \u00b1 0.0036 0.1520 \u00b1 0.0036 1.0000 LC 0.8480 \u00b1 0.0036 0.8480 \u00b1 0.0036 1.0000"},{"location":"case-studies/pendulum/#visualizations","title":"Visualizations","text":""},{"location":"case-studies/pendulum/#basin-stability","title":"Basin Stability","text":""},{"location":"case-studies/pendulum/#state-space","title":"State Space","text":""},{"location":"case-studies/pendulum/#feature-space","title":"Feature Space","text":""},{"location":"case-studies/pendulum/#template-trajectories","title":"Template Trajectories","text":""},{"location":"case-studies/pendulum/#case-2-parameter-sweep","title":"Case 2: Parameter Sweep","text":""},{"location":"case-studies/pendulum/#comparison-with-matlab-bstab_1","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 1.0000</p> <p>The average excludes cases where there is only a single attractor and the basin stability values are the same since MCC is 0 for single class cases, and would therefore drop the average.</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 0.01 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 0.06 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 0.11 FP 1.0000 \u00b1 0.0000 1.0000 \u00b1 0.0000 0.0000 0.16 FP 0.4830 \u00b1 0.0050 0.4830 \u00b1 0.0050 1.0000 LC 0.5170 \u00b1 0.0050 0.5170 \u00b1 0.0050 0.21 FP 0.3967 \u00b1 0.0049 0.3967 \u00b1 0.0049 1.0000 LC 0.6033 \u00b1 0.0049 0.6033 \u00b1 0.0049 0.26 FP 0.3268 \u00b1 0.0047 0.3268 \u00b1 0.0047 1.0000 LC 0.6732 \u00b1 0.0047 0.6732 \u00b1 0.0047 0.31 FP 0.2756 \u00b1 0.0045 0.2756 \u00b1 0.0045 1.0000 LC 0.7244 \u00b1 0.0045 0.7244 \u00b1 0.0045 0.36 FP 0.2380 \u00b1 0.0043 0.2380 \u00b1 0.0043 1.0000 LC 0.7620 \u00b1 0.0043 0.7620 \u00b1 0.0043 0.41 FP 0.2065 \u00b1 0.0040 0.2065 \u00b1 0.0040 1.0000 LC 0.7935 \u00b1 0.0040 0.7935 \u00b1 0.0040 0.46 FP 0.1703 \u00b1 0.0038 0.1703 \u00b1 0.0038 1.0000 LC 0.8297 \u00b1 0.0038 0.8297 \u00b1 0.0038 0.51 FP 0.1470 \u00b1 0.0035 0.1470 \u00b1 0.0035 1.0000 LC 0.8530 \u00b1 0.0035 0.8530 \u00b1 0.0035 0.56 FP 0.1279 \u00b1 0.0033 0.1279 \u00b1 0.0033 1.0000 LC 0.8721 \u00b1 0.0033 0.8721 \u00b1 0.0033 0.61 FP 0.1034 \u00b1 0.0030 0.1034 \u00b1 0.0030 1.0000 LC 0.8966 \u00b1 0.0030 0.8966 \u00b1 0.0030 0.66 FP 0.0839 \u00b1 0.0028 0.0839 \u00b1 0.0028 1.0000 LC 0.9161 \u00b1 0.0028 0.9161 \u00b1 0.0028 0.71 FP 0.0653 \u00b1 0.0025 0.0653 \u00b1 0.0025 1.0000 LC 0.9347 \u00b1 0.0025 0.9347 \u00b1 0.0025 0.76 FP 0.0545 \u00b1 0.0023 0.0545 \u00b1 0.0023 1.0000 LC 0.9455 \u00b1 0.0023 0.9455 \u00b1 0.0023 0.81 FP 0.0391 \u00b1 0.0019 0.0391 \u00b1 0.0019 1.0000 LC 0.9609 \u00b1 0.0019 0.9609 \u00b1 0.0019 0.86 FP 0.0244 \u00b1 0.0015 0.0244 \u00b1 0.0015 1.0000 LC 0.9756 \u00b1 0.0015 0.9756 \u00b1 0.0015 0.91 FP 0.0172 \u00b1 0.0013 0.0172 \u00b1 0.0013 1.0000 LC 0.9828 \u00b1 0.0013 0.9828 \u00b1 0.0013 0.96 FP 0.0071 \u00b1 0.0008 0.0071 \u00b1 0.0008 1.0000 LC 0.9929 \u00b1 0.0008 0.9929 \u00b1 0.0008"},{"location":"case-studies/pendulum/#visualizations_1","title":"Visualizations","text":""},{"location":"case-studies/pendulum/#basin-stability-variation","title":"Basin Stability Variation","text":""},{"location":"case-studies/pendulum/#bifurcation-diagram","title":"Bifurcation Diagram","text":""},{"location":"case-studies/pendulum/#case-3-sample-size-convergence-study","title":"Case 3: Sample Size Convergence Study","text":"<p>This hyperparameter study varies the number of initial conditions \\(N\\) from ~50 to ~5000 (using \\(5 \\times \\text{logspace}(1, 3, 20)\\)) to assess how basin stability estimates converge as sample size increases. The relative standard error decreases as \\(\\text{SE}/\\mathcal{S}_{\\mathcal{B}} \\sim 1/\\sqrt{N}\\).</p>"},{"location":"case-studies/pendulum/#comparison-with-matlab-bstab_2","title":"Comparison with MATLAB bSTAB","text":"<p>Average MCC = 1.0000</p> Parameter Attractor pyBasin BS \u00b1 SE bSTAB BS \u00b1 SE MCC 50 FP 0.1000 \u00b1 0.0424 0.1000 \u00b1 0.0424 1.0000 LC 0.9000 \u00b1 0.0424 0.9000 \u00b1 0.0424 63.7137 FP 0.0938 \u00b1 0.0364 0.0938 \u00b1 0.0364 1.0000 LC 0.9062 \u00b1 0.0364 0.9062 \u00b1 0.0364 81.1888 FP 0.1585 \u00b1 0.0403 0.1585 \u00b1 0.0403 1.0000 LC 0.8415 \u00b1 0.0403 0.8415 \u00b1 0.0403 103.4569 FP 0.1731 \u00b1 0.0371 0.1731 \u00b1 0.0371 1.0000 LC 0.8269 \u00b1 0.0371 0.8269 \u00b1 0.0371 131.8325 FP 0.1515 \u00b1 0.0312 0.1515 \u00b1 0.0312 1.0000 LC 0.8485 \u00b1 0.0312 0.8485 \u00b1 0.0312 167.9909 FP 0.1667 \u00b1 0.0288 0.1667 \u00b1 0.0288 1.0000 LC 0.8333 \u00b1 0.0288 0.8333 \u00b1 0.0288 214.0666 FP 0.1256 \u00b1 0.0226 0.1256 \u00b1 0.0226 1.0000 LC 0.8744 \u00b1 0.0226 0.8744 \u00b1 0.0226 272.7797 FP 0.1319 \u00b1 0.0205 0.1319 \u00b1 0.0205 1.0000 LC 0.8681 \u00b1 0.0205 0.8681 \u00b1 0.0205 347.5964 FP 0.1408 \u00b1 0.0186 0.1408 \u00b1 0.0186 1.0000 LC 0.8592 \u00b1 0.0186 0.8592 \u00b1 0.0186 442.9334 FP 0.1625 \u00b1 0.0175 0.1625 \u00b1 0.0175 1.0000 LC 0.8375 \u00b1 0.0175 0.8375 \u00b1 0.0175 564.4189 FP 0.1611 \u00b1 0.0155 0.1611 \u00b1 0.0155 1.0000 LC 0.8389 \u00b1 0.0155 0.8389 \u00b1 0.0155 719.2249 FP 0.1528 \u00b1 0.0134 0.1528 \u00b1 0.0134 1.0000 LC 0.8472 \u00b1 0.0134 0.8472 \u00b1 0.0134 916.4904 FP 0.1265 \u00b1 0.0110 0.1265 \u00b1 0.0110 1.0000 LC 0.8735 \u00b1 0.0110 0.8735 \u00b1 0.0110 1167.8607 FP 0.1558 \u00b1 0.0106 0.1558 \u00b1 0.0106 1.0000 LC 0.8442 \u00b1 0.0106 0.8442 \u00b1 0.0106 1488.1757 FP 0.1659 \u00b1 0.0096 0.1659 \u00b1 0.0096 1.0000 LC 0.8341 \u00b1 0.0096 0.8341 \u00b1 0.0096 1896.3451 FP 0.1465 \u00b1 0.0081 0.1465 \u00b1 0.0081 1.0000 LC 0.8535 \u00b1 0.0081 0.8535 \u00b1 0.0081 2416.4651 FP 0.1618 \u00b1 0.0075 0.1618 \u00b1 0.0075 1.0000 LC 0.8382 \u00b1 0.0075 0.8382 \u00b1 0.0075 3079.2411 FP 0.1523 \u00b1 0.0065 0.1523 \u00b1 0.0065 1.0000 LC 0.8477 \u00b1 0.0065 0.8477 \u00b1 0.0065 3923.7999 FP 0.1585 \u00b1 0.0058 0.1585 \u00b1 0.0058 1.0000 LC 0.8415 \u00b1 0.0058 0.8415 \u00b1 0.0058 5000 FP 0.1618 \u00b1 0.0052 0.1618 \u00b1 0.0052 1.0000 LC 0.8382 \u00b1 0.0052 0.8382 \u00b1 0.0052"},{"location":"case-studies/pendulum/#visualizations_2","title":"Visualizations","text":""},{"location":"case-studies/pendulum/#basin-stability-variation_1","title":"Basin Stability Variation","text":""},{"location":"case-studies/pendulum/#bifurcation-diagram_1","title":"Bifurcation Diagram","text":""},{"location":"case-studies/rossler-network/","title":"R\u00f6ssler Network","text":""},{"location":"case-studies/rossler-network/#system-description","title":"System Description","text":"<p>Network of 100 coupled R\u00f6ssler oscillators studying synchronization dynamics:</p> \\[ \\begin{aligned} \\dot{x}_i &amp;= -y_i - z_i + K \\sum_{j \\in \\mathcal{N}_i} (x_j - x_i) \\\\ \\dot{y}_i &amp;= x_i + ay_i \\\\ \\dot{z}_i &amp;= b + z_i(x_i - c) \\end{aligned} \\] <p>where \\(i = 1, \\ldots, 100\\) and \\(\\mathcal{N}_i\\) denotes the neighbors of node \\(i\\) in the network.</p>"},{"location":"case-studies/rossler-network/#system-parameters","title":"System Parameters","text":"Parameter Symbol Value R\u00f6ssler parameter \\(a\\) 0.2 R\u00f6ssler parameter \\(b\\) 0.2 R\u00f6ssler parameter \\(c\\) 7.0 Coupling strength \\(K\\) 0.218 (baseline) Network topology \u2014 Scale-free, 100 nodes"},{"location":"case-studies/rossler-network/#sampling","title":"Sampling","text":"<ul> <li>Dimension: \\(D = 300\\) (3 states \\(\\times\\) 100 nodes)</li> <li>Sample size: \\(N = 500\\)</li> <li>Distribution: \\(\\rho\\) = Uniform</li> <li>Region of interest: \\(\\mathcal{Q}(x_i, y_i, z_i) : [-15, 15] \\times [-15, 15] \\times [-5, 35]\\) per node</li> </ul>"},{"location":"case-studies/rossler-network/#solver","title":"Solver","text":"Setting Value Method Dopri5 (Diffrax) Time span \\([0, 1000]\\) Steps 1000 (\\(f_s\\) = 1 Hz) Relative tolerance 1e-03 Absolute tolerance 1e-06 Event function Divergence at \\(\\lvert y \\rvert &gt; 400\\)"},{"location":"case-studies/rossler-network/#feature-extraction","title":"Feature Extraction","text":"<p>Maximum pairwise deviation at final time step:</p> <ul> <li>States: all \\(x_i, y_i, z_i\\)</li> <li>Formula: \\(\\Delta_x = \\max_i(x_i) - \\min_i(x_i)\\), similarly for \\(y\\), \\(z\\); plus \\(\\Delta_{\\text{all}} = \\max(\\Delta_x, \\Delta_y, \\Delta_z)\\)</li> <li>Transient cutoff: \\(t^* = 950.0\\)</li> </ul>"},{"location":"case-studies/rossler-network/#clustering","title":"Clustering","text":"<ul> <li>Method: Threshold classifier (<code>SynchronizationClassifier</code>)</li> <li>Threshold: \\(\\epsilon = 1.5\\)</li> <li>Rule: Synchronized if \\(\\Delta_{\\text{all}} &lt; \\epsilon\\), desynchronized otherwise</li> </ul>"},{"location":"case-studies/rossler-network/#attractors","title":"Attractors","text":"<p>The system exhibits three types of behavior:</p> <ul> <li>Synchronized: All oscillators converge to a common trajectory</li> <li>Desynchronized: Oscillators remain coupled but do not synchronize</li> <li>Unbounded: Some trajectories diverge to infinity (detected by event function)</li> </ul> <p>Basin stability is computed for non-unbounded states (synchronized + desynchronized).</p>"},{"location":"case-studies/rossler-network/#reproduction-code","title":"Reproduction Code","text":""},{"location":"case-studies/rossler-network/#setup","title":"Setup","text":"<pre><code>def setup_rossler_network_system() -&gt; SetupProperties:\n    \"\"\"Setup the R\u00f6ssler network system for basin stability estimation.\n\n    Uses coupling strength K=0.218 (expected S_B \u2248 0.496 from paper).\n\n    :return: Configuration dictionary for BasinStabilityEstimator.\n    \"\"\"\n    k = 0.218\n    n = 500\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up R\u00f6ssler network system on device: {device}\")\n    print(f\"  N = {N_NODES} nodes, k = {k}\")\n\n    params: RosslerNetworkParams = {\n        \"a\": 0.2,\n        \"b\": 0.2,\n        \"c\": 7.0,\n        \"K\": k,\n        \"edges_i\": EDGES_I,\n        \"edges_j\": EDGES_J,\n        \"N\": N_NODES,\n    }\n\n    ode_system = RosslerNetworkJaxODE(params)\n\n    min_limits = (\n        [-15.0] * N_NODES  # x_i in [-15, 15]\n        + [-15.0] * N_NODES  # y_i in [-15, 15]\n        + [-5.0] * N_NODES  # z_i in [-5, 35]\n    )\n    max_limits = (\n        [15.0] * N_NODES  # x_i\n        + [15.0] * N_NODES  # y_i\n        + [35.0] * N_NODES  # z_i\n    )\n\n    sampler = UniformRandomSampler(\n        min_limits=min_limits,\n        max_limits=max_limits,\n        device=device,\n    )\n\n    solver = JaxSolver(\n        time_span=(0, 1000),\n        n_steps=1000,\n        device=device,\n        rtol=1e-3,\n        atol=1e-6,\n        cache_dir=\".pybasin_cache/rossler_network\",\n        event_fn=rossler_stop_event,\n    )\n\n    feature_extractor = SynchronizationFeatureExtractor(\n        n_nodes=N_NODES,\n        time_steady=950,\n        device=device,\n    )\n\n    sync_classifier = SynchronizationClassifier(\n        epsilon=1.5,\n    )\n\n    return {\n        \"n\": n,\n        \"ode_system\": ode_system,\n        \"sampler\": sampler,\n        \"solver\": solver,\n        \"feature_extractor\": feature_extractor,\n        \"estimator\": sync_classifier,\n    }\n</code></pre>"},{"location":"case-studies/rossler-network/#single-k-value","title":"Single K Value","text":"<pre><code>def main() -&gt; BasinStabilityEstimator:\n    \"\"\"Run basin stability estimation for R\u00f6ssler network.\n\n    Uses coupling strength K=0.218 (expected S_B \u2248 0.496 from paper).\n\n    :return: Basin stability estimator with results.\n    \"\"\"\n    props = setup_rossler_network_system()\n\n    bse = BasinStabilityEstimator(\n        n=props[\"n\"],\n        ode_system=props[\"ode_system\"],\n        sampler=props[\"sampler\"],\n        solver=props.get(\"solver\"),\n        feature_extractor=props.get(\"feature_extractor\"),\n        predictor=props.get(\"estimator\"),\n        template_integrator=props.get(\"template_integrator\"),\n        save_to=\"results\",\n        feature_selector=None,\n        detect_unbounded=False,\n    )\n\n    bse.estimate_bs()\n\n    return bse\n</code></pre>"},{"location":"case-studies/rossler-network/#k-parameter-sweep","title":"K Parameter Sweep","text":"<pre><code>def main() -&gt; BasinStabilityStudy:\n    \"\"\"Run adaptive parameter study for R\u00f6ssler network coupling strength.\n\n    Sweeps through K values from paper to analyze basin stability variation.\n\n    :return: Adaptive basin stability estimator with results.\n    \"\"\"\n    props = setup_rossler_network_system()\n\n    study_params = SweepStudyParams(\n        name='ode_system.params[\"K\"]',\n        values=K_VALUES_FROM_PAPER.tolist(),\n    )\n\n    solver = props.get(\"solver\")\n    feature_extractor = props.get(\"feature_extractor\")\n    estimator = props.get(\"estimator\")\n    template_integrator = props.get(\"template_integrator\")\n    assert solver is not None\n    assert feature_extractor is not None\n    assert estimator is not None\n\n    bse = BasinStabilityStudy(\n        n=props[\"n\"],\n        ode_system=props[\"ode_system\"],\n        sampler=props[\"sampler\"],\n        solver=solver,\n        feature_extractor=feature_extractor,\n        estimator=estimator,\n        study_params=study_params,\n        template_integrator=template_integrator,\n        save_to=\"results_k_study\",\n    )\n\n    bse.estimate_as_bs()\n\n    return bse\n</code></pre>"},{"location":"case-studies/rossler-network/#baseline-results-k0218","title":"Baseline Results (K=0.218)","text":""},{"location":"case-studies/rossler-network/#comparison-with-paper-results","title":"Comparison with Paper Results","text":"Attractor pyBasin BS \u00b1 SE Paper BS \u00b1 SE Difference 95% CI Status synchronized 0.4840 \u00b1 0.0223 0.4960 \u00b1 0.0224 -0.0120 \u00b10.0620 \u2713 unbounded 0.5160 \u00b1 0.0223 0.5040 \u00b1 0.0224 +0.0120 \u00b10.0620 \u2713"},{"location":"case-studies/rossler-network/#visualizations","title":"Visualizations","text":""},{"location":"case-studies/rossler-network/#basin-stability","title":"Basin Stability","text":""},{"location":"case-studies/rossler-network/#state-space","title":"State Space","text":""},{"location":"case-studies/rossler-network/#feature-space","title":"Feature Space","text":""},{"location":"case-studies/rossler-network/#k-parameter-sweep_1","title":"K Parameter Sweep","text":""},{"location":"case-studies/rossler-network/#comparison-with-paper-results_1","title":"Comparison with Paper Results","text":"Parameter Attractor pyBasin BS \u00b1 SE Paper BS \u00b1 SE Difference 95% CI Status 0.119 synchronized 0.0780 \u00b1 0.0120 0.2260 \u00b1 0.0187 -0.1480 \u00b10.0435 \u2717 unbounded 0.7980 \u00b1 0.0180 0.7740 \u00b1 0.0187 +0.0240 \u00b10.0508 0.139 synchronized 0.2220 \u00b1 0.0186 0.2740 \u00b1 0.0199 -0.0520 \u00b10.0534 \u2713 unbounded 0.7520 \u00b1 0.0193 0.7260 \u00b1 0.0199 +0.0260 \u00b10.0544 0.159 synchronized 0.3020 \u00b1 0.0205 0.3300 \u00b1 0.0210 -0.0280 \u00b10.0576 \u2713 unbounded 0.6920 \u00b1 0.0206 0.6700 \u00b1 0.0210 +0.0220 \u00b10.0578 0.179 synchronized 0.3600 \u00b1 0.0215 0.3460 \u00b1 0.0213 +0.0140 \u00b10.0592 \u2713 unbounded 0.6380 \u00b1 0.0215 0.6540 \u00b1 0.0213 -0.0160 \u00b10.0593 0.198 synchronized 0.4200 \u00b1 0.0221 0.4720 \u00b1 0.0223 -0.0520 \u00b10.0615 \u2713 unbounded 0.5800 \u00b1 0.0221 0.5280 \u00b1 0.0223 +0.0520 \u00b10.0615 0.218 synchronized 0.4840 \u00b1 0.0223 0.4960 \u00b1 0.0224 -0.0120 \u00b10.0620 \u2713 unbounded 0.5160 \u00b1 0.0223 0.5040 \u00b1 0.0224 +0.0120 \u00b10.0620 0.238 synchronized 0.5500 \u00b1 0.0222 0.5940 \u00b1 0.0220 -0.0440 \u00b10.0613 \u2713 unbounded 0.4500 \u00b1 0.0222 0.4060 \u00b1 0.0220 +0.0440 \u00b10.0613 0.258 synchronized 0.6080 \u00b1 0.0218 0.6280 \u00b1 0.0216 -0.0200 \u00b10.0602 \u2713 unbounded 0.3920 \u00b1 0.0218 0.3720 \u00b1 0.0216 +0.0200 \u00b10.0602 0.278 synchronized 0.6560 \u00b1 0.0212 0.6560 \u00b1 0.0212 +0.0000 \u00b10.0589 \u2713 unbounded 0.3440 \u00b1 0.0212 0.3440 \u00b1 0.0212 +0.0000 \u00b10.0589 0.297 synchronized 0.7120 \u00b1 0.0203 0.6940 \u00b1 0.0206 +0.0180 \u00b10.0566 \u2713 unbounded 0.2880 \u00b1 0.0203 0.3060 \u00b1 0.0206 -0.0180 \u00b10.0566 0.317 synchronized 0.6780 \u00b1 0.0209 0.6900 \u00b1 0.0207 -0.0120 \u00b10.0576 \u2713 unbounded 0.2840 \u00b1 0.0202 0.3100 \u00b1 0.0207 -0.0260 \u00b10.0566"},{"location":"case-studies/rossler-network/#visualizations_1","title":"Visualizations","text":""},{"location":"case-studies/rossler-network/#basin-stability-variation","title":"Basin Stability Variation","text":""},{"location":"case-studies/rossler-network/#references","title":"References","text":"<p>Menck, P. J., Heitzig, J., Marwan, N., &amp; Kurths, J. (2013). How basin stability complements the linear-stability paradigm. Nature Physics, 9(2), 89-92.</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>TODO</p> <p>This section is under development.</p>"},{"location":"development/contributing/#adding-a-new-case-study","title":"Adding a New Case Study","text":"<p>To add a new case study:</p> <ol> <li>Create a new directory under <code>case_studies/</code></li> <li>Implement the ODE system and feature extractor</li> <li>Create a main script that runs the analysis</li> <li>Add corresponding integration test</li> <li>Document in the case studies section</li> </ol>"},{"location":"development/contributing/#development-guidelines","title":"Development Guidelines","text":"<p>More information coming soon.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>pip or uv package manager</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<p>Once published, you'll be able to install pyBasin using pip:</p> <pre><code>pip install pybasin\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":""},{"location":"getting-started/installation/#using-uv-recommended","title":"Using UV (Recommended)","text":"<pre><code># Clone the repository\ngit clone https://github.com/adrianwix/pyBasin.git\ncd pyBasinWorkspace\n\n# Install with UV\nuv add -e .\n\n# Or install with all optional dependencies\nuv add -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/adrianwix/pyBasin.git\ncd pyBasinWorkspace\n\n# Install in editable mode\npip install -e .\n\n# Or install with all optional dependencies\npip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>Install testing and linting tools:</p> <pre><code>uv add -e \".[dev]\"\n</code></pre> <p>Includes:</p> <ul> <li>pytest</li> <li>pytest-cov</li> <li>mypy</li> <li>ruff</li> <li>black</li> </ul>"},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>Install documentation building tools:</p> <pre><code>uv add -e \".[docs]\"\n</code></pre> <p>Includes:</p> <ul> <li>mkdocs-material</li> <li>mkdocstrings</li> <li>mkdocs-jupyter</li> </ul>"},{"location":"getting-started/installation/#case-studies","title":"Case Studies","text":"<p>Install additional dependencies for running case studies:</p> <pre><code>uv add -e \".[case-studies]\"\n</code></pre> <p>Includes:</p> <ul> <li>jupyter</li> <li>openpyxl</li> <li>notebook</li> </ul>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>import pybasin\nprint(pybasin.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Quick Start guide</li> <li>Explore the API Reference</li> <li>Run the Case Studies</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started with pyBasin in just a few minutes.</p>"},{"location":"getting-started/quickstart/#basic-example","title":"Basic Example","text":"<p>Here's a simple example of estimating basin stability for a 2D dynamical system:</p> <pre><code>import numpy as np\nfrom pybasin import BasinStabilityEstimator, ODESystem\n\n# Step 1: Define your dynamical system\nclass SimpleSystem(ODESystem):\n    \"\"\"A simple 2D system with two stable fixed points.\"\"\"\n\n    def dynamics(self, t, state):\n        \"\"\"Define the differential equations.\"\"\"\n        x, y = state\n        dx = x * (1 - x**2 - y**2)\n        dy = y * (1 - x**2 - y**2)\n        return np.array([dx, dy])\n\n    def classify_attractor(self, solution):\n        \"\"\"Classify which attractor the solution reached.\"\"\"\n        final_state = solution.y[:, -1]\n        x_final, y_final = final_state\n\n        # Classify based on final position\n        if x_final &gt; 0:\n            return 0  # Right attractor\n        else:\n            return 1  # Left attractor\n\n# Step 2: Create the estimator\nsystem = SimpleSystem()\nestimator = BasinStabilityEstimator(\n    system=system,\n    t_span=(0, 50),  # Integration time\n    n_samples=1000   # Number of initial conditions\n)\n\n# Step 3: Define the sampling region\nbounds = [(-2, 2), (-2, 2)]  # [x_min, x_max], [y_min, y_max]\n\n# Step 4: Estimate basin stability\nresults = estimator.estimate(bounds)\n\n# Step 5: Analyze results\nprint(f\"Basin Stability (Attractor 0): {results.basin_stability[0]:.3f}\")\nprint(f\"Basin Stability (Attractor 1): {results.basin_stability[1]:.3f}\")\nprint(f\"Total samples: {results.n_samples}\")\nprint(f\"Attractor distribution: {results.attractor_counts}\")\n\n# Step 6: Visualize\nresults.plot_basin_2d()\n</code></pre>"},{"location":"getting-started/quickstart/#using-adaptive-sampling","title":"Using Adaptive Sampling","text":"<p>For more efficient sampling, use the adaptive sampling estimator:</p> <pre><code>from pybasin import BasinStabilityStudy\n\n# Create adaptive sampling estimator\nas_estimator = BasinStabilityStudy(\n    system=system,\n    initial_samples=100,\n    max_samples=1000,\n    uncertainty_threshold=0.1\n)\n\n# Estimate with adaptive sampling\nas_results = as_estimator.estimate(bounds)\n\nprint(f\"Samples used: {as_results.n_samples}\")\nprint(f\"Convergence achieved: {as_results.converged}\")\n</code></pre>"},{"location":"getting-started/quickstart/#custom-feature-extraction","title":"Custom Feature Extraction","text":"<p>You can define custom features for better classification:</p> <pre><code>from pybasin import FeatureExtractor\n\nclass MyFeatureExtractor(FeatureExtractor):\n    \"\"\"Extract custom features from solutions.\"\"\"\n\n    def extract(self, solution):\n        \"\"\"Extract features from the solution.\"\"\"\n        t = solution.t\n        y = solution.y\n\n        features = {\n            'final_x': y[0, -1],\n            'final_y': y[1, -1],\n            'max_distance': np.max(np.sqrt(y[0]**2 + y[1]**2)),\n            'period': self._estimate_period(t, y),\n        }\n        return features\n\n    def _estimate_period(self, t, y):\n        \"\"\"Estimate the period of oscillation.\"\"\"\n        # Your period estimation logic here\n        return 0.0\n\n# Use custom feature extractor\nestimator = BasinStabilityEstimator(\n    system=system,\n    feature_extractor=MyFeatureExtractor()\n)\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-high-dimensional-systems","title":"Working with High-Dimensional Systems","text":"<p>For systems with more than 2 dimensions:</p> <pre><code>class LorenzSystem(ODESystem):\n    \"\"\"The Lorenz system.\"\"\"\n\n    def __init__(self, sigma=10, rho=28, beta=8/3):\n        self.sigma = sigma\n        self.rho = rho\n        self.beta = beta\n\n    def dynamics(self, t, state):\n        x, y, z = state\n        dx = self.sigma * (y - x)\n        dy = x * (self.rho - z) - y\n        dz = x * y - self.beta * z\n        return np.array([dx, dy, dz])\n\n    def classify_attractor(self, solution):\n        # Classification logic for Lorenz attractors\n        final_state = solution.y[:, -1]\n        if final_state[2] &gt; 20:\n            return 0\n        return 1\n\n# Sample in 3D space\nlorenz = LorenzSystem()\nestimator = BasinStabilityEstimator(lorenz)\nbounds = [(-20, 20), (-30, 30), (0, 50)]\nresults = estimator.estimate(bounds)\n</code></pre>"},{"location":"getting-started/quickstart/#saving-and-loading-results","title":"Saving and Loading Results","text":"<pre><code># Save results\nresults.save('my_results.json')\n\n# Load results\nfrom pybasin import BasinStabilityResult\nloaded_results = BasinStabilityResult.load('my_results.json')\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference</li> <li>Check out the Case Studies for real-world examples</li> </ul>"},{"location":"guides/custom-feature-extractor/","title":"Creating Custom Feature Extractors","text":"<p>Feature extractors transform ODE solution trajectories into feature vectors used for basin of attraction classification. This guide shows how to create your own.</p>"},{"location":"guides/custom-feature-extractor/#basic-implementation","title":"Basic Implementation","text":"<p>To create a custom feature extractor, subclass <code>FeatureExtractor</code> and implement the <code>extract_features</code> method:</p> <pre><code>import torch\nfrom pybasin.feature_extractors.feature_extractor import FeatureExtractor\nfrom pybasin.solution import Solution\n\n\nclass AmplitudeFeatureExtractor(FeatureExtractor):\n    def extract_features(self, solution: Solution) -&gt; torch.Tensor:\n        # Filter out transient behavior\n        y_filtered: torch.Tensor = self.filter_time(solution)\n\n        # Compute features - here we extract max amplitude per state\n        # y_filtered shape: (n_times, n_samples, n_states)\n        max_amplitude: torch.Tensor = torch.max(torch.abs(y_filtered), dim=0).values\n\n        # Set _num_features for automatic feature naming\n        self._num_features = max_amplitude.shape[1]\n\n        # Return shape: (n_samples, n_features)\n        return max_amplitude\n</code></pre>"},{"location":"guides/custom-feature-extractor/#key-points","title":"Key Points","text":"<ol> <li>Use <code>filter_time</code>: Call <code>self.filter_time(solution)</code> to remove transient dynamics based on <code>time_steady</code></li> <li>Return a tensor: The return type must be <code>torch.Tensor</code> with shape <code>(n_samples, n_features)</code></li> <li>Set <code>_num_features</code>: Assign <code>self._num_features</code> to enable automatic feature naming</li> <li>Do NOT modify the Solution object: The <code>extract_features</code> method should be pure - read from the solution, compute features, and return them. Never assign to <code>solution.features</code>, <code>solution.extracted_features</code>, or any other solution attributes.</li> </ol>"},{"location":"guides/custom-feature-extractor/#using-the-extractor","title":"Using the Extractor","text":"<pre><code>extractor = AmplitudeFeatureExtractor(time_steady=100.0)\nfeatures = extractor.extract_features(solution)\n\n# Feature names are automatically generated\nprint(extractor.feature_names)  # ['amplitude_1', 'amplitude_2', ...]\n</code></pre>"},{"location":"guides/custom-feature-extractor/#custom-feature-names","title":"Custom Feature Names","text":"<p>By default, feature names are generated automatically from the class name:</p> <ul> <li><code>AmplitudeFeatureExtractor</code> \u2192 <code>amplitude_1</code>, <code>amplitude_2</code>, ...</li> <li><code>SynchronizationFeatureExtractor</code> \u2192 <code>synchronization_1</code>, <code>synchronization_2</code>, ...</li> </ul>"},{"location":"guides/custom-feature-extractor/#overriding-feature-names","title":"Overriding Feature Names","text":"<p>To use custom, meaningful names, set <code>_feature_names</code> in <code>__init__</code>:</p> <pre><code>class SynchronizationFeatureExtractor(FeatureExtractor):\n    def __init__(\n        self,\n        n_nodes: int,\n        time_steady: float = 1000.0,\n    ):\n        super().__init__(time_steady=time_steady)\n        self.n_nodes = n_nodes\n        # Define custom feature names\n        self._feature_names = [\n            \"max_deviation_x\",\n            \"max_deviation_y\",\n            \"max_deviation_z\",\n            \"max_deviation_all\",\n        ]\n\n    def extract_features(self, solution: Solution) -&gt; torch.Tensor:\n        y_filtered: torch.Tensor = self.filter_time(solution)\n        # ... compute features ...\n        return features  # shape: (n_samples, 4)\n</code></pre> <p>When <code>_feature_names</code> is set, it takes precedence over automatic name generation.</p>"},{"location":"guides/custom-feature-extractor/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nfrom pybasin.feature_extractors.feature_extractor import FeatureExtractor\nfrom pybasin.solution import Solution\n\n\nclass MeanAndStdFeatureExtractor(FeatureExtractor):\n    \"\"\"Extract mean and standard deviation of each state variable.\"\"\"\n\n    def __init__(self, n_states: int, time_steady: float = 0.0):\n        super().__init__(time_steady=time_steady)\n        self.n_states = n_states\n        # Custom names: mean_0, std_0, mean_1, std_1, ...\n        self._feature_names = []\n        for i in range(n_states):\n            self._feature_names.extend([f\"mean_{i}\", f\"std_{i}\"])\n\n    def extract_features(self, solution: Solution) -&gt; torch.Tensor:\n        y_filtered: torch.Tensor = self.filter_time(solution)\n        # y_filtered: (n_times, n_samples, n_states)\n\n        mean_vals: torch.Tensor = y_filtered.mean(dim=0)  # (n_samples, n_states)\n        std_vals: torch.Tensor = y_filtered.std(dim=0)    # (n_samples, n_states)\n\n        # Interleave: [mean_0, std_0, mean_1, std_1, ...]\n        features: torch.Tensor = torch.stack(\n            [mean_vals, std_vals], dim=2\n        ).reshape(mean_vals.shape[0], -1)\n\n        return features\n</code></pre>"},{"location":"guides/dynamics-based-clustering/","title":"Dynamics-Based Clustering","text":""},{"location":"guides/dynamics-based-clustering/#overview","title":"Overview","text":"<p>Standard clustering predictors like <code>HDBSCANClusterer</code> group trajectories purely by statistical similarity in feature space. This works well when attractors produce visibly different feature distributions, but it has no awareness of what the features mean physically. A fixed point, a limit cycle, and a chaotic attractor might overlap in some statistical features yet differ fundamentally in their dynamics.</p> <p><code>DynamicalSystemClusterer</code> takes a different approach: a two-stage hierarchical classification that first identifies the type of attractor (fixed point, limit cycle, or chaos) and then sub-classifies within each type. Because Stage 1 uses physics-based heuristics -- variance thresholds, periodicity measures, drift detection -- it can separate attractors that generic clustering merges.</p> <p>This guide covers the full setup: the required feature configuration, the clusterer itself, how the two stages work, and tuning advice.</p>"},{"location":"guides/dynamics-based-clustering/#when-to-use-this","title":"When to Use This","text":"<p>Reach for dynamics-based clustering when:</p> <ul> <li>The system has multiple attractor types (e.g., fixed points coexisting with limit cycles)</li> <li>Generic clusterers merge attractors that differ in dynamical character but overlap in simple statistics</li> <li>You want physically interpretable labels like <code>FP_0</code>, <code>LC_1</code>, <code>chaos_0</code> rather than opaque integer IDs</li> </ul> <p>Standard <code>HDBSCANClusterer</code> or <code>DBSCANClusterer</code> remain better choices for exploratory analysis where attractor types are unknown, or for systems where all attractors are of the same type (e.g., multiple fixed points that only differ in location).</p>"},{"location":"guides/dynamics-based-clustering/#required-feature-configuration","title":"Required Feature Configuration","text":"<p><code>DynamicalSystemClusterer</code> reads specific features by name. It expects feature names following the <code>state_X__feature_name</code> convention, and it requires these seven base features to be present for at least one state variable:</p> Required feature What it provides to the clusterer <code>variance</code> Fixed point detection (near-zero variance = FP) <code>amplitude</code> Limit cycle sub-classification by oscillation size <code>mean</code> Spatial location for FP and chaos sub-classification <code>linear_trend__attr_slope</code> Drift detection -- identifies rotating solutions <code>autocorrelation_periodicity__output_strength</code> Periodicity strength [0, 1] for LC detection <code>autocorrelation_periodicity__output_period</code> Dominant period for LC sub-grouping <code>spectral_frequency_ratio</code> Period-n detection (period-1 vs period-2 limit cycles, etc.) <p>The <code>DYNAMICAL_SYSTEM_FC_PARAMETERS</code> preset in <code>pybasin.ts_torch.settings</code> produces exactly these features. It yields 7 feature columns per state variable (since <code>autocorrelation_periodicity</code> generates two outputs: strength and period). The preset is defined as:</p> <pre><code>DYNAMICAL_SYSTEM_FC_PARAMETERS: FCParameters = {\n    \"variance\": None,\n    \"amplitude\": None,\n    \"mean\": None,\n    \"linear_trend\": [{\"attr\": \"slope\"}],\n    \"autocorrelation_periodicity\": [\n        {\"output\": \"strength\"},\n        {\"output\": \"period\"},\n    ],\n    \"spectral_frequency_ratio\": None,\n}\n</code></pre> <p>For a 2D system, this produces 14 feature columns -- <code>state_0__variance</code>, <code>state_0__amplitude</code>, ..., <code>state_1__spectral_frequency_ratio</code>. A 3D system like Lorenz would yield 21. See the <code>TorchFeatureExtractor</code> API for details on feature naming and extraction.</p>"},{"location":"guides/dynamics-based-clustering/#full-setup","title":"Full Setup","text":"<p>Both the feature extractor and the predictor must be configured together. The extractor must produce the features the clusterer expects:</p> <pre><code>from pybasin.basin_stability_estimator import BasinStabilityEstimator\nfrom pybasin.ts_torch.torch_feature_extractor import TorchFeatureExtractor\nfrom pybasin.predictors import DynamicalSystemClusterer\nfrom pybasin.ts_torch.settings import DYNAMICAL_SYSTEM_FC_PARAMETERS\n\n# Feature extractor with the required feature set\nextractor = TorchFeatureExtractor(\n    features=DYNAMICAL_SYSTEM_FC_PARAMETERS,\n    time_steady=800.0,\n    normalize=False,  # recommended -- keeps variance thresholds interpretable\n)\n\n# The dynamics-based clusterer with default parameters\npredictor = DynamicalSystemClusterer()\n\n# Wire both into the estimator\nbse = BasinStabilityEstimator(\n    n=10_000,\n    ode_system=ode_system,\n    sampler=sampler,\n    solver=solver,\n    feature_extractor=extractor,\n    predictor=predictor,\n)\n\nbs_vals = bse.estimate_bs()\n# Labels will be like: {\"FP_0\": 0.35, \"LC_0\": 0.45, \"LC_1\": 0.15, \"chaos_0\": 0.05}\n</code></pre> <p>The <code>BasinStabilityEstimator</code> automatically calls <code>set_feature_names()</code> on the clusterer before prediction, so there is no manual wiring step for feature names.</p> <p>Mismatched features</p> <p>If the feature extractor does not produce the required features, <code>DynamicalSystemClusterer</code> raises a <code>ValueError</code> at classification time listing which features are missing. The <code>DYNAMICAL_SYSTEM_FC_PARAMETERS</code> preset is the simplest way to guarantee compatibility.</p> <p>Normalization</p> <p>Setting <code>normalize=False</code> on the feature extractor is recommended when using <code>DynamicalSystemClusterer</code>. The default thresholds (<code>fp_variance_threshold=1e-6</code>, etc.) are calibrated for raw, unnormalized feature values. With normalization enabled, variance gets z-score scaled and the thresholds would need manual adjustment.</p>"},{"location":"guides/dynamics-based-clustering/#algorithm","title":"Algorithm","text":"<p>The classification proceeds in three stages: drift detection (Stage 0), attractor type assignment (Stage 1), and sub-classification within each type (Stage 2). The final output is a label array where each trajectory receives a string label like <code>FP_0</code>, <code>LC_2</code>, or <code>chaos_1</code>.</p>"},{"location":"guides/dynamics-based-clustering/#stage-0-drift-detection","title":"Stage 0: Drift Detection","text":"<p>Before any classification, the clusterer scans for drifting dimensions -- state variables where a significant fraction of trajectories show monotonic trends. A rotating pendulum is the canonical example: the angle variable increases without bound even though the underlying attractor (continuous rotation) is well-defined.</p> <p>The detection works as follows. For each state dimension \\(i\\), the clusterer reads the <code>linear_trend__attr_slope</code> feature and computes the fraction of trajectories whose absolute slope exceeds <code>drift_threshold</code> (i.e., \\(|\\text{slope}_i| &gt; \\texttt{drift_threshold}\\)). If more than 30% (<code>drift_fraction=0.3</code>) of trajectories in that dimension have a high slope, dimension \\(i\\) is flagged as drifting. All subsequent variance and mean calculations for FP and chaos classification exclude drifting dimensions, since their variance is inflated by the drift rather than by genuine attractor dynamics.</p> <p>If all dimensions are drifting, the clusterer falls back to using all of them to avoid discarding all information.</p>"},{"location":"guides/dynamics-based-clustering/#stage-1-attractor-type-classification","title":"Stage 1: Attractor Type Classification","text":"<p>Every trajectory is assigned exactly one of three types: FP, LC, or chaos. The <code>tiers</code> parameter controls which types are active and in what priority order. The default is <code>[\"FP\", \"LC\", \"chaos\"]</code>, meaning FP is checked first, then LC, and anything unmatched falls into chaos.</p> <p>Three aggregate features are computed per trajectory before the tier checks:</p> <ul> <li>Variance: the mean of per-dimension <code>variance</code> values across non-drifting dimensions only.</li> <li>Periodicity strength: the <code>autocorrelation_periodicity__output_strength</code> value from the first non-drifting dimension. Ranges from 0 (no periodic pattern) to 1 (perfect periodicity).</li> <li>Max absolute slope: the maximum of <code>|linear_trend__attr_slope|</code> across all dimensions, including drifting ones.</li> </ul> <p>The tiers are checked in order. Each trajectory starts labeled as \"chaos\" and the first matching tier overwrites that label. Trajectories already claimed by an earlier tier are skipped by later ones.</p> <p>Fixed Point (FP):</p> \\[\\text{variance} &lt; \\texttt{fp_variance_threshold}\\] <p>A trajectory whose steady-state variance falls below <code>fp_variance_threshold</code> (default <code>1e-6</code>) has settled to a constant value. For unnormalized features from a well-converged integration, <code>1e-6</code> catches fixed points reliably.</p> <p>Limit Cycle (LC): A trajectory is classified as LC if either condition holds:</p> <ol> <li>Periodic oscillation: periodicity strength \\(&gt;\\) <code>lc_periodicity_threshold</code> AND variance \\(&lt;\\) <code>chaos_variance_threshold</code></li> <li>Rotating solution: max |slope| \\(&gt;\\) <code>drift_threshold</code> AND variance \\(&gt;\\) <code>fp_variance_threshold</code></li> </ol> <p>The first condition captures standard limit cycles -- regular oscillations with strong periodicity and bounded amplitude. The <code>chaos_variance_threshold</code> (default <code>5.0</code>) serves as the upper bound here: a trajectory with high periodicity but extremely large variance is more likely chaotic than a clean limit cycle, so it gets excluded. The second condition catches rotating solutions (e.g., a pendulum spinning over the top) where the angle drifts monotonically but the trajectory is still periodic in a phase-space sense. The lower bound <code>variance &gt; fp_variance_threshold</code> prevents fixed points from being misclassified as rotating LCs.</p> <p>Chaos: Everything not matched by FP or LC remains labeled \"chaos.\" Typically these trajectories have high variance combined with low periodicity strength. Note that \"chaos\" here is a catch-all for non-FP, non-LC behavior -- it will also capture unbounded trajectories unless those are handled separately (see <code>UnboundednessMetaEstimator</code>).</p> <p>Because tiers are checked in order, the ordering matters. With <code>tiers=[\"LC\", \"FP\"]</code>, a trajectory with both near-zero variance and high periodicity would be labeled LC, not FP.</p>"},{"location":"guides/dynamics-based-clustering/#stage-2-sub-classification","title":"Stage 2: Sub-classification","text":"<p>After Stage 1, each attractor type may contain multiple distinct attractors. This stage separates them. The labeling is sequential across all types -- if FP sub-classification produces labels 0 and 1, the first LC sub-cluster starts at label 2. This gives globally unique labels like <code>FP_0</code>, <code>FP_1</code>, <code>LC_2</code>, <code>LC_3</code>.</p>"},{"location":"guides/dynamics-based-clustering/#fixed-point-sub-classification","title":"Fixed Point Sub-classification","text":"<p>Fixed points are clustered by their steady-state location, using the <code>mean</code> feature across non-drifting dimensions.</p> <p>Before running any clustering, the algorithm checks whether the data range (max - min) of mean values in every dimension falls below 0.01. If so, all trajectories go into a single cluster -- this avoids spurious splits when all fixed points converge to the same location.</p> <p>Otherwise, HDBSCAN runs on StandardScaler-normalized mean values with <code>min_cluster_size = max(50, n // 10)</code>, where <code>n</code> is the number of FP trajectories. Noise points are assigned to their nearest cluster via the <code>assign_noise=True</code> behavior in pyBasin's HDBSCAN wrapper.</p> <p>A custom sub-classifier can replace this step via <code>fp_sub_classifier</code>. It receives a feature matrix with one row per fixed-point trajectory and one column per non-drifting dimension, containing the mean values.</p>"},{"location":"guides/dynamics-based-clustering/#limit-cycle-sub-classification","title":"Limit Cycle Sub-classification","text":"<p>Limit cycles use a hierarchical two-level approach:</p> <p>Level 1 -- Period grouping: The <code>spectral_frequency_ratio</code> is rounded to the nearest integer (clamped to [1, 10]) to determine the period number. Period-1, period-2, etc. trajectories form separate groups. This step uses a single analysis dimension -- the first non-drifting dimension.</p> <p>Level 2 -- Within-period clustering: Inside each period group, the algorithm checks whether amplitude and mean values vary enough to warrant splitting:</p> <ul> <li>Amplitude variation is measured by the coefficient of variation (CV = std/mean). If CV \\(&gt; 0.1\\), amplitude varies meaningfully.</li> <li>Mean variation is measured by the range (max - min). If range \\(&gt; 0.05\\), spatial location varies.</li> </ul> <p>When neither varies, the whole period group becomes one cluster. When one or both vary, the variable features are passed to a gap-based 1D clusterer (for a single varying feature) or to HDBSCAN (for two varying features). The 1D gap detector sorts values and finds gaps \\(&gt;5\\times\\) the median spacing, creating up to 5 clusters.</p> <p>A custom sub-classifier can replace the entire two-level process via <code>lc_sub_classifier</code>. It receives <code>[freq_ratio, amplitude, mean]</code> columns from the first analysis dimension.</p>"},{"location":"guides/dynamics-based-clustering/#chaos-sub-classification","title":"Chaos Sub-classification","text":"<p>Chaotic attractors are clustered by their spatial mean across non-drifting dimensions, using <code>HDBSCANClusterer</code> with <code>auto_tune=True</code>. Two butterfly wings of the Lorenz attractor, for instance, have different mean \\(x\\)-values and will separate into <code>chaos_0</code> and <code>chaos_1</code>.</p> <p>Non-finite values (e.g., from diverged trajectories) are zeroed before scaling. If fewer than 2 finite samples exist, everything goes into one cluster.</p> <p>A custom sub-classifier via <code>chaos_sub_classifier</code> receives a matrix of mean values with one row per chaotic trajectory and one column per non-drifting dimension.</p>"},{"location":"guides/dynamics-based-clustering/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>drift_threshold</code> <code>float</code> <code>0.1</code> Minimum |slope| to flag a dimension as drifting. Units: state_units / time_units. Also the threshold for detecting rotating LC solutions. <code>drift_fraction</code> <code>float</code> <code>0.3</code> Minimum fraction of trajectories with high slope for a dimension to be flagged as drifting. 0.3 means 30% of trajectories must show drift. <code>tiers</code> <code>list[str] \\| None</code> <code>[\"FP\", \"LC\", \"chaos\"]</code> Attractor types to detect, in priority order. First match wins. Options: <code>\"FP\"</code>, <code>\"LC\"</code>, <code>\"chaos\"</code>. Set to e.g. <code>[\"FP\", \"LC\"]</code> to skip chaos. <code>fp_variance_threshold</code> <code>float</code> <code>1e-6</code> Maximum variance for FP classification. For unnormalized features, set based on expected steady-state fluctuations. <code>fp_sub_classifier</code> sklearn estimator <code>None</code> Custom sub-classifier for FP. Receives mean values per non-drifting dimension. Must implement <code>fit_predict()</code>. Default: HDBSCAN. <code>lc_periodicity_threshold</code> <code>float</code> <code>0.5</code> Minimum periodicity strength [0--1]. 0.3--0.5 is weak/noisy, 0.5--0.8 is clear periodic, 0.8--1.0 is strong. <code>lc_sub_classifier</code> sklearn estimator <code>None</code> Custom sub-classifier for LC. Receives <code>[freq_ratio, amplitude, mean]</code> columns. Must implement <code>fit_predict()</code>. Default: hierarchical. <code>chaos_variance_threshold</code> <code>float</code> <code>5.0</code> Upper variance bound for the periodic-LC condition. Trajectories above this with low periodicity become chaos. <code>chaos_sub_classifier</code> sklearn estimator <code>None</code> Custom sub-classifier for chaos. Receives mean values per dimension. Must implement <code>fit_predict()</code>. Default: HDBSCAN with auto_tune. <p>For the full API reference, including method signatures and attribute details, see the <code>DynamicalSystemClusterer</code> API documentation.</p>"},{"location":"guides/dynamics-based-clustering/#tuning-tips","title":"Tuning Tips","text":"<p>Start with defaults, then adjust based on the label distribution. If too many limit cycles are classified as chaos, lower <code>chaos_variance_threshold</code> or raise <code>lc_periodicity_threshold</code>. If fixed points are missed, increase <code>fp_variance_threshold</code>.</p> <p>Normalization matters. The variance threshold operates on the actual feature values. When using <code>TorchFeatureExtractor</code> with <code>normalize=True</code> (the default), steady-state variance is z-score normalized, so the raw threshold <code>1e-6</code> will not match. Either disable normalization for this workflow or increase the threshold to account for scaling.</p> <p>Inspect the tiers. If your system has no chaotic behavior, set <code>tiers=[\"FP\", \"LC\"]</code> to avoid misclassifying noisy limit cycles as chaos. Conversely, <code>tiers=[\"chaos\"]</code> skips structured classification entirely and just clusters everything by spatial location.</p> <p>Custom sub-classifiers. Each attractor type's Stage 2 step accepts a custom sklearn-compatible estimator. Pass any clusterer that implements <code>fit_predict()</code>. For example, use <code>KMeans(n_clusters=2)</code> as <code>fp_sub_classifier</code> if you know exactly how many fixed points exist.</p> <p>Handling unbounded trajectories. <code>DynamicalSystemClusterer</code> by itself does not distinguish divergent trajectories from chaotic ones -- both end up in chaos. To separate them, wrap it with <code>UnboundednessMetaEstimator</code>, which detects unbounded solutions via a solver event function before the clusterer runs. The Lorenz case study uses this pattern.</p>"},{"location":"guides/dynamics-based-clustering/#validation-results","title":"Validation Results","text":"<p>Each case study was run with initial conditions taken from MATLAB bSTAB ground truth CSVs. Predicted labels were mapped to ground truth labels via majority vote per cluster, and the Matthews Correlation Coefficient (MCC) was computed against 5,000--20,000 labeled samples per system. All runs used default clusterer parameters with unnormalized features.</p> System N Attractors found MCC Duffing 10,000 5 limit cycles 0.9977 Friction 5,000 1 fixed point, 1 LC 1.0000 Pendulum 10,000 1 fixed point, 1 LC 1.0000 Lorenz 20,000 2 chaotic, 1 unbounded 0.9985 <p>Friction and pendulum achieve perfect classification -- the two attractor types (FP and LC) are cleanly separated by the variance and periodicity thresholds. Duffing is harder because it has five coexisting limit cycles of different periods, yet the hierarchical period-based sub-classification still reaches MCC 0.9977. For Lorenz, the two butterfly attractors overlap in feature space enough to produce a small number of mis-assignments between <code>chaos_0</code> and <code>chaos_1</code>, but overall accuracy remains high at 0.9985.</p>"},{"location":"guides/torchode-solver/","title":"TorchOdeSolver - Alternative ODE Solver","text":"<p>This document explains the <code>TorchOdeSolver</code> implementation, an alternative to <code>TorchDiffEqSolver</code>.</p>"},{"location":"guides/torchode-solver/#overview","title":"Overview","text":"<p><code>TorchOdeSolver</code> is a PyTorch-based ODE solver that uses the torchode library. It provides:</p> <ul> <li>JIT Compilation: Optional PyTorch JIT compilation for better performance</li> <li>Batch Parallelization: Efficient parallel solving across batches</li> <li>Multiple Methods: Various integration methods (dopri5, tsit5, euler, etc.)</li> <li>GPU Support: Full CUDA support like TorchDiffEqSolver</li> </ul>"},{"location":"guides/torchode-solver/#performance-comparison","title":"Performance Comparison","text":"<p>\u26a0\ufe0f Important: In the current implementation, TorchDiffEqSolver is faster than TorchOdeSolver for single-trajectory integration:</p> <ul> <li>TorchDiffEqSolver: ~76 seconds for 10,000 samples (pendulum case study)</li> <li>TorchOdeSolver: ~119 seconds for 10,000 samples (pendulum case study)</li> </ul> <p>This is because:</p> <ol> <li>The current architecture integrates one trajectory at a time (batch_size=1)</li> <li>torchode's batch parallelization doesn't help with batch_size=1</li> <li>torchdiffeq is more optimized for single-trajectory integration</li> </ol> <p>When TorchOdeSolver would be faster:</p> <ul> <li>When integrating multiple trajectories in parallel (requires code restructuring)</li> <li>When using JIT compilation with repeated solves of the same system</li> <li>For problems where variable step sizes per trajectory are needed</li> </ul>"},{"location":"guides/torchode-solver/#installation","title":"Installation","text":"<p>To use <code>TorchOdeSolver</code>, you need to install the <code>torchode</code> package:</p> <pre><code># Using pip\npip install torchode\n\n# Using uv\nuv add torchode\n\n# Or install with the optional solvers dependency\npip install -e \".[solvers]\"\n</code></pre>"},{"location":"guides/torchode-solver/#comparison-torchdiffeqsolver-vs-torchodesolver","title":"Comparison: TorchDiffEqSolver vs TorchOdeSolver","text":""},{"location":"guides/torchode-solver/#torchdiffeqsolver-torchdiffeq","title":"TorchDiffEqSolver (torchdiffeq)","text":"<ul> <li>Default Solver: dopri5 (Dormand-Prince 5(4))</li> <li>Similar to: MATLAB's ode45</li> <li>Pros:</li> <li>Well-established, widely used</li> <li>Adjoint method for memory-efficient backpropagation</li> <li>Simple API</li> <li>Cons:</li> <li>No batch parallelization</li> <li>No JIT compilation support</li> </ul>"},{"location":"guides/torchode-solver/#torchodesolver-torchode","title":"TorchOdeSolver (torchode)","text":"<ul> <li>Default Solver: dopri5 (Dormand-Prince 5(4))</li> <li>Pros:</li> <li>JIT compilation support for performance</li> <li>Batch parallelization (different step sizes per sample)</li> <li>Modern PyTorch integration</li> <li>Multiple solver methods</li> <li>Cons:</li> <li>Newer library, less widespread adoption</li> <li>More complex API</li> </ul>"},{"location":"guides/torchode-solver/#available-methods","title":"Available Methods","text":""},{"location":"guides/torchode-solver/#adaptive-step-methods","title":"Adaptive-Step Methods","text":"<ul> <li><code>dopri5</code> (default): Dormand-Prince 5(4) - similar to MATLAB's ode45</li> <li><code>tsit5</code>: Tsitouras 5(4) - often more efficient than dopri5</li> </ul>"},{"location":"guides/torchode-solver/#fixed-step-methods","title":"Fixed-Step Methods","text":"<ul> <li><code>euler</code>: Explicit Euler (1st order)</li> <li><code>midpoint</code>: Explicit midpoint (2nd order)</li> <li><code>heun</code>: Heun's method (2nd order)</li> </ul>"},{"location":"guides/torchode-solver/#usage-example","title":"Usage Example","text":""},{"location":"guides/torchode-solver/#basic-usage","title":"Basic Usage","text":"<pre><code>from pybasin.solvers import TorchOdeSolver\n\n# Create solver with default settings (dopri5)\nsolver = TorchOdeSolver(\n    time_span=(0, 1000),\n    n_steps=25001,\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"guides/torchode-solver/#with-custom-settings","title":"With Custom Settings","text":"<pre><code>solver = TorchOdeSolver(\n    time_span=(0, 1000),\n    n_steps=25001,\n    device=\"cuda\",\n    method=\"tsit5\",      # Use Tsitouras method\n    rtol=1e-8,           # Relative tolerance\n    atol=1e-6,           # Absolute tolerance\n    use_jit=True         # Enable JIT compilation\n)\n</code></pre>"},{"location":"guides/torchode-solver/#complete-example","title":"Complete Example","text":"<p>See <code>case_studies/pendulum/main_pendulum_case1_torchode.py</code> for a complete working example.</p> <pre><code>from case_studies.pendulum.setup_pendulum_system_torchode import (\n    setup_pendulum_system_torchode,\n)\nfrom pybasin.basin_stability_estimator import BasinStabilityEstimator\n\n# Setup system with TorchOdeSolver\nprops = setup_pendulum_system_torchode()\n\n# Create estimator\nbse = BasinStabilityEstimator(\n    n=props[\"n\"],\n    ode_system=props[\"ode_system\"],\n    sampler=props[\"sampler\"],\n    solver=props[\"solver\"],  # Using TorchOdeSolver\n    feature_extractor=props[\"feature_extractor\"],\n    estimator=props[\"estimator\"],\n)\n\n# Estimate basin stability\nbasin_stability = bse.estimate_bs()\n</code></pre>"},{"location":"guides/torchode-solver/#running-the-test-case-study","title":"Running the Test Case Study","text":"<p>To test the TorchOdeSolver implementation:</p> <pre><code># First, install torchode\nuv add torchode\n\n# Run the pendulum case study with TorchOdeSolver\npython case_studies/pendulum/main_pendulum_case1_torchode.py\n</code></pre>"},{"location":"guides/torchode-solver/#performance-tips","title":"Performance Tips","text":"<ol> <li>Enable JIT Compilation: Set <code>use_jit=True</code> for repeated solves with the same system</li> </ol> <pre><code>solver = TorchOdeSolver(time_span=(0, 1000), n_steps=25001, use_jit=True)\n</code></pre> <ol> <li>Choose the Right Method:</li> <li>For general problems: <code>dopri5</code> (default)</li> <li>For better efficiency: <code>tsit5</code></li> <li> <p>For simple/fast problems: <code>euler</code> or <code>midpoint</code> (fixed step)</p> </li> <li> <p>Adjust Tolerances:</p> </li> <li>Tighter tolerances (smaller rtol/atol) = more accurate but slower</li> <li> <p>Looser tolerances = faster but less accurate</p> </li> <li> <p>GPU Acceleration: Always specify <code>device=\"cuda\"</code> if available</p> </li> </ol>"},{"location":"guides/torchode-solver/#implementation-details","title":"Implementation Details","text":"<p>The <code>TorchOdeSolver</code> class:</p> <ul> <li>Inherits from the abstract <code>Solver</code> base class</li> <li>Implements the <code>_integrate()</code> method</li> <li>Handles batch dimension conversion (torchode expects batched inputs)</li> <li>Supports caching like TorchDiffEqSolver</li> <li>Falls back gracefully if torchode is not installed</li> </ul>"},{"location":"guides/torchode-solver/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/torchode-solver/#import-error","title":"Import Error","text":"<pre><code>ImportError: torchode is not installed\n</code></pre> <p>Solution: Install torchode with <code>pip install torchode</code></p>"},{"location":"guides/torchode-solver/#unknown-method-error","title":"Unknown Method Error","text":"<pre><code>ValueError: Unknown method: xyz\n</code></pre> <p>Solution: Use one of the available methods: dopri5, tsit5, euler, midpoint, heun</p>"},{"location":"guides/torchode-solver/#integration-failed","title":"Integration Failed","text":"<pre><code>RuntimeError: torchode integration failed\n</code></pre> <p>Solution: Try adjusting tolerances or using a different method</p>"},{"location":"guides/torchode-solver/#references","title":"References","text":"<ul> <li>torchode GitHub</li> <li>torchode Documentation</li> <li>torchode Paper</li> <li>torchdiffeq GitHub (for comparison)</li> </ul>"},{"location":"guides/type-safety-generics/","title":"Type Safety and Generics in pyBasin","text":""},{"location":"guides/type-safety-generics/#overview","title":"Overview","text":"<p>pyBasin uses Python's generic type system to provide strong type safety for ODE parameters across the entire library. This guide explains how to use generics effectively when extending pyBasin with your own ODE systems.</p>"},{"location":"guides/type-safety-generics/#why-generics","title":"Why Generics?","text":"<p>The generic type system provides:</p> <ol> <li>Type Safety: Static type checkers (mypy, pyright) can verify parameter types at development time</li> <li>IDE Autocomplete: Your IDE will know exactly which parameters are available</li> <li>Self-Documentation: Types serve as documentation for what parameters an ODE system expects</li> <li>Refactoring Safety: Renaming or changing parameter types will show errors across your codebase</li> </ol>"},{"location":"guides/type-safety-generics/#how-to-define-a-new-ode-system","title":"How to Define a New ODE System","text":""},{"location":"guides/type-safety-generics/#step-1-define-your-parameter-type","title":"Step 1: Define Your Parameter Type","text":"<p>Use <code>TypedDict</code> to define the exact parameters your ODE system needs:</p> <pre><code>from typing import TypedDict\n\nclass MyODEParams(TypedDict):\n    \"\"\"Parameters for my ODE system.\"\"\"\n    alpha: float      # damping coefficient\n    beta: float       # forcing amplitude\n    omega: float      # forcing frequency\n    initial_mass: float  # initial mass\n</code></pre> <p>Benefits:</p> <ul> <li>Type checkers will enforce that all required keys are present</li> <li>IDE will autocomplete parameter names</li> <li>Docstrings on each field document what they mean</li> </ul>"},{"location":"guides/type-safety-generics/#step-2-create-your-ode-system","title":"Step 2: Create Your ODE System","text":"<p>Inherit from <code>ODESystem[YourParamsType]</code>:</p> <pre><code>from pybasin.ode_system import ODESystem\nimport torch\n\nclass MyODE(ODESystem[MyODEParams]):\n    def __init__(self, params: MyODEParams):\n        super().__init__(params)\n        # self.params is now typed as MyODEParams!\n\n    def ode(self, t: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        # Type checker knows these keys exist\n        alpha = self.params[\"alpha\"]\n        beta = self.params[\"beta\"]\n        omega = self.params[\"omega\"]\n\n        # ... your ODE logic here ...\n        dy_dt = -alpha * y + beta * torch.sin(omega * t)\n        return dy_dt\n\n    def get_str(self) -&gt; str:\n        return f\"My ODE with \u03b1={self.params['alpha']}\"\n</code></pre>"},{"location":"guides/type-safety-generics/#step-3-use-type-safe-parameters-everywhere","title":"Step 3: Use Type-Safe Parameters Everywhere","text":"<p>When creating classifiers or other components, you can pass your typed parameters:</p> <pre><code>from pybasin.predictors.knn_classifier import KNNClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create your parameters with full type safety\nparams: MyODEParams = {\n    \"alpha\": 0.1,\n    \"beta\": 1.0,\n    \"omega\": 2.0,\n    \"initial_mass\": 1.5,\n}\n\n# Type checker ensures params matches MyODEParams\node_system = MyODE(params)\n\n# Pass the parameters to the classifier\nknn_classifier = KNNClassifier(\n    classifier=KNeighborsClassifier(n_neighbors=3),\n    template_y0=[[0.0, 1.0], [1.0, 0.0]],\n    labels=[\"stable\", \"unstable\"],\n    ode_params=params,\n)\n</code></pre>"},{"location":"guides/type-safety-generics/#complete-example-pendulum-system","title":"Complete Example: Pendulum System","text":"<p>Here's the pendulum example showing full type safety:</p> <pre><code># 1. Define parameters\nfrom typing import TypedDict\n\nclass PendulumParams(TypedDict):\n    \"\"\"Parameters for the pendulum ODE system.\"\"\"\n    alpha: float  # damping coefficient\n    T: float      # external torque\n    K: float      # stiffness coefficient\n\n# 2. Create ODE system\nfrom pybasin.ode_system import ODESystem\nimport torch\n\nclass PendulumODE(ODESystem[PendulumParams]):\n    def __init__(self, params: PendulumParams):\n        super().__init__(params)\n\n    def ode(self, t: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        # IDE autocompletes these parameter names!\n        alpha = self.params[\"alpha\"]\n        T = self.params[\"T\"]\n        K = self.params[\"K\"]\n\n        theta = y[..., 0]\n        theta_dot = y[..., 1]\n\n        dtheta_dt = theta_dot\n        dtheta_dot_dt = -alpha * theta_dot + T - K * torch.sin(theta)\n\n        return torch.stack([dtheta_dt, dtheta_dot_dt], dim=-1)\n\n    def get_str(self) -&gt; str:\n        return (\n            f\"Pendulum ODE:\\n\"\n            f\"  d\u03b8/dt = \u03c9\\n\"\n            f\"  d\u03c9/dt = -{self.params['alpha']}\u03c9 + \"\n            f\"{self.params['T']} - {self.params['K']}sin(\u03b8)\"\n        )\n\n# 3. Use with type safety\ndef setup_pendulum():\n    # Type checker verifies all required keys are present\n    params: PendulumParams = {\n        \"alpha\": 0.1,\n        \"T\": 0.5,\n        \"K\": 1.0,\n    }\n\n    # If you forget a parameter or misspell it, you'll get a type error!\n    # params: PendulumParams = {\"alpha\": 0.1}  # ERROR: Missing 'T' and 'K'\n\n    ode = PendulumODE(params)\n    return ode\n</code></pre>"},{"location":"guides/type-safety-generics/#comparison-with-typescript","title":"Comparison with TypeScript","text":"<p>If you're familiar with TypeScript, here's how the concepts map:</p>"},{"location":"guides/type-safety-generics/#typescript","title":"TypeScript:","text":"<pre><code>interface PendulumParams {\n  alpha: number;\n  T: number;\n  K: number;\n}\n\nclass PendulumODE extends ODESystem&lt;PendulumParams&gt; {\n  constructor(params: PendulumParams) {\n    super(params);\n    // this.params is typed as PendulumParams\n  }\n}\n</code></pre>"},{"location":"guides/type-safety-generics/#python-pybasin","title":"Python (pyBasin):","text":"<pre><code>class PendulumParams(TypedDict):\n    alpha: float\n    T: float\n    K: float\n\nclass PendulumODE(ODESystem[PendulumParams]):\n    def __init__(self, params: PendulumParams):\n        super().__init__(params)\n        # self.params is typed as PendulumParams\n</code></pre> <p>The main difference is that Python uses <code>TypedDict</code> instead of <code>interface</code>, and square brackets <code>[]</code> for generics instead of angle brackets <code>&lt;&gt;</code>.</p>"},{"location":"guides/type-safety-generics/#type-checking","title":"Type Checking","text":"<p>To verify your types are correct, run:</p> <pre><code># With pyright (recommended for VS Code)\nuv run pyright\n\n# Or with mypy\nuv run mypy src/\n</code></pre>"},{"location":"guides/type-safety-generics/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/type-safety-generics/#optional-parameters","title":"Optional Parameters","text":"<pre><code>from typing import TypedDict, NotRequired\n\nclass OptionalParams(TypedDict):\n    alpha: float                    # Required\n    beta: NotRequired[float]        # Optional (Python 3.11+)\n</code></pre>"},{"location":"guides/type-safety-generics/#multiple-parameter-types","title":"Multiple Parameter Types","text":"<p>If you need to support multiple parameter configurations:</p> <pre><code>from typing import Union\n\nParamVariant1 = TypedDict(\"ParamVariant1\", {\"a\": float, \"b\": float})\nParamVariant2 = TypedDict(\"ParamVariant2\", {\"x\": float, \"y\": float})\n\nclass FlexibleODE(ODESystem[Union[ParamVariant1, ParamVariant2]]):\n    def ode(self, t, y):\n        if \"a\" in self.params:\n            # Handle variant 1\n            pass\n        else:\n            # Handle variant 2\n            pass\n</code></pre>"},{"location":"guides/type-safety-generics/#best-practices","title":"Best Practices","text":"<ol> <li>Always use TypedDict for parameters: Don't use plain <code>dict[str, float]</code></li> <li>Document your parameter fields: Add docstrings or comments to each field</li> <li>Be specific with types: Use <code>float</code>, <code>int</code>, <code>str</code> instead of <code>Any</code></li> <li>Run type checkers regularly: Integrate <code>pyright</code> or <code>mypy</code> into your workflow</li> <li>Keep parameters immutable: Don't modify <code>self.params</code> after initialization</li> </ol>"},{"location":"guides/type-safety-generics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/type-safety-generics/#type-is-not-assignable-to-typevar","title":"\"Type is not assignable to TypeVar\"","text":"<p>If you see this error, make sure:</p> <ul> <li>Your parameter type is a <code>TypedDict</code> or plain <code>dict</code></li> <li>You're consistently using the same generic type throughout</li> </ul>"},{"location":"guides/type-safety-generics/#ide-not-showing-autocomplete","title":"IDE not showing autocomplete","text":"<ul> <li>Restart your Python language server</li> <li>Ensure your <code>TypedDict</code> is properly defined</li> <li>Check that you're using the latest version of your type checker</li> </ul>"},{"location":"guides/type-safety-generics/#further-reading","title":"Further Reading","text":"<ul> <li>Python TypedDict documentation</li> <li>PEP 589 \u2013 TypedDict</li> <li>Pyright documentation</li> </ul>"},{"location":"guides/unbounded-trajectories/","title":"Handling Unbounded Trajectories","text":""},{"location":"guides/unbounded-trajectories/#overview","title":"Overview","text":"<p>When computing basin stability, some initial conditions may lead to unbounded trajectories that diverge to infinity. These trajectories need special handling to:</p> <ol> <li>Stop integration early to save computation time</li> <li>Classify correctly as a distinct attractor state</li> <li>Avoid numerical overflow in the solver</li> </ol> <p>This guide explains the recommended approaches for handling unbounded trajectories in pyBasin.</p> <p>Quick Recommendation</p> <p>Use JaxSolver with event functions for the best performance and flexibility when dealing with unbounded trajectories.</p>"},{"location":"guides/unbounded-trajectories/#understanding-the-problem","title":"Understanding the Problem","text":"<p>In dynamical systems like the Lorenz system, some regions of the state space lead to trajectories that grow without bound:</p> <pre><code># Example: Lorenz \"broken butterfly\" system\nparams = {\"sigma\": 0.12, \"r\": 0.0, \"b\": -0.6}\n\n# Some initial conditions lead to bounded attractors:\nic_butterfly1 = [0.8, -3.0, 0.0]   # \u2192 bounded attractor\nic_butterfly2 = [-0.8, 3.0, 0.0]   # \u2192 bounded attractor\n\n# Others lead to unbounded trajectories:\nic_unbounded = [10.0, 50.0, 0.0]   # \u2192 |y| \u2192 \u221e\n</code></pre> <p>Without proper handling, unbounded trajectories will:</p> <ul> <li>Waste computation time integrating to large values</li> <li>Risk numerical overflow errors</li> <li>Contaminate basin stability estimates</li> </ul>"},{"location":"guides/unbounded-trajectories/#recommended-approach-jaxsolver-with-event-functions","title":"Recommended Approach: JaxSolver with Event Functions","text":"<p>The recommended solution is to use <code>JaxSolver</code> with an event function that stops integration when trajectories exceed a threshold.</p>"},{"location":"guides/unbounded-trajectories/#why-jaxsolver","title":"Why JaxSolver?","text":"<p>JAX's <code>diffrax</code> library supports event-based termination where each trajectory in a batch can stop independently:</p> <ul> <li>\u2705 Individual termination: Each trajectory stops when it meets the condition</li> <li>\u2705 Efficient: Other trajectories continue integrating</li> <li>\u2705 Clean classification: Stopped trajectories are marked appropriately</li> </ul>"},{"location":"guides/unbounded-trajectories/#complete-example","title":"Complete Example","text":"<p>Here's a complete example from the Lorenz case study:</p> <pre><code>import torch\nfrom case_studies.lorenz.lorenz_jax_ode import LorenzJaxODE, LorenzParams\nfrom case_studies.lorenz.setup_lorenz_system import lorenz_stop_event\nfrom pybasin.basin_stability_estimator import BasinStabilityEstimator\nfrom pybasin.sampler import UniformRandomSampler\nfrom pybasin.solvers.jax_solver import JaxSolver\n\ndef main():\n    # Number of initial conditions to sample\n    n = 10_000\n\n    # Auto-detect device (use GPU if available)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Setting up Lorenz system on device: {device}\")\n\n    # Parameters for broken butterfly system\n    params: LorenzParams = {\"sigma\": 0.12, \"r\": 0.0, \"b\": -0.6}\n    ode_system = LorenzJaxODE(params)\n\n    # Sample uniformly in the region of interest\n    sampler = UniformRandomSampler(\n        min_limits=[-10.0, -20.0, 0.0],\n        max_limits=[10.0, 20.0, 0.0],\n        device=device\n    )\n\n    # JaxSolver with event function to stop unbounded trajectories\n    solver = JaxSolver(\n        device=device,\n        event_fn=lorenz_stop_event,  # \u2190 Key: event function\n    )\n\n    # Estimate basin stability\n    bse = BasinStabilityEstimator(\n        n=n,\n        ode_system=ode_system,\n        sampler=sampler,\n        solver=solver,\n        save_to=\"results_lorenz\",\n    )\n\n    basin_stability = bse.estimate_bs()\n    print(\"Basin Stability:\", basin_stability)\n\n    return bse\n\nif __name__ == \"__main__\":\n    bse = main()\n</code></pre>"},{"location":"guides/unbounded-trajectories/#defining-event-functions","title":"Defining Event Functions","text":"<p>The event function determines when to stop integration. Here's the <code>lorenz_stop_event</code> example:</p> <pre><code>import jax.numpy as jnp\nfrom diffrax import Event\n\ndef lorenz_stop_event(t, y, args):\n    \"\"\"\n    Stop integration when trajectory magnitude exceeds 200.\n\n    Args:\n        t: Current time\n        y: Current state [x, y, z]\n        args: Additional arguments (unused)\n\n    Returns:\n        Scalar value that triggers event when it crosses zero.\n        Negative = continue, positive = stop.\n    \"\"\"\n    # Compute maximum absolute value across all state components\n    max_magnitude = jnp.max(jnp.abs(y))\n\n    # Return (threshold - magnitude)\n    # When magnitude &gt; 200, this becomes negative \u2192 event triggers\n    return 200.0 - max_magnitude\n</code></pre> <p>Event Function Behavior</p> <p>The event triggers when the returned value crosses zero from positive to negative. Design your function accordingly:</p> <pre><code>- `threshold - magnitude`: triggers when magnitude exceeds threshold\n- `magnitude - threshold`: triggers when magnitude falls below threshold\n</code></pre>"},{"location":"guides/unbounded-trajectories/#benefits","title":"Benefits","text":"<p>\u2705 Performance: Only unbounded trajectories stop early \u2705 Accuracy: Bounded trajectories integrate to full completion \u2705 Simplicity: Clean, declarative API \u2705 Flexibility: Custom event functions for any stopping condition</p>"},{"location":"guides/unbounded-trajectories/#alternative-approach-zero-masking-with-torchdiffeq","title":"Alternative Approach: Zero Masking with TorchDiffEq","text":"<p>An alternative approach uses zero masking where derivatives are set to zero once a stopping condition is met. This \"freezes\" the solution at the threshold value.</p> <p>Performance Considerations</p> <p>This approach is slower than JaxSolver with events because:</p> <pre><code>- All trajectories integrate for the full time span (no early stopping)\n- The solver continues stepping even though derivatives are zero\n- Better suited for systems where most trajectories are bounded\n</code></pre>"},{"location":"guides/unbounded-trajectories/#how-zero-masking-works","title":"How Zero Masking Works","text":"<p>The ODE system returns zero derivatives when a trajectory exceeds the threshold:</p> <pre><code>import torch\nfrom pybasin.ode_system import ODESystem\n\nclass LorenzODE(ODESystem):\n    def ode(self, t: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute Lorenz dynamics with zero masking for unbounded trajectories.\n        \"\"\"\n        # Compute standard Lorenz dynamics\n        sigma = self.params[\"sigma\"]\n        r = self.params[\"r\"]\n        b = self.params[\"b\"]\n\n        x, y_coord, z = y[..., 0], y[..., 1], y[..., 2]\n\n        dx_dt = sigma * (y_coord - x)\n        dy_dt = r * x - x * z - y_coord\n        dz_dt = x * y_coord - b * z\n\n        dydt = torch.stack([dx_dt, dy_dt, dz_dt], dim=-1)\n\n        # Create mask: 1 if |y| &lt; 200, otherwise 0\n        mask = (torch.max(torch.abs(y), dim=-1)[0] &lt; 200).float().unsqueeze(-1)\n\n        # Return masked derivatives (zeros for unbounded trajectories)\n        return dydt * mask\n</code></pre>"},{"location":"guides/unbounded-trajectories/#key-points","title":"Key Points","text":"<ul> <li>Freezing behavior: When <code>|y| \u2265 200</code>, derivatives become zero, \"freezing\" the solution</li> <li>No early termination: Integration continues for the full time span</li> <li>Post-processing: Feature extraction must detect frozen trajectories by checking final magnitude</li> </ul>"},{"location":"guides/unbounded-trajectories/#using-with-torchdiffeqsolver","title":"Using with TorchDiffEqSolver","text":"<pre><code>from pybasin.solvers.torchdiffeq_solver import TorchDiffEqSolver\n\nsolver = TorchDiffEqSolver(\n    device=device,\n    t_span=(0.0, 1000.0),\n    method=\"dopri5\",\n    rtol=1e-8,\n    atol=1e-6,\n)\n\n# Use with LorenzODE that has zero masking\nbse = BasinStabilityEstimator(\n    n=n,\n    ode_system=LorenzODE(params),  # Has zero masking in ode()\n    sampler=sampler,\n    solver=solver,\n)\n</code></pre>"},{"location":"guides/unbounded-trajectories/#limitations-of-torchdiffeq-event-handling","title":"Limitations of TorchDiffEq Event Handling","text":"<p>TorchDiffEq Limitation</p> <p><code>torchdiffeq.odeint_event()</code> does not support individual trajectory termination. When the event condition is met for any trajectory in a batch, all integrations stop simultaneously.</p> <pre><code>This makes `odeint_event` unsuitable for basin stability estimation where different trajectories should stop at different times.\n</code></pre>"},{"location":"guides/unbounded-trajectories/#comparison","title":"Comparison","text":"Feature JaxSolver + Event TorchDiffEq + Zero Mask Individual termination \u2705 Yes \u274c No (zero masking workaround) Performance \u2705 Fast (early stopping) \u26a0\ufe0f Slower (full integration) Setup complexity \ud83d\udfe2 Simple (event function) \ud83d\udfe2 Simple (mask in ODE) GPU support \u2705 Yes \u2705 Yes Batch processing \u2705 Efficient \u26a0\ufe0f Less efficient Best for Most use cases Systems with few unbounded trajectories"},{"location":"guides/unbounded-trajectories/#best-practices","title":"Best Practices","text":""},{"location":"guides/unbounded-trajectories/#1-choose-the-right-threshold","title":"1. Choose the Right Threshold","text":"<p>Set your stopping threshold based on your system's dynamics:</p> <pre><code># Too low: May incorrectly classify bounded trajectories\nthreshold = 10.0  # \u274c May catch transient behavior\n\n# Good: Well above bounded attractor magnitudes\nthreshold = 200.0  # \u2705 Clear separation\n\n# Check your attractors first:\n# - Bounded attractors: |y| &lt; 50\n# - Set threshold at 4\u00d7 max bounded value\n</code></pre>"},{"location":"guides/unbounded-trajectories/#2-verify-event-triggering","title":"2. Verify Event Triggering","text":"<p>Test your event function with known unbounded initial conditions:</p> <pre><code>def test_event_function():\n    \"\"\"Verify event triggers for unbounded IC.\"\"\"\n    unbounded_ic = torch.tensor([10.0, 50.0, 0.0])\n\n    solution = solver.solve(\n        ode_system=ode_system,\n        initial_conditions=unbounded_ic,\n    )\n\n    # Check if integration stopped early\n    assert solution.t[-1] &lt; t_final, \"Event should trigger before t_final\"\n    assert torch.max(torch.abs(solution.y[-1])) &gt;= threshold\n</code></pre>"},{"location":"guides/unbounded-trajectories/#3-handle-classification-correctly","title":"3. Handle Classification Correctly","text":"<p>Ensure your feature extractor identifies unbounded trajectories:</p> <pre><code>def extract_features(solution):\n    \"\"\"Extract features, handling unbounded trajectories.\"\"\"\n    max_magnitude = torch.max(torch.abs(solution.y), dim=0)[0]\n\n    if max_magnitude &gt;= 200.0:\n        # Unbounded trajectory\n        return torch.tensor([0.0, 0.0])  # Special marker\n    else:\n        # Bounded trajectory - extract features from attractor\n        tail = solution.y[-100:]  # Last 100 points\n        mean_x = torch.mean(tail[:, 0])\n\n        if mean_x &gt; 0:\n            return torch.tensor([1.0, 0.0])  # Attractor 1\n        else:\n            return torch.tensor([0.0, 1.0])  # Attractor 2\n</code></pre>"},{"location":"guides/unbounded-trajectories/#summary","title":"Summary","text":"<ul> <li>Recommended: Use <code>JaxSolver</code> with event functions for efficient, individual trajectory termination</li> <li>Alternative: Use zero masking with <code>TorchDiffEqSolver</code> if you need PyTorch-only solution</li> <li>Avoid: Using <code>odeint_event()</code> for basin stability (stops all trajectories simultaneously)</li> <li>Test: Always verify your event function or masking logic with known unbounded cases</li> </ul> <p>For more examples, see the Lorenz case study.</p>"},{"location":"user-guide/adaptive-studies/","title":"Adaptive Parameter Studies","text":"<p>Documentation in Progress</p> <p>This page is under construction.</p>"},{"location":"user-guide/adaptive-studies/#use-case","title":"Use Case","text":"<p>Study how basin stability changes with a system parameter using <code>BasinStabilityStudy</code>.</p>"},{"location":"user-guide/adaptive-studies/#the-basinstabilitystudy-class","title":"The <code>BasinStabilityStudy</code> Class","text":"<p>Runs BSE multiple times for different parameter values, returning parameter values, BS values, and full results per run.</p>"},{"location":"user-guide/adaptive-studies/#example-pendulum-damping-study","title":"Example: Pendulum Damping Study","text":"<pre><code>import numpy as np\nfrom pybasin.basin_stability_study import BasinStabilityStudy\nfrom pybasin.study_params import SweepStudyParams\n\nstudy_params = SweepStudyParams(\n    name='ode_system.params[\"gamma\"]',\n    values=np.linspace(0.1, 0.5, 10),\n)\n\nas_bse = BasinStabilityStudy(\n    n=10_000,\n    ode_system=pendulum_ode,\n    sampler=sampler,\n    solver=solver,\n    feature_extractor=feature_extractor,\n    estimator=predictor,\n    study_params=study_params,\n)\nlabels, bs_vals, results = as_bse.estimate_as_bs()\n</code></pre>"},{"location":"user-guide/adaptive-studies/#visualization-with-asplotter","title":"Visualization with <code>ASPlotter</code>","text":"<p>Use the <code>ASPlotter</code> class to create parameter vs BS bifurcation diagrams.</p>"},{"location":"user-guide/bse-overview/","title":"Basin Stability Estimator","text":"<p><code>BasinStabilityEstimator</code> is the core class for computing basin stability values. It orchestrates the full pipeline: sampling initial conditions, integrating the ODE, extracting features from trajectories, classifying them into attractor basins, and computing the stability fractions. Only two arguments are required -- an ODE system and a sampler -- while every other component has sensible defaults.</p>"},{"location":"user-guide/bse-overview/#minimal-example","title":"Minimal Example","text":"<p>At its simplest, basin stability estimation requires an ODE definition and a sampler. Everything else is auto-configured:</p> <pre><code>from pybasin.basin_stability_estimator import BasinStabilityEstimator\nfrom pybasin.sampler import UniformRandomSampler\nfrom case_studies.duffing_oscillator.duffing_jax_ode import DuffingJaxODE\n\node = DuffingJaxODE(params={\"delta\": 0.08, \"k3\": 1.0, \"A\": 0.2, \"omega\": 1.0})\nsampler = UniformRandomSampler(min_limits=[-3.0, -3.0], max_limits=[3.0, 3.0])\n\nbse = BasinStabilityEstimator(ode_system=ode, sampler=sampler)\nbs_vals = bse.estimate_bs()\n\nprint(bs_vals)  # e.g. {'0': 0.42, '1': 0.58}\n</code></pre> <p>With these defaults, the estimator generates 10,000 initial conditions, integrates them using <code>JaxSolver</code> (since the ODE inherits from <code>JaxODESystem</code>), extracts 10 statistical features per state variable, filters redundant features, clusters the results with HDBSCAN, and returns basin stability fractions.</p>"},{"location":"user-guide/bse-overview/#pipeline-steps","title":"Pipeline Steps","text":"<p>The <code>estimate_bs()</code> method runs seven sequential steps. Each step maps to a configurable component:</p> Step What Happens Component Default 1 Sample N initial conditions <code>sampler</code> Required 2 Integrate the ODE for each IC <code>solver</code> <code>JaxSolver</code> or <code>TorchDiffEqSolver</code> (auto) 3 Create Solution object -- -- 3b Detect unbounded trajectories (optional) <code>detect_unbounded</code> <code>True</code> (active only with <code>JaxSolver</code> + event) 4 Extract features from steady-state <code>feature_extractor</code> <code>TorchFeatureExtractor</code> (minimal, 10/state) 5 Filter redundant features <code>feature_selector</code> <code>DefaultFeatureSelector</code> 5b Fit classifier on templates (supervised) <code>template_integrator</code> <code>None</code> (unsupervised by default) 6 Cluster or classify features <code>predictor</code> <code>HDBSCANClusterer(auto_tune=True)</code> 7 Compute basin stability fractions -- -- <pre><code>Sample ICs --&gt; Integrate ODEs --&gt; Detect Unbounded --&gt; Extract Features\n--&gt; Filter Features --&gt; Cluster/Classify --&gt; Compute BS Values\n</code></pre>"},{"location":"user-guide/bse-overview/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>ode_system</code> <code>ODESystemProtocol</code> Required The dynamical system to analyze. Use <code>JaxODESystem</code> or <code>ODESystem</code>. <code>sampler</code> <code>Sampler</code> Required Generates initial conditions from the region of interest. <code>n</code> <code>int</code> <code>10_000</code> Number of initial conditions to sample. <code>solver</code> <code>SolverProtocol</code> or <code>None</code> Auto-detect ODE integrator. Auto-selects based on ODE type if <code>None</code>. <code>feature_extractor</code> <code>FeatureExtractor</code> or <code>None</code> <code>TorchFeatureExtractor</code> Computes feature vectors from trajectories. <code>predictor</code> <code>BaseEstimator</code> or <code>None</code> <code>HDBSCANClusterer</code> Sklearn-compatible classifier or clusterer. <code>template_integrator</code> <code>TemplateIntegrator</code> or <code>None</code> <code>None</code> Required for supervised classifiers. Holds template ICs and labels. <code>feature_selector</code> <code>BaseEstimator</code> or <code>None</code> <code>DefaultFeatureSelector</code> Filters redundant features. <code>None</code> disables filtering. <code>detect_unbounded</code> <code>bool</code> <code>True</code> Separate diverging trajectories before feature extraction. <code>save_to</code> <code>str</code> or <code>None</code> <code>None</code> Directory path for saving results to JSON or Excel. <p>For full method signatures and docstrings, see the API reference.</p>"},{"location":"user-guide/bse-overview/#automatic-solver-selection","title":"Automatic Solver Selection","text":"<p>When no <code>solver</code> is passed, the estimator picks one based on the ODE class:</p> <ul> <li><code>JaxODESystem</code> --&gt; <code>JaxSolver(time_span=(0, 1000), n_steps=1000)</code></li> <li><code>ODESystem</code> --&gt; <code>TorchDiffEqSolver(time_span=(0, 1000), n_steps=1000)</code></li> </ul> <p>Both auto-selected solvers inherit the <code>device</code> from the sampler. For most workloads, <code>JaxSolver</code> delivers the best GPU performance. Override the solver when you need custom time spans, tolerances, or caching:</p> <pre><code>from pybasin.solvers import JaxSolver\n\nsolver = JaxSolver(\n    time_span=(0, 500),\n    n_steps=5000,\n    device=\"cuda\",\n    rtol=1e-8,\n    atol=1e-6,\n    cache_dir=\".pybasin_cache/duffing\",\n)\n\nbse = BasinStabilityEstimator(\n    ode_system=ode, sampler=sampler, solver=solver\n)\n</code></pre> <p>See the Solvers guide for a detailed comparison of available solvers.</p>"},{"location":"user-guide/bse-overview/#unsupervised-clustering-default","title":"Unsupervised Clustering (Default)","text":"<p>By default, the estimator uses <code>HDBSCANClusterer</code> to discover attractor basins without any prior knowledge. This is the simplest workflow -- no templates, no labels:</p> <pre><code>bse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    n=10_000,\n)\nbs_vals = bse.estimate_bs()\n</code></pre> <p>HDBSCAN auto-tunes its <code>min_cluster_size</code> parameter and reassigns noise points so that every trajectory receives a basin label. To swap in a different clusterer, pass any sklearn-compatible estimator:</p> <pre><code>from sklearn.cluster import KMeans\n\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    predictor=KMeans(n_clusters=3),\n)\n</code></pre> <p>See the Predictors guide for all available clusterers and their tuning options.</p>"},{"location":"user-guide/bse-overview/#supervised-classification","title":"Supervised Classification","text":"<p>When the attractor structure of the system is known, supervised classification produces more reliable basin labels. This requires a <code>TemplateIntegrator</code> that provides labeled initial conditions -- one per known attractor -- along with a sklearn classifier.</p> <pre><code>import torch\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pybasin.template_integrator import TemplateIntegrator\n\n# Template ICs: one per known attractor\ntemplate_y0 = torch.tensor([\n    [1.2, 0.0],   # IC converging to attractor \"fp\"\n    [2.5, 0.0],   # IC converging to attractor \"lc\"\n])\ntemplate_labels = [\"fp\", \"lc\"]\n\ntemplate_integrator = TemplateIntegrator(\n    template_y0=template_y0,\n    labels=template_labels,\n)\n\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    predictor=KNeighborsClassifier(n_neighbors=1),\n    template_integrator=template_integrator,\n)\nbs_vals = bse.estimate_bs()\n# e.g. {'fp': 0.35, 'lc': 0.65}\n</code></pre> <p>The estimator integrates template trajectories alongside the main batch, extracts features from both, fits the classifier on the template features, and then predicts basin labels for all N sampled ICs. By default, template and main integrations run in parallel.</p> <p>Classifier requires templates</p> <p>Passing a classifier without a <code>template_integrator</code> raises <code>ValueError</code>. Regressors are rejected outright with <code>TypeError</code>.</p> <p>See the Predictors guide for more on supervised vs. unsupervised workflows.</p>"},{"location":"user-guide/bse-overview/#customizing-feature-extraction","title":"Customizing Feature Extraction","text":"<p>The default feature extractor computes 10 statistical features per state variable (mean, variance, min, max, etc.) from the steady-state portion of each trajectory. Override it for richer or more targeted feature sets:</p> <pre><code>from pybasin.feature_extractors import TorchFeatureExtractor\n\nextractor = TorchFeatureExtractor(\n    features=\"comprehensive\",  # ~800 features per state\n    time_steady=800.0,         # discard transient before t=800\n    device=\"cuda\",\n)\n\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    feature_extractor=extractor,\n)\n</code></pre> <p>Per-state configuration is also supported -- useful when different state variables carry different physical meaning:</p> <pre><code>extractor = TorchFeatureExtractor(\n    features=\"minimal\",\n    features_per_state={\n        0: {\"maximum\": None, \"minimum\": None},  # position: just extrema\n        1: None,                                  # velocity: skip entirely\n    },\n)\n</code></pre> <p>See the Feature Extractors guide for the full feature catalog and configuration options.</p>"},{"location":"user-guide/bse-overview/#feature-selection","title":"Feature Selection","text":"<p>After extraction, a feature selector removes uninformative columns (near-zero variance) and redundant ones (high pairwise correlation). The default <code>DefaultFeatureSelector</code> handles this automatically:</p> <pre><code># Default: variance + correlation filtering\nbse = BasinStabilityEstimator(ode_system=ode, sampler=sampler)\n\n# Disable filtering entirely\nbse = BasinStabilityEstimator(\n    ode_system=ode, sampler=sampler, feature_selector=None\n)\n\n# Custom sklearn selector\nfrom sklearn.feature_selection import VarianceThreshold\n\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    feature_selector=VarianceThreshold(threshold=0.1),\n)\n</code></pre> <p>Minimal features + filtering</p> <p>When using <code>features=\"minimal\"</code> (10 features per state), the default selector may drop useful columns. Consider disabling it or lowering thresholds for small feature sets.</p> <p>See the Feature Selectors guide for threshold configuration and custom selectors.</p>"},{"location":"user-guide/bse-overview/#unboundedness-detection","title":"Unboundedness Detection","text":"<p>Some dynamical systems produce trajectories that diverge to infinity. When <code>detect_unbounded=True</code> (the default), the estimator separates these trajectories before feature extraction to prevent extreme values from contaminating the clustering. Unbounded ICs receive the label <code>\"unbounded\"</code> in the final results.</p> <p>This detection only activates when the solver is a <code>JaxSolver</code> configured with an <code>event_fn</code> for early termination. Without an event function, trajectories are not stopped early and no Inf values appear.</p> <pre><code>from pybasin.solvers import JaxSolver\n\nsolver = JaxSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    event_fn=lambda t, y: jnp.max(jnp.abs(y)) - 1e6,  # stop at |y| &gt; 1e6\n)\n\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    solver=solver,\n    detect_unbounded=True,  # default\n)\nbs_vals = bse.estimate_bs()\n# e.g. {'0': 0.45, '1': 0.40, 'unbounded': 0.15}\n</code></pre> <p>See the Handling Unbounded Trajectories guide for a deeper look at event functions and unboundedness strategies.</p>"},{"location":"user-guide/bse-overview/#output-attributes","title":"Output Attributes","text":"<p>After <code>estimate_bs()</code> completes, three attributes hold the results:</p> Attribute Type Description <code>bse.bs_vals</code> <code>dict[str, float]</code> Basin stability fractions per class (e.g., <code>{'0': 0.6, '1': 0.4}</code>) <code>bse.y0</code> <code>torch.Tensor</code> Sampled initial conditions, shape <code>(N, n_states)</code> <code>bse.solution</code> <code>Solution</code> Full results: trajectories, features, labels, metadata <p>The <code>Solution</code> object carries everything downstream components need -- trajectories for plotting, features for analysis, labels for visualization. See the Solution guide for details on its properties.</p>"},{"location":"user-guide/bse-overview/#error-estimation","title":"Error Estimation","text":"<p>Basin stability values are Monte Carlo estimates, so they carry statistical uncertainty. Call <code>get_errors()</code> to compute the absolute and relative standard errors based on Bernoulli experiment statistics:</p> <pre><code>bs_vals = bse.estimate_bs()\nerrors = bse.get_errors()\n\nfor label, err in errors.items():\n    print(f\"Basin {label}: S_B = {bs_vals[label]:.3f} +/- {err['e_abs']:.4f}\")\n</code></pre> <p>The absolute error for each basin is:</p> \\[e_{\\text{abs}} = \\sqrt{\\frac{S_B(A) \\cdot (1 - S_B(A))}{N}}\\] <p>Increasing <code>n</code> reduces the error proportionally to \\(1/\\sqrt{N}\\).</p>"},{"location":"user-guide/bse-overview/#saving-results","title":"Saving Results","text":"<p>Two export methods are available after running <code>estimate_bs()</code>. Both require <code>save_to</code> to be set in the constructor.</p>"},{"location":"user-guide/bse-overview/#json-export","title":"JSON Export","text":"<p><code>save()</code> writes basin stability values, ODE system definition, sampler settings, and feature selection metadata:</p> <pre><code>bse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    save_to=\"results/pendulum\",\n)\nbse.estimate_bs()\nbse.save()  # writes results/pendulum/basin_stability_results_&lt;timestamp&gt;.json\n</code></pre>"},{"location":"user-guide/bse-overview/#excel-export","title":"Excel Export","text":"<p><code>save_to_excel()</code> writes initial conditions, labels, and bifurcation amplitudes in tabular form:</p> <pre><code>bse.save_to_excel()  # writes results/pendulum/basin_stability_results_&lt;timestamp&gt;.xlsx\n</code></pre>"},{"location":"user-guide/bse-overview/#visualization","title":"Visualization","text":"<p>Pass the estimator to a plotter for visual inspection of the results. <code>MatplotlibPlotter</code> produces static figures suitable for publications, while <code>InteractivePlotter</code> launches a Dash web app for exploration.</p> <pre><code>from pybasin.plotters.matplotlib_plotter import MatplotlibPlotter\n\nplotter = MatplotlibPlotter(bse)\nplotter.plot_bse_results()              # 4-panel diagnostic\nplotter.plot_basin_stability_bars()     # bar chart of BS values\nplotter.plot_state_space()              # labeled phase portrait\nplotter.plot_feature_space()            # feature space clusters\n</code></pre> <pre><code>from pybasin.plotters.interactive_plotter import InteractivePlotter\n\nplotter = InteractivePlotter(bse, state_labels={0: \"x\", 1: \"v\"})\nplotter.run(port=8050)\n</code></pre> <p>See the Plotters guide for the full set of visualization methods.</p>"},{"location":"user-guide/bse-overview/#full-configured-example","title":"Full Configured Example","text":"<p>Below is a complete example showing every configurable component:</p> <pre><code>import torch\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom pybasin.basin_stability_estimator import BasinStabilityEstimator\nfrom pybasin.sampler import UniformRandomSampler\nfrom pybasin.solvers import JaxSolver\nfrom pybasin.feature_extractors import TorchFeatureExtractor\nfrom pybasin.feature_selector import DefaultFeatureSelector\nfrom pybasin.template_integrator import TemplateIntegrator\nfrom pybasin.plotters.matplotlib_plotter import MatplotlibPlotter\n\n# 1. ODE system (defined elsewhere)\nfrom case_studies.pendulum.pendulum_jax_ode import PendulumJaxODE\n\node = PendulumJaxODE(params={\"alpha\": 0.1, \"T\": 0.5, \"K\": 1.0})\n\n# 2. Sampler\nsampler = UniformRandomSampler(\n    min_limits=[-3.14, -2.0],\n    max_limits=[3.14, 2.0],\n    device=\"cuda\",\n)\n\n# 3. Solver\nsolver = JaxSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    device=\"cuda\",\n    cache_dir=\".pybasin_cache/pendulum\",\n)\n\n# 4. Feature extractor\nextractor = TorchFeatureExtractor(\n    features=\"minimal\",\n    time_steady=850.0,\n    device=\"cuda\",\n)\n\n# 5. Feature selector\nselector = DefaultFeatureSelector(\n    variance_threshold=0.01,\n    correlation_threshold=0.95,\n)\n\n# 6. Templates for supervised classification\ntemplate_y0 = torch.tensor([[0.5, 0.0], [2.5, 0.0]])\ntemplate_integrator = TemplateIntegrator(\n    template_y0=template_y0,\n    labels=[\"fixed_point\", \"limit_cycle\"],\n)\n\n# 7. Classifier\npredictor = KNeighborsClassifier(n_neighbors=1)\n\n# 8. Assemble and run\nbse = BasinStabilityEstimator(\n    ode_system=ode,\n    sampler=sampler,\n    n=20_000,\n    solver=solver,\n    feature_extractor=extractor,\n    predictor=predictor,\n    template_integrator=template_integrator,\n    feature_selector=selector,\n    save_to=\"results/pendulum\",\n)\n\nbs_vals = bse.estimate_bs()\n\n# 9. Inspect results\nprint(bs_vals)\nerrors = bse.get_errors()\nprint(errors)\n\n# 10. Visualize\nplotter = MatplotlibPlotter(bse)\nplotter.plot_bse_results()\n\n# 11. Save\nbse.save()\nbse.save_to_excel()\n</code></pre>"},{"location":"user-guide/bse-overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>Solution -- what <code>bse.solution</code> contains and how to inspect it</li> <li>Samplers -- uniform, grid, Gaussian, and CSV sampling strategies</li> <li>Solvers -- solver comparison, caching, and GPU acceleration</li> <li>Feature Extractors -- feature catalogs and per-state configuration</li> <li>Feature Selectors -- variance and correlation filtering</li> <li>Predictors -- HDBSCAN, DBSCAN, supervised classifiers, and custom predictors</li> <li>Plotters -- static and interactive visualization options</li> <li>Adaptive Parameter Studies -- sweeping ODE parameters with <code>BasinStabilityStudy</code></li> <li>Handling Unbounded Trajectories -- event functions and unboundedness detection</li> <li>Case Studies -- worked examples with Pendulum, Duffing, Lorenz, and more</li> </ul>"},{"location":"user-guide/feature-extractors/","title":"Feature Extractors","text":"<p>Feature extractors transform ODE solution trajectories into numerical feature vectors. These vectors capture time-series characteristics -- statistical moments, spectral properties, complexity measures -- that distinguish different attractor types. The downstream predictor (clusterer or classifier) then operates on these features to assign basin labels.</p> <p>All extractors inherit from <code>FeatureExtractor</code>, which defines two things every subclass must provide: an <code>extract_features(solution)</code> method returning a tensor of shape <code>(B, F)</code>, and a <code>feature_names</code> property listing the F feature names.</p>"},{"location":"user-guide/feature-extractors/#available-extractors","title":"Available Extractors","text":"Class Features GPU Speed Best for <code>TorchFeatureExtractor</code> 10 -- 800+ Yes Fast Default. General-purpose, GPU-accelerated. <code>JaxFeatureExtractor</code> 10 -- 700+ Yes Fastest JAX-only pipelines. <code>TsfreshFeatureExtractor</code> 20 -- 800+ No Slow Recommended for production. Battle-tested reference. <code>NoldsFeatureExtractor</code> 2 -- 8 No Slow Nonlinear dynamics features only (Lyapunov, correlation dim). <p>Experimental: Torch and JAX extractors</p> <p><code>TorchFeatureExtractor</code> and <code>JaxFeatureExtractor</code> are experimental reimplementations that recreate the tsfresh feature extraction API in a GPU-optimized way. They have not been deeply validated against tsfresh for correctness in all cases. Results are close but not identical to tsfresh. For maximum confidence, prefer <code>TsfreshFeatureExtractor</code>.</p>"},{"location":"user-guide/feature-extractors/#feature-configuration-fcparameters","title":"Feature Configuration (FCParameters)","text":"<p>All extractors except <code>NoldsFeatureExtractor</code> use a tsfresh-style dictionary to specify which features to compute. The type alias is:</p> <pre><code>FCParameters = Mapping[str, list[dict[str, Any]] | None]\n</code></pre> <p>Each key is a feature calculator name, and the value is either <code>None</code> (use default parameters) or a list of parameter dictionaries. One feature output is produced per parameter combination.</p> <pre><code># No parameters -- just compute the feature once\n{\"mean\": None, \"variance\": None}\n\n# Parameterized -- compute autocorrelation at three different lags\n{\"autocorrelation\": [{\"lag\": 1}, {\"lag\": 5}, {\"lag\": 10}]}\n\n# Mixed\n{\n    \"mean\": None,\n    \"quantile\": [{\"q\": 0.1}, {\"q\": 0.5}, {\"q\": 0.9}],\n}\n</code></pre>"},{"location":"user-guide/feature-extractors/#preset-configurations","title":"Preset Configurations","text":"<p><code>TorchFeatureExtractor</code> ships with string shortcuts for common presets:</p> Preset Features per state Description <code>\"comprehensive\"</code> ~800 Full tsfresh-equivalent sweep. Includes FFT coefficients, entropy measures, trend statistics, and more. <code>\"minimal\"</code> 10 Basic statistics: <code>median</code>, <code>mean</code>, <code>standard_deviation</code>, <code>variance</code>, <code>root_mean_square</code>, <code>maximum</code>, <code>absolute_maximum</code>, <code>minimum</code>, <code>delta</code>, <code>log_delta</code>. <p><code>JaxFeatureExtractor</code> defaults to <code>JAX_MINIMAL_FC_PARAMETERS</code>, which matches the same 10 features as Torch minimal.</p> <p>JAX comprehensive JIT compile time</p> <p>Using <code>JAX_COMPREHENSIVE_FC_PARAMETERS</code> triggers a JIT compilation step that can take ~40 minutes. Stick to <code>JAX_MINIMAL_FC_PARAMETERS</code> or a custom subset unless compilation time is acceptable.</p>"},{"location":"user-guide/feature-extractors/#feature-calculators","title":"Feature Calculators","text":"<p>Below is a summary of all available Torch feature calculators, grouped by category. Each calculator can be referenced by name in an <code>FCParameters</code> dictionary. JAX and tsfresh support overlapping but not identical sets. For full function signatures and parameter details, see the Torch Feature Calculators API reference.</p> Category Calculators Statistical <code>sum_values</code>, <code>median</code>, <code>mean</code>, <code>length</code>, <code>standard_deviation</code>, <code>variance</code>, <code>root_mean_square</code>, <code>maximum</code>, <code>absolute_maximum</code>, <code>minimum</code>, <code>abs_energy</code>, <code>kurtosis</code>, <code>skewness</code>, <code>quantile</code>, <code>variation_coefficient</code> Change / Difference <code>absolute_sum_of_changes</code>, <code>mean_abs_change</code>, <code>mean_change</code>, <code>mean_second_derivative_central</code> Counting <code>count_above</code>, <code>count_above_mean</code>, <code>count_below</code>, <code>count_below_mean</code>, <code>count_in_range</code>, <code>count_value</code> Boolean <code>has_duplicate</code>, <code>has_duplicate_max</code>, <code>has_duplicate_min</code>, <code>has_variance_larger_than_standard_deviation</code>, <code>large_standard_deviation</code> Location <code>first_location_of_maximum</code>, <code>first_location_of_minimum</code>, <code>last_location_of_maximum</code>, <code>last_location_of_minimum</code>, <code>index_mass_quantile</code> Pattern / Streak <code>longest_strike_above_mean</code>, <code>longest_strike_below_mean</code>, <code>number_crossing_m</code>, <code>number_peaks</code>, <code>number_cwt_peaks</code> Autocorrelation <code>autocorrelation</code>, <code>partial_autocorrelation</code>, <code>agg_autocorrelation</code>, <code>autocorrelation_periodicity</code> Entropy / Complexity <code>permutation_entropy</code>, <code>binned_entropy</code>, <code>fourier_entropy</code>, <code>lempel_ziv_complexity</code>, <code>cid_ce</code>, <code>approximate_entropy</code>, <code>sample_entropy</code> Frequency Domain <code>fft_coefficient</code>, <code>fft_aggregated</code>, <code>spkt_welch_density</code>, <code>cwt_coefficients</code>, <code>spectral_frequency_ratio</code> Trend / Regression <code>linear_trend</code>, <code>linear_trend_timewise</code>, <code>agg_linear_trend</code>, <code>ar_coefficient</code>, <code>augmented_dickey_fuller</code> Reoccurrence <code>percentage_of_reoccurring_datapoints_to_all_datapoints</code>, <code>percentage_of_reoccurring_values_to_all_values</code>, <code>sum_of_reoccurring_data_points</code>, <code>sum_of_reoccurring_values</code>, <code>ratio_value_number_to_time_series_length</code> Advanced <code>benford_correlation</code>, <code>c3</code>, <code>energy_ratio_by_chunks</code>, <code>time_reversal_asymmetry_statistic</code> Custom (Torch/JAX only) <code>delta</code> (max - min), <code>log_delta</code> (log of delta), <code>amplitude</code> (half of peak-to-peak) Dynamical Systems <code>lyapunov_r</code>, <code>lyapunov_e</code>, <code>correlation_dimension</code>, <code>friedrich_coefficients</code>, <code>max_langevin_fixed_point</code>"},{"location":"user-guide/feature-extractors/#default-features-in-basinstabilityestimator","title":"Default Features in BasinStabilityEstimator","text":"<p>When no <code>feature_extractor</code> is passed to <code>BasinStabilityEstimator</code>, it creates a <code>TorchFeatureExtractor</code> with <code>DEFAULT_TORCH_FC_PARAMETERS</code>. This preset matches <code>TORCH_MINIMAL_FC_PARAMETERS</code> -- 10 features per state variable:</p> Feature What it measures <code>median</code> Central tendency (robust to outliers) <code>mean</code> Arithmetic average <code>standard_deviation</code> Spread around the mean <code>variance</code> Squared spread <code>root_mean_square</code> Energy-like measure <code>maximum</code> Peak value <code>absolute_maximum</code> Peak absolute value <code>minimum</code> Trough value <code>delta</code> Range (<code>max - min</code>) <code>log_delta</code> <code>log(delta)</code> -- compresses large amplitude differences"},{"location":"user-guide/feature-extractors/#transient-filtering-time_steady","title":"Transient Filtering (time_steady)","text":"<p>Before computing features, extractors discard the initial transient portion of each trajectory. This prevents startup dynamics from contaminating the steady-state features.</p> <p>The <code>time_steady</code> parameter controls when the transient ends:</p> Value Behavior <code>None</code> (default) Automatically set to 85% of the integration time span. For a <code>time_span=(0, 1000)</code> integration, features are computed from <code>t &gt; 850</code> onward -- the last 150 time units. <code>0.0</code> Use the entire time series, no transient removal. Any positive float Use data after that time value. For example, <code>time_steady=900.0</code> keeps only <code>t &gt; 900</code>. <p>Choosing time_steady</p> <p>The 85% default is conservative. If your system settles quickly, setting <code>time_steady</code> to a lower value (e.g., 50% of the span) retains more data points for feature extraction, which can improve feature quality. On the other hand, systems with slow transients may need a higher threshold.</p> <p>NoldsFeatureExtractor exception</p> <p><code>NoldsFeatureExtractor</code> defaults to <code>time_steady=0.0</code> (no transient removal), unlike all other extractors which default to <code>None</code> (85%). This is because nonlinear dynamics measures like Lyapunov exponents often benefit from longer time series. Override this explicitly if transient removal is needed.</p>"},{"location":"user-guide/feature-extractors/#normalization","title":"Normalization","text":"<p>All extractors except <code>NoldsFeatureExtractor</code> support z-score normalization (<code>normalize=True</code> by default). The scaler is fitted on the first call to <code>extract_features()</code> and then reused for subsequent calls.</p> <p>This fit-on-first-call behavior has an important consequence for supervised workflows: the scaler trains on whichever dataset is extracted first. When using a <code>TemplateIntegrator</code> with a classifier, pyBasin integrates both template and main trajectories, extracts features from the main dataset first (fitting the scaler on the larger sample), then transforms the template features using the same scaler. To reset the scaler between runs, call <code>reset_scaler()</code>.</p> Extractor Normalization backend Scaler reset method <code>TorchFeatureExtractor</code> Manual z-score (PyTorch) <code>reset_scaler()</code> <code>JaxFeatureExtractor</code> Manual z-score (JAX) <code>reset_scaler()</code> <code>TsfreshFeatureExtractor</code> <code>sklearn.preprocessing.StandardScaler</code> <code>reset_scaler()</code> <code>NoldsFeatureExtractor</code> None N/A"},{"location":"user-guide/feature-extractors/#imputation","title":"Imputation","text":"<p>Trajectories that diverge or contain numerical issues can produce <code>NaN</code> or <code>Inf</code> feature values. Each extractor handles this with an imputation step after extraction.</p> Method Behavior When to use <code>\"extreme\"</code> (default for Torch/JAX) Replace <code>NaN</code>/<code>Inf</code> with <code>1e10</code>. Systems with divergent solutions -- the extreme value separates unbounded trajectories from bounded ones in feature space. <code>\"tsfresh\"</code> Per-column: <code>+Inf</code> -&gt; column max, <code>-Inf</code> -&gt; column min, <code>NaN</code> -&gt; column median. Fully bounded systems where all trajectories converge. <p><code>TsfreshFeatureExtractor</code> uses tsfresh's built-in <code>impute()</code> function, which follows the tsfresh-style strategy.</p> <p><code>NoldsFeatureExtractor</code> always uses column-wise imputation: <code>-Inf</code> -&gt; column min, <code>+Inf</code> -&gt; column max, <code>NaN</code> -&gt; column median. Columns that are entirely <code>NaN</code> are filled with <code>0</code>.</p>"},{"location":"user-guide/feature-extractors/#feature-naming-convention","title":"Feature Naming Convention","text":"<p>Feature names follow the pattern <code>state_&lt;index&gt;__&lt;feature_name&gt;</code>, where <code>&lt;index&gt;</code> is the zero-based state variable index. Parameterized features append parameter values:</p> <pre><code>state_0__mean\nstate_0__quantile__q_0.1\nstate_1__autocorrelation__lag_5\n</code></pre> <p>These names appear in <code>Solution.feature_names</code> after extraction and are used by feature selectors and plotting utilities.</p> <p>Names available only after extraction</p> <p>The <code>feature_names</code> property raises <code>RuntimeError</code> if accessed before calling <code>extract_features()</code>, because the number of states (and therefore the number of features) is only known at extraction time.</p>"},{"location":"user-guide/feature-extractors/#torchfeatureextractor","title":"TorchFeatureExtractor","text":"<p>The default extractor. Reimplements tsfresh feature calculators in pure PyTorch for GPU acceleration.</p> <pre><code>from pybasin.feature_extractors import TorchFeatureExtractor\n\n# Comprehensive features on GPU\nextractor = TorchFeatureExtractor(\n    time_steady=800.0,\n    features=\"comprehensive\",\n    device=\"gpu\",\n)\n\n# Minimal features on CPU (fast, good baseline)\nextractor = TorchFeatureExtractor(\n    time_steady=None,       # default: 85% of time span\n    features=\"minimal\",     # 10 features per state\n    normalize=True,         # z-score normalization\n    impute_method=\"extreme\",\n)\n\n# Custom feature set\nextractor = TorchFeatureExtractor(\n    features={\n        \"mean\": None,\n        \"variance\": None,\n        \"autocorrelation\": [{\"lag\": 1}, {\"lag\": 5}],\n        \"fft_aggregated\": [{\"aggtype\": s} for s in [\"centroid\", \"variance\"]],\n    },\n)\n</code></pre>"},{"location":"user-guide/feature-extractors/#per-state-configuration","title":"Per-State Configuration","text":"<p>Different state variables often carry different physical meaning (e.g., position vs. velocity). You can assign distinct feature sets to each state:</p> <pre><code>extractor = TorchFeatureExtractor(\n    features=\"minimal\",            # default for states not listed below\n    features_per_state={\n        0: {\"maximum\": None, \"minimum\": None},  # override for state 0\n        1: None,                                  # skip state 1 entirely\n    },\n)\n</code></pre> <p>When <code>features_per_state</code> is provided, it overrides the global <code>features</code> for the specified indices. Setting a state's value to <code>None</code> skips feature extraction for that state entirely.</p> <p>Uniform config optimization</p> <p>When all states share the same configuration object (by identity, not just by equality), the extractor processes all states in a single batched pass. Mixing per-state overrides disables this optimization and processes each state separately.</p>"},{"location":"user-guide/feature-extractors/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_steady</code> <code>float \\| None</code> <code>None</code> Transient cutoff. <code>None</code> = 85% of span. <code>features</code> <code>\"comprehensive\" \\| \"minimal\" \\| FCParameters \\| None</code> <code>\"comprehensive\"</code> Feature configuration for all states. <code>features_per_state</code> <code>dict[int, FCParameters \\| None] \\| None</code> <code>None</code> Per-state overrides. <code>normalize</code> <code>bool</code> <code>True</code> Apply z-score normalization. <code>device</code> <code>\"cpu\" \\| \"gpu\"</code> <code>\"cpu\"</code> Extraction device. Raises <code>RuntimeError</code> if <code>\"gpu\"</code> and CUDA unavailable. <code>n_jobs</code> <code>int \\| None</code> <code>None</code> CPU worker count. <code>None</code> = all cores. Ignored on GPU. <code>impute_method</code> <code>\"extreme\" \\| \"tsfresh\"</code> <code>\"extreme\"</code> How to handle <code>NaN</code>/<code>Inf</code> in features."},{"location":"user-guide/feature-extractors/#tsfreshfeatureextractor","title":"TsfreshFeatureExtractor","text":"<p>Wraps the tsfresh library directly. This is the most reliable extractor for correctness, since tsfresh is a widely-used, well-tested time-series feature library. The tradeoff is slower extraction (CPU-only, pandas-based).</p> <pre><code>from pybasin.feature_extractors.tsfresh_feature_extractor import TsfreshFeatureExtractor\nfrom tsfresh.feature_extraction import MinimalFCParameters, ComprehensiveFCParameters\n\n# Minimal extraction (fast, ~20 features per state)\nextractor = TsfreshFeatureExtractor(\n    time_steady=800.0,\n    default_fc_parameters=MinimalFCParameters(),\n    n_jobs=1,\n    normalize=True,\n)\n\n# Comprehensive extraction (~800 features per state)\nextractor = TsfreshFeatureExtractor(\n    time_steady=800.0,\n    default_fc_parameters=ComprehensiveFCParameters(),\n    n_jobs=-1,  # use all cores\n)\n\n# Custom feature set\nextractor = TsfreshFeatureExtractor(\n    default_fc_parameters={\"mean\": None, \"maximum\": None, \"minimum\": None},\n)\n</code></pre>"},{"location":"user-guide/feature-extractors/#per-state-configuration_1","title":"Per-State Configuration","text":"<p>Uses <code>kind_to_fc_parameters</code>, keyed by integer state index:</p> <pre><code>extractor = TsfreshFeatureExtractor(\n    time_steady=950.0,\n    kind_to_fc_parameters={\n        0: {\"mean\": None, \"maximum\": None},       # position: basic stats\n        1: ComprehensiveFCParameters(),             # velocity: full analysis\n    },\n    n_jobs=1,\n)\n</code></pre>"},{"location":"user-guide/feature-extractors/#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_steady</code> <code>float \\| None</code> <code>None</code> Transient cutoff. <code>default_fc_parameters</code> <code>FCParameters</code> <code>MinimalFCParameters()</code> Default features for all states. <code>kind_to_fc_parameters</code> <code>dict[int, FCParameters] \\| None</code> <code>None</code> Per-state overrides. <code>n_jobs</code> <code>int</code> <code>1</code> Parallel jobs. <code>-1</code> = all cores. <code>normalize</code> <code>bool</code> <code>True</code> Apply <code>StandardScaler</code> normalization. <p>Non-determinism with n_jobs &gt; 1</p> <p>Parallel feature extraction introduces non-determinism from floating-point arithmetic ordering. Classification results may vary between runs. Set <code>n_jobs=1</code> for reproducible results.</p>"},{"location":"user-guide/feature-extractors/#jaxfeatureextractor","title":"JaxFeatureExtractor","text":"<p>A JAX-based reimplementation targeting JAX-native workflows. Features are JIT-compiled for speed, but compilation adds a one-time overhead.</p> <pre><code>from pybasin.feature_extractors.jax import JaxFeatureExtractor\nfrom pybasin.feature_extractors.jax.jax_feature_calculators import JAX_MINIMAL_FC_PARAMETERS\n\nextractor = JaxFeatureExtractor(\n    time_steady=800.0,\n    features=JAX_MINIMAL_FC_PARAMETERS,  # 12 features per state (default)\n    device=\"gpu\",\n    normalize=True,\n)\n</code></pre>"},{"location":"user-guide/feature-extractors/#constructor-parameters_2","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_steady</code> <code>float \\| None</code> <code>None</code> Transient cutoff. <code>features</code> <code>FCParameters \\| None</code> <code>JAX_MINIMAL_FC_PARAMETERS</code> Feature configuration. <code>features_per_state</code> <code>dict[int, FCParameters \\| None] \\| None</code> <code>None</code> Per-state overrides. <code>normalize</code> <code>bool</code> <code>True</code> Apply z-score normalization. <code>use_jit</code> <code>bool</code> <code>True</code> JIT-compile extraction functions. <code>device</code> <code>str \\| None</code> <code>None</code> JAX device (<code>\"cpu\"</code>, <code>\"gpu\"</code>, <code>\"cuda\"</code>, <code>\"cuda:N\"</code>, or <code>None</code> for auto). <code>impute_method</code> <code>\"extreme\" \\| \"tsfresh\"</code> <code>\"extreme\"</code> How to handle <code>NaN</code>/<code>Inf</code>."},{"location":"user-guide/feature-extractors/#noldsfeatureextractor","title":"NoldsFeatureExtractor","text":"<p>Computes nonlinear dynamics measures using the nolds library. These features -- Lyapunov exponents, correlation dimension, sample entropy, Hurst exponent -- characterize the complexity and chaoticity of attractors. They are slow to compute but carry information that statistical features miss.</p> <p>Requires <code>nolds</code> as an optional dependency (<code>uv add nolds</code>).</p> <pre><code>from pybasin.feature_extractors import NoldsFeatureExtractor\n\n# Default: Lyapunov exponent (Rosenstein) + correlation dimension\nextractor = NoldsFeatureExtractor()\n\n# Custom set with multiple parameter combinations\nextractor = NoldsFeatureExtractor(\n    time_steady=0.0,\n    features={\n        \"lyap_r\": [{\"emb_dim\": 10}, {\"emb_dim\": 15}],\n        \"sampen\": None,\n        \"hurst_rs\": None,\n    },\n    n_jobs=-1,\n)\n</code></pre>"},{"location":"user-guide/feature-extractors/#available-features","title":"Available Features","text":"Feature key nolds function What it measures <code>lyap_r</code> <code>nolds.lyap_r</code> Largest Lyapunov exponent (Rosenstein's method) <code>lyap_e</code> <code>nolds.lyap_e</code> Lyapunov exponent (Eckmann's method) <code>sampen</code> <code>nolds.sampen</code> Sample entropy <code>hurst_rs</code> <code>nolds.hurst_rs</code> Hurst exponent (R/S method) <code>corr_dim</code> <code>nolds.corr_dim</code> Correlation dimension <code>dfa</code> <code>nolds.dfa</code> Detrended fluctuation analysis <code>mfhurst_b</code> <code>nolds.mfhurst_b</code> Multifractal Hurst exponent (basic) <code>mfhurst_dm</code> <code>nolds.mfhurst_dm</code> Multifractal Hurst exponent (DMA)"},{"location":"user-guide/feature-extractors/#constructor-parameters_3","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_steady</code> <code>float \\| None</code> <code>0.0</code> Transient cutoff. Defaults to <code>0.0</code> (no removal), unlike other extractors. <code>features</code> <code>NoldsFCParameters</code> <code>{\"lyap_r\": None, \"corr_dim\": None}</code> Feature configuration. <code>features_per_state</code> <code>dict[int, NoldsFCParameters \\| None] \\| None</code> <code>None</code> Per-state overrides. <code>n_jobs</code> <code>int \\| None</code> <code>None</code> Parallel workers. <code>None</code> = all cores. <p>No normalization</p> <p><code>NoldsFeatureExtractor</code> does not support normalization. If normalization is needed, apply it externally before passing features to the predictor.</p> <p>Error handling</p> <p>Individual feature computations that fail (e.g., due to insufficient data length) silently produce <code>NaN</code>, which is then imputed column-wise. No exceptions are raised for individual failures.</p>"},{"location":"user-guide/feature-extractors/#standalone-usage","title":"Standalone Usage","text":"<p>Feature extractors can be used independently of <code>BasinStabilityEstimator</code>, for example to inspect features from a single trajectory or a custom solution.</p> <pre><code>import torch\nfrom pybasin.feature_extractors import TorchFeatureExtractor\nfrom pybasin.solution import Solution\n\n# Create a solution manually (N=1000 timesteps, B=1 trajectory, S=2 states)\nt = torch.linspace(0, 10, 1000)\ny = torch.stack([torch.sin(t), torch.cos(t)], dim=-1).unsqueeze(1)  # (N, 1, S)\n\nsolution = Solution(\n    initial_condition=torch.tensor([[0.0, 1.0]]),\n    time=t,\n    y=y,\n)\n\nextractor = TorchFeatureExtractor(features=\"minimal\", time_steady=0.0, normalize=False)\nfeatures = extractor.extract_features(solution)\n\nprint(f\"Feature shape: {features.shape}\")     # (1, 20) for 2 states x 10 features\nprint(f\"Feature names: {extractor.feature_names}\")\n</code></pre>"},{"location":"user-guide/feature-extractors/#creating-custom-feature-extractors","title":"Creating Custom Feature Extractors","text":"<p>See the Custom Feature Extractor guide for details on subclassing <code>FeatureExtractor</code>.</p>"},{"location":"user-guide/feature-selectors/","title":"Feature Selectors","text":"<p>Feature selectors remove redundant or uninformative features from the extracted feature matrix before it reaches the predictor. After feature extraction, the matrix can contain hundreds of columns -- many with near-zero variance or high mutual correlation. Filtering these out reduces noise, speeds up clustering, and often improves classification accuracy.</p> <p>By default, <code>BasinStabilityEstimator</code> applies a <code>DefaultFeatureSelector</code> that chains variance thresholding with pairwise correlation filtering. Any sklearn-compatible transformer can be used instead, and passing <code>None</code> disables selection entirely.</p>"},{"location":"user-guide/feature-selectors/#available-selectors","title":"Available Selectors","text":"Class Strategy Best for <code>DefaultFeatureSelector</code> Variance threshold + correlation filter Default. General-purpose two-stage pipeline. <code>CorrelationSelector</code> Pairwise absolute correlation filter Targeted removal of redundant correlated features. Any sklearn transformer Custom (e.g., <code>SelectKBest</code>, k-best) Domain-specific selection when defaults are insufficient."},{"location":"user-guide/feature-selectors/#default-behavior-in-basinstabilityestimator","title":"Default Behavior in BasinStabilityEstimator","text":"<p>When no <code>feature_selector</code> argument is provided, the estimator creates a <code>DefaultFeatureSelector()</code> with these defaults:</p> <ul> <li>Variance threshold: 0.01 -- features with variance below this are dropped</li> <li>Correlation threshold: 0.95 -- among features with |correlation| above this, only one is kept</li> <li>Minimum features: 2 -- at least two features are always retained</li> </ul> <p>The selector's <code>feature_selector</code> parameter uses a sentinel pattern to distinguish \"not specified\" (use default) from <code>None</code> (disable). Passing <code>None</code> explicitly turns off all feature filtering.</p> <pre><code>from pybasin import BasinStabilityEstimator\n\n# Default: uses DefaultFeatureSelector()\nbse = BasinStabilityEstimator(ode_system=..., sampler=..., n=5000)\n\n# Disabled: no feature filtering at all\nbse = BasinStabilityEstimator(ode_system=..., sampler=..., n=5000, feature_selector=None)\n</code></pre> <p>When to disable</p> <p>For pipelines with <code>features=\"minimal\"</code> (10 features per state), feature selection may remove potentially useful columns. Consider disabling it or lowering the thresholds when working with small feature sets.</p>"},{"location":"user-guide/feature-selectors/#defaultfeatureselector","title":"DefaultFeatureSelector","text":"<p>A two-step sklearn <code>Pipeline</code> that first removes low-variance features, then filters out highly correlated ones. Because it subclasses <code>Pipeline</code>, it exposes the full sklearn transformer API: <code>fit()</code>, <code>transform()</code>, <code>fit_transform()</code>, <code>get_params()</code>, and <code>set_params()</code>.</p> <pre><code>from pybasin.feature_selector import DefaultFeatureSelector\n\n# Default thresholds\nselector = DefaultFeatureSelector()\n\n# Custom thresholds\nselector = DefaultFeatureSelector(\n    variance_threshold=0.05,\n    correlation_threshold=0.90,\n    min_features=3,\n)\n</code></pre> <p>The pipeline steps run in order:</p> <ol> <li><code>VarianceThreshold(threshold=variance_threshold)</code> -- drops constant or near-constant columns</li> <li><code>CorrelationSelector(threshold=correlation_threshold, min_features=min_features)</code> -- among the surviving features, drops those with high pairwise |correlation|</li> </ol>"},{"location":"user-guide/feature-selectors/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>variance_threshold</code> <code>float</code> <code>0.01</code> Minimum variance required to keep a feature. <code>correlation_threshold</code> <code>float</code> <code>0.95</code> Maximum absolute pairwise correlation allowed between features. <code>min_features</code> <code>int</code> <code>2</code> Floor on the number of features retained by the correlation filtering step."},{"location":"user-guide/feature-selectors/#retrieving-the-selection-mask","title":"Retrieving the Selection Mask","text":"<p>Call <code>get_support()</code> after fitting to see which original features survived both stages:</p> <pre><code>import numpy as np\n\nselector = DefaultFeatureSelector()\nX_filtered = selector.fit_transform(X)\n\nmask = selector.get_support()          # boolean array, length = original feature count\nindices = selector.get_support(indices=True)  # integer indices of kept features\n</code></pre> <p>The method composes the variance mask and the correlation mask internally, returning a single combined result against the original feature indices.</p>"},{"location":"user-guide/feature-selectors/#correlationselector","title":"CorrelationSelector","text":"<p>An sklearn transformer that removes features with high pairwise absolute correlation. It iterates over the upper triangle of the correlation matrix and drops the later column in each correlated pair, subject to a minimum feature count.</p> <pre><code>from pybasin.feature_selector import CorrelationSelector\n\nselector = CorrelationSelector(threshold=0.95, min_features=2)\nX_filtered = selector.fit_transform(X)\n</code></pre>"},{"location":"user-guide/feature-selectors/#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Default Description <code>threshold</code> <code>float</code> <code>0.95</code> Correlation ceiling. Pairs with |corr| above this trigger removal. <code>min_features</code> <code>int</code> <code>2</code> Minimum features to retain. Prevents aggressive filtering from removing all columns. <p>Greedy column-order removal</p> <p>The selector iterates column pairs in order and always drops the higher-indexed column. This greedy strategy is fast but not globally optimal -- the retained set depends on column ordering.</p>"},{"location":"user-guide/feature-selectors/#retrieving-the-selection-mask_1","title":"Retrieving the Selection Mask","text":"<pre><code>mask = selector.get_support()          # boolean mask\nindices = selector.get_support(indices=True)  # integer indices\n</code></pre>"},{"location":"user-guide/feature-selectors/#using-custom-sklearn-transformers","title":"Using Custom sklearn Transformers","text":"<p>Any sklearn-compatible transformer that implements <code>fit_transform()</code> can serve as a feature selector. If the transformer also provides <code>get_support()</code>, filtered feature names will appear in <code>Solution.feature_names</code>.</p> <pre><code>from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n\n# Simple variance filter\nbse = BasinStabilityEstimator(\n    ode_system=...,\n    sampler=...,\n    feature_selector=VarianceThreshold(threshold=0.1),\n)\n\n# k-best features (supervised -- requires labels via TemplateIntegrator)\nbse = BasinStabilityEstimator(\n    ode_system=...,\n    sampler=...,\n    feature_selector=SelectKBest(f_classif, k=20),\n    predictor=some_classifier,\n    template_integrator=some_template,\n)\n</code></pre> <p>Feature name tracking</p> <p>Only selectors with a <code>get_support()</code> method enable automatic feature name filtering. Without it, <code>Solution.feature_names</code> falls back to the full unfiltered list.</p>"},{"location":"user-guide/feature-selectors/#standalone-usage","title":"Standalone Usage","text":"<p>Feature selectors work independently outside <code>BasinStabilityEstimator</code>. This is useful for inspecting which features survive filtering before running a full pipeline.</p> <pre><code>import numpy as np\nfrom pybasin.feature_selector import DefaultFeatureSelector\n\n# Simulated feature matrix: 100 samples, 50 features\nrng = np.random.default_rng(42)\nX = rng.standard_normal((100, 50))\n\nselector = DefaultFeatureSelector(variance_threshold=0.01, correlation_threshold=0.90)\nX_filtered = selector.fit_transform(X)\n\nprint(f\"Before: {X.shape[1]} features\")\nprint(f\"After:  {X_filtered.shape[1]} features\")\nprint(f\"Kept indices: {selector.get_support(indices=True)}\")\n</code></pre> <p>For full class signatures and attribute documentation, see the Feature Selection API reference.</p>"},{"location":"user-guide/plotters/","title":"Plotters","text":"<p>Documentation in Progress</p> <p>This page is under construction.</p>"},{"location":"user-guide/plotters/#overview","title":"Overview","text":"<p>Plotters visualize basin stability results.</p>"},{"location":"user-guide/plotters/#available-plotters","title":"Available Plotters","text":"Class Type Use Case <code>MatplotlibPlotter</code> Static Publication figures, quick inspection <code>InteractivePlotter</code> Web app Exploration, presentations <code>ASPlotter</code> Static Parameter study bifurcation diagrams"},{"location":"user-guide/plotters/#matplotlibplotter","title":"MatplotlibPlotter","text":"<pre><code>from pybasin.plotters.matplotlib_plotter import MatplotlibPlotter\n\nplotter = MatplotlibPlotter(bse)\nplotter.plot_bse_results()      # 4-panel diagnostic plot\nplotter.plot_phase(x_var=0, y_var=1)  # Phase space\nplotter.plot_templates(plotted_var=0) # Template time series\n</code></pre>"},{"location":"user-guide/plotters/#interactiveplotter","title":"InteractivePlotter","text":"<pre><code>from pybasin.plotters.interactive_plotter import InteractivePlotter\n\nplotter = InteractivePlotter(\n    bse,\n    state_labels={0: \"\u03b8\", 1: \"\u03c9\"},\n)\nplotter.run(port=8050)  # Opens web browser\n</code></pre>"},{"location":"user-guide/plotters/#asplotter","title":"ASPlotter","text":"<p>Used for visualizing parameter study results from <code>BasinStabilityStudy</code>.</p>"},{"location":"user-guide/predictors/","title":"Predictors","text":"<p>Predictors classify trajectories into attractor classes based on extracted features. The <code>BasinStabilityEstimator</code> supports any sklearn-compatible estimator, falling into two main categories: unsupervised clusterers that discover attractor classes automatically, and supervised classifiers that learn from labeled template trajectories.</p>"},{"location":"user-guide/predictors/#predictor-types","title":"Predictor Types","text":"<p>When you pass a predictor to <code>BasinStabilityEstimator</code>, the library detects its type using sklearn's <code>is_classifier()</code> and <code>is_clusterer()</code> functions:</p> <ul> <li>Clusterers (<code>is_clusterer(predictor) == True</code>): Implement <code>fit_predict(X)</code> and discover classes from the data without prior labels. Any sklearn clusterer works.</li> <li>Classifiers (<code>is_classifier(predictor) == True</code>): Implement <code>fit(X, y)</code> + <code>predict(X)</code> and require labeled training data via <code>TemplateIntegrator</code>. Any sklearn classifier works.</li> </ul> <p>If no predictor is specified, the default is <code>HDBSCANClusterer(auto_tune=True, assign_noise=True)</code>.</p>"},{"location":"user-guide/predictors/#available-predictors","title":"Available Predictors","text":"Class Type Description <code>HDBSCANClusterer</code> Unsupervised Default. Density-based, auto-tunes parameters <code>DBSCANClusterer</code> Unsupervised Classic DBSCAN with epsilon auto-tuning <code>DynamicalSystemClusterer</code> Unsupervised Physics-based two-stage hierarchical. See guide. <code>UnboundednessMetaEstimator</code> Meta Wraps any estimator to handle unbounded cases Any sklearn clusterer Unsupervised <code>KMeans</code>, <code>GaussianMixture</code>, <code>AgglomerativeClustering</code>, etc. Any sklearn classifier Supervised <code>KNeighborsClassifier</code>, <code>SVC</code>, <code>RandomForestClassifier</code>, etc. <p>The built-in predictors (<code>HDBSCANClusterer</code>, <code>DBSCANClusterer</code>) include auto-tuning to improve the probability of finding meaningful attractor clusters when analyzing unknown systems. Rather than requiring manual parameter selection, they search for optimal clustering parameters using silhouette analysis.</p> <p>The noise reassignment option (<code>assign_noise=True</code>) addresses a key property of basin stability: every trajectory from an initial condition either converges to an attractor or diverges to infinity. There are no \"noise\" points in the traditional clustering sense. When HDBSCAN or DBSCAN labels low-density samples as noise, these samples still belong to some basin -- reassigning them to the nearest cluster ensures every initial condition receives an attractor label.</p> <p>For complete API documentation, see the Predictors API Reference.</p>"},{"location":"user-guide/predictors/#hdbscanclusterer-default","title":"HDBSCANClusterer (Default)","text":"<p>HDBSCAN excels at finding clusters of varying densities without requiring the number of clusters upfront. Our wrapper adds optional auto-tuning and noise reassignment.</p> <pre><code>from pybasin.predictors.hdbscan_clusterer import HDBSCANClusterer\n\npredictor = HDBSCANClusterer(\n    auto_tune=True,      # Auto-select min_cluster_size\n    assign_noise=True,   # Reassign noise points to nearest cluster\n)\n</code></pre>"},{"location":"user-guide/predictors/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>hdbscan</code> <code>HDBSCAN</code> or <code>None</code> <code>HDBSCAN(min_cluster_size=50)</code> Configured sklearn HDBSCAN instance <code>auto_tune</code> <code>bool</code> <code>False</code> Auto-select <code>min_cluster_size</code> via silhouette <code>assign_noise</code> <code>bool</code> <code>False</code> Reassign noise points (-1) to nearest cluster <code>nearest_neighbors</code> <code>NearestNeighbors</code> <code>NearestNeighbors(n_neighbors=5)</code> KNN for noise reassignment"},{"location":"user-guide/predictors/#auto-tuning-algorithm","title":"Auto-Tuning Algorithm","text":"<p>When <code>auto_tune=True</code>, the clusterer searches for the optimal <code>min_cluster_size</code> using silhouette score analysis. Given \\(n\\) samples, it evaluates several candidate sizes:</p> \\[ S_{\\text{candidates}} = \\left\\{ \\max(10, 0.005n), \\max(25, 0.01n), \\max(50, 0.02n), \\max(100, 0.03n), \\max(150, 0.05n) \\right\\} \\] <p>For each candidate \\(s \\in S_{\\text{candidates}}\\):</p> <ol> <li>Set <code>min_cluster_size = s</code> and <code>min_samples = min(10, s // 5)</code></li> <li>Run HDBSCAN clustering to obtain labels \\(L\\)</li> <li>Compute the silhouette score \\(\\sigma(s)\\) over non-noise samples:</li> </ol> \\[ \\sigma(s) = \\frac{1}{|L_{valid}|} \\sum_{i: L_i \\neq -1} \\frac{b_i - a_i}{\\max(a_i, b_i)} \\] <p>where \\(a_i\\) is the mean intra-cluster distance and \\(b_i\\) is the mean nearest-cluster distance for sample \\(i\\).</p> <p>The algorithm selects the <code>min_cluster_size</code> with the highest \\(\\sigma\\):</p> \\[ s^* = \\arg\\max_{s \\in S_{\\text{candidates}}} \\sigma(s) \\] <p>When to disable auto-tuning</p> <p>Auto-tuning adds computational overhead. If you know the approximate cluster sizes for your system, passing a pre-configured <code>HDBSCAN</code> instance is faster.</p>"},{"location":"user-guide/predictors/#noise-reassignment","title":"Noise Reassignment","text":"<p>HDBSCAN labels low-density points as noise (label -1). When <code>assign_noise=True</code>, these points are reassigned to their nearest cluster using k-nearest neighbors voting. This ensures every trajectory receives a basin label:</p> <pre><code># All noise points assigned to clusters\npredictor = HDBSCANClusterer(auto_tune=True, assign_noise=True)\n</code></pre>"},{"location":"user-guide/predictors/#dbscanclusterer","title":"DBSCANClusterer","text":"<p>Standard DBSCAN with optional epsilon auto-tuning based on the MATLAB bSTAB algorithm.</p> <pre><code>from pybasin.predictors.dbscan_clusterer import DBSCANClusterer\n\npredictor = DBSCANClusterer(\n    auto_tune=True,       # Automatic epsilon search\n    assign_noise=True,    # Reassign noise to nearest cluster\n)\n</code></pre>"},{"location":"user-guide/predictors/#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Default Description <code>dbscan</code> <code>DBSCAN</code> or <code>None</code> <code>DBSCAN(eps=0.5, min_samples=10)</code> Pre-configured DBSCAN instance <code>auto_tune</code> <code>bool</code> <code>False</code> Auto-find epsilon via silhouette peaks <code>n_eps_grid</code> <code>int</code> <code>200</code> Number of epsilon candidates <code>tune_sample_size</code> <code>int</code> <code>2000</code> Max samples for tuning (subsampled if larger) <code>min_peak_height</code> <code>float</code> <code>0.9</code> Minimum silhouette peak height <code>assign_noise</code> <code>bool</code> <code>False</code> Reassign noise points to nearest cluster"},{"location":"user-guide/predictors/#epsilon-auto-tuning-algorithm","title":"Epsilon Auto-Tuning Algorithm","text":"<p>DBSCAN requires an <code>eps</code> (epsilon) parameter -- the maximum distance between two samples for them to be considered neighbors. Choosing the right value is critical: too small and most points become noise with fragmented clusters; too large and distinct clusters merge together. The auto-tuning algorithm automates this selection so users do not need to manually tune epsilon for each feature space.</p> <p>The epsilon search replicates the MATLAB bSTAB <code>classify_solution.m</code> unsupervised branch. Given a feature matrix \\(X \\in \\mathbb{R}^{n \\times d}\\) where \\(n\\) is the number of samples and \\(d\\) is the number of features:</p> <p>Step 1: Build epsilon grid</p> <p>Let \\(X_j\\) denote the \\(j\\)-th column (feature) of \\(X\\). Compute feature ranges \\(R_j = \\max(X_j) - \\min(X_j)\\) for each dimension \\(j \\in \\{1, \\ldots, d\\}\\). The minimum range \\(R_{\\min} = \\min_j R_j\\) determines the search grid:</p> \\[ \\varepsilon_{\\text{grid}} = \\left\\{ \\frac{R_{\\min}}{N}, \\frac{2 R_{\\min}}{N}, \\ldots, R_{\\min} \\right\\} \\] <p>where \\(N\\) is the number of grid points (<code>n_eps_grid</code>, default 200).</p> <p>Using \\(R_{\\min}\\) as the upper bound is a heuristic from the original bSTAB implementation. The reasoning: if epsilon exceeds the smallest feature dimension's range, all points become neighbors in that dimension, losing discriminative power. This approach also provides scale invariance -- the search adapts to the actual data range rather than using arbitrary fixed values.</p> <p>Step 2: Evaluate silhouette scores</p> <p>For each candidate \\(\\varepsilon\\) in the grid, run DBSCAN to obtain cluster labels \\(L_i\\) for each sample \\(i\\). DBSCAN assigns \\(L_i = -1\\) to noise points (samples not belonging to any cluster). Compute the minimum per-sample silhouette score over non-noise samples:</p> \\[ \\sigma_{\\min}(\\varepsilon) = \\min_{i: L_i \\neq -1} s_i \\] <p>where \\(s_i\\) is the silhouette coefficient of sample \\(i\\). The silhouette coefficient measures how similar a sample is to its own cluster compared to other clusters, ranging from -1 (wrong cluster) to +1 (well-matched). Using the minimum rather than the mean provides a worst-case quality measure.</p> <p>Step 3: Peak detection</p> <p>Rather than simply taking the maximum silhouette score, the algorithm uses scipy's <code>find_peaks</code> to detect local maxima in \\(\\sigma_{\\min}\\) and selects the most prominent peak above <code>min_peak_height</code>. Let \\(P\\) denote the set of grid indices where local maxima with \\(\\sigma_{\\min} \\geq\\) <code>min_peak_height</code> were detected. The optimal epsilon \\(\\varepsilon^*\\) is:</p> \\[ \\varepsilon^* = \\varepsilon_{\\text{grid}}[k^*] \\quad \\text{where} \\quad k^* = \\arg\\max_{k \\in P} \\text{prominence}(k) \\] <p>Why prominence instead of maximum? The silhouette curve often has multiple local maxima. A peak's prominence measures how much you would need to descend before climbing to a higher peak -- it captures how \"significant\" a peak is, not just how tall. Using <code>argmax</code> alone might select a sharp spike caused by noise or edge effects. Prominence identifies stable, well-separated clustering regimes.</p> <p>If no peak exceeds the threshold (i.e., \\(P = \\emptyset\\)), the algorithm falls back to the global maximum: \\(\\varepsilon^* = \\varepsilon_{\\text{grid}}[\\arg\\max_k \\sigma_{\\min}(\\varepsilon_{\\text{grid}}[k])]\\).</p> <p>Performance consideration</p> <p>When the dataset exceeds <code>tune_sample_size</code>, a random subsample is used for the search. This keeps tuning fast while still finding a good epsilon.</p>"},{"location":"user-guide/predictors/#supervised-classification-with-templateintegrator","title":"Supervised Classification with TemplateIntegrator","text":"<p>When the attractor types are known beforehand (e.g., from bifurcation analysis), supervised classification produces more reliable results than unsupervised clustering. The workflow requires two components:</p> <ol> <li>A classifier -- any sklearn estimator where <code>is_classifier(predictor)</code> returns <code>True</code> (e.g., <code>KNeighborsClassifier</code>, <code>SVC</code>, <code>RandomForestClassifier</code>)</li> <li>A TemplateIntegrator -- holds template initial conditions and their labels</li> </ol>"},{"location":"user-guide/predictors/#how-it-works","title":"How It Works","text":"<p>The <code>TemplateIntegrator</code> integrates template initial conditions (one per known attractor) and extracts features from the resulting trajectories. These features, paired with their labels, form the training set for the classifier.</p>"},{"location":"user-guide/predictors/#example-duffing-oscillator-with-knn-classifier","title":"Example: Duffing Oscillator with KNN Classifier","text":"<pre><code>from sklearn.neighbors import KNeighborsClassifier\n\nfrom pybasin.basin_stability_estimator import BasinStabilityEstimator\nfrom pybasin.template_integrator import TemplateIntegrator\nfrom pybasin.sampler import UniformRandomSampler\nfrom pybasin.solvers import JaxSolver\n\n# Template initial conditions -- one per known attractor\ntemplate_y0 = [\n    [-0.21, 0.02],   # Attractor 1\n    [1.05, 0.77],    # Attractor 2\n    [-0.67, 0.02],   # Attractor 3\n]\n\n# Labels corresponding to each template\nlabels = [\"y1\", \"y2\", \"y3\"]\n\n# ODE parameters for template integration (may differ from main study)\node_params = {\"delta\": 0.08, \"k3\": 1, \"A\": 0.2}\n\ntemplate_integrator = TemplateIntegrator(\n    template_y0=template_y0,\n    labels=labels,\n    ode_params=ode_params,\n)\n\n# Any sklearn classifier works\nclassifier = KNeighborsClassifier(n_neighbors=1)\n\nbse = BasinStabilityEstimator(\n    n=10_000,\n    ode_system=ode_system,\n    sampler=sampler,\n    predictor=classifier,\n    template_integrator=template_integrator,  # Required for classifiers\n)\n\nbs_vals = bse.estimate_bs()\n</code></pre>"},{"location":"user-guide/predictors/#templateintegrator-parameters","title":"TemplateIntegrator Parameters","text":"Parameter Type Description <code>template_y0</code> <code>list[list[float]]</code> Initial conditions for template trajectories <code>labels</code> <code>list[str]</code> Ground truth labels (one per template IC) <code>ode_params</code> <code>Mapping[str, Any]</code> ODE parameters for template integration <code>solver</code> <code>SolverProtocol</code> Optional dedicated solver (defaults to CPU variant) <p>For complete API documentation, see the TemplateIntegrator API.</p>"},{"location":"user-guide/predictors/#when-to-use-supervised-classification","title":"When to Use Supervised Classification","text":"<p>Use supervised classification when:</p> <ul> <li>You know the attractor types from bifurcation analysis or domain knowledge</li> <li>The system has been studied before and attractors are well-characterized</li> <li>Unsupervised clustering fails to separate known attractor types</li> <li>You need consistent labeling across parameter studies</li> </ul> <p>Unsupervised clustering is preferable when:</p> <ul> <li>Attractor types are unknown or exploratory analysis is needed</li> <li>The number of attractors varies with parameters</li> <li>Manual labeling of templates is impractical</li> </ul>"},{"location":"user-guide/predictors/#creating-custom-estimators","title":"Creating Custom Estimators","text":"<p>Any sklearn-compatible estimator works with <code>BasinStabilityEstimator</code>. Write a custom clusterer by subclassing <code>BaseEstimator</code> and <code>ClusterMixin</code>, then implementing <code>fit_predict()</code>. The key requirement: return an array of labels (strings or integers) with one label per sample.</p> <p>For the complete sklearn developer guide, see Developing scikit-learn estimators.</p>"},{"location":"user-guide/predictors/#quick-reference","title":"Quick Reference","text":"<p>Clusterer (unsupervised):</p> <pre><code>from sklearn.base import BaseEstimator, ClusterMixin\n\nclass MyClusterer(BaseEstimator, ClusterMixin):\n    def __init__(self, threshold: float = 0.5):\n        self.threshold = threshold\n\n    def fit_predict(self, X, y=None):\n        # X is a numpy array of shape (n_samples, n_features)\n        # Return labels array of shape (n_samples,)\n        labels = ...\n        return labels\n</code></pre> <p>Classifier (supervised):</p> <pre><code>from sklearn.base import BaseEstimator, ClassifierMixin\n\nclass MyClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, n_neighbors: int = 5):\n        self.n_neighbors = n_neighbors\n\n    def fit(self, X, y):\n        # Store training data\n        self.X_train_ = X\n        self.y_train_ = y\n        return self\n\n    def predict(self, X):\n        # Return predicted labels\n        labels = ...\n        return labels\n</code></pre>"},{"location":"user-guide/predictors/#example-synchronizationclassifier","title":"Example: SynchronizationClassifier","text":"<p>The Rossler network case study uses a threshold-based classifier that labels trajectories as \"synchronized\" or \"desynchronized\" based on the maximum deviation between network nodes:</p> <pre><code>from typing import Any\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClusterMixin\n\n\nclass SynchronizationClassifier(BaseEstimator, ClusterMixin):\n    \"\"\"\n    Classifier that labels trajectories as 'synchronized' or 'desynchronized'.\n\n    Works with features from SynchronizationFeatureExtractor, which computes\n    the max deviation across all node pairs. Synchronization is achieved when:\n        max_deviation_all &lt; epsilon\n\n    :param epsilon: Synchronization threshold.\n    :param feature_index: Index of the feature to use for thresholding.\n        Options: 0=max_deviation_x, 1=max_deviation_y, 2=max_deviation_z, 3=max_deviation_all\n    \"\"\"\n\n    def __init__(\n        self,\n        epsilon: float = 0.1,\n        feature_index: int = 3,\n    ):\n        self.epsilon = epsilon\n        self.feature_index = feature_index\n\n    def fit_predict(self, X: Any, y: Any = None) -&gt; np.ndarray:\n        \"\"\"\n        Classify each trajectory as synchronized or desynchronized.\n\n        :param X: Feature matrix of shape (n_samples, 4).\n        :param y: Ignored. Present for API compatibility.\n        :return: Labels array with 'synchronized' or 'desynchronized'.\n        \"\"\"\n        X_arr = np.asarray(X)\n        max_deviation = X_arr[:, self.feature_index]\n\n        labels = np.where(\n            max_deviation &lt; self.epsilon,\n            \"synchronized\",\n            \"desynchronized\",\n        )\n\n        return labels\n</code></pre> <p>This classifier inherits from <code>ClusterMixin</code> because it implements <code>fit_predict()</code> directly -- it does not need separate training. Despite returning semantic labels, sklearn treats it as a clusterer.</p>"},{"location":"user-guide/predictors/#string-labels-vs-integer-labels","title":"String Labels vs Integer Labels","text":"<p>pyBasin handles both string and integer labels. HDBSCAN returns integers (0, 1, 2, ..., -1 for noise), while the <code>SynchronizationClassifier</code> returns strings. The <code>BasinStabilityEstimator</code> converts all labels to strings internally when computing basin stability fractions.</p>"},{"location":"user-guide/predictors/#using-sklearn-estimators-directly","title":"Using Sklearn Estimators Directly","text":"<p>Beyond the built-in predictors, any sklearn estimator works. A few examples:</p> <pre><code># Gaussian Mixture Model clustering\nfrom sklearn.mixture import GaussianMixture\npredictor = GaussianMixture(n_components=3)\n\n# K-Means clustering\nfrom sklearn.cluster import KMeans\npredictor = KMeans(n_clusters=2)\n\n# Support Vector Classification (supervised)\nfrom sklearn.svm import SVC\npredictor = SVC(kernel='rbf')\n\n# Decision Tree (supervised)\nfrom sklearn.tree import DecisionTreeClassifier\npredictor = DecisionTreeClassifier(max_depth=5)\n</code></pre> <p>Supervised classifiers require TemplateIntegrator</p> <p>When using a classifier (<code>is_classifier(predictor)</code> returns <code>True</code>), you must provide a <code>template_integrator</code> to the <code>BasinStabilityEstimator</code>. Without it, the estimator raises a <code>ValueError</code>.</p>"},{"location":"user-guide/predictors/#feature-name-awareness","title":"Feature Name Awareness","text":"<p>Some predictors need feature names to select specific columns (for example, <code>DynamicalSystemClusterer</code> -- see the Dynamics-Based Clustering guide). If your custom predictor needs this capability, implement the <code>FeatureNameAware</code> protocol:</p> <pre><code>from pybasin.protocols import FeatureNameAware\n\nclass MyFeatureAwareClusterer(BaseEstimator, ClusterMixin, FeatureNameAware):\n    def set_feature_names(self, names: list[str]) -&gt; None:\n        self.feature_names_ = names\n\n    def fit_predict(self, X, y=None):\n        # Access self.feature_names_ to find specific features by name\n        ...\n</code></pre> <p>The <code>BasinStabilityEstimator</code> automatically calls <code>set_feature_names()</code> on predictors that implement it.</p>"},{"location":"user-guide/predictors/#summary","title":"Summary","text":"Use Case Recommended Predictor Unknown attractors, exploratory <code>HDBSCANClusterer(auto_tune=True)</code> Known attractors, labeled data <code>KNeighborsClassifier</code> + <code>TemplateIntegrator</code> Physics-aware classification <code>DynamicalSystemClusterer</code> (guide) Simple threshold-based logic Custom <code>ClusterMixin</code> class Need MATLAB bSTAB compatibility <code>DBSCANClusterer(auto_tune=True)</code>"},{"location":"user-guide/samplers/","title":"Samplers","text":"<p>Samplers generate initial conditions for basin stability estimation. All samplers return PyTorch tensors (<code>float32</code>) and support GPU acceleration.</p> <p>Tensor Precision</p> <p>All samplers use <code>float32</code> precision for GPU efficiency (5-10x faster than <code>float64</code>). Samples are returned as <code>torch.Tensor</code> on the configured device.</p>"},{"location":"user-guide/samplers/#available-samplers","title":"Available Samplers","text":"Class Description Returns Exact N? Deterministic? <code>UniformRandomSampler</code> Uniform random in hypercube \u2713 With seed <code>GridSampler</code> Evenly spaced regular grid \u2717 (scales up) \u2713 <code>GaussianSampler</code> Gaussian around midpoint \u2713 \u2717 <code>CsvSampler</code> Load from CSV file \u2713 \u2713"},{"location":"user-guide/samplers/#common-parameters","title":"Common Parameters","text":"<p>All samplers share these constructor parameters:</p> Parameter Type Description <code>min_limits</code> <code>list[float]</code> Minimum value for each state dimension <code>max_limits</code> <code>list[float]</code> Maximum value for each state dimension <code>device</code> <code>str</code> or <code>None</code> <code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect"},{"location":"user-guide/samplers/#fixed-dimensions","title":"Fixed Dimensions","text":"<p>To fix a state variable to a constant value (e.g., initial velocity = 0), set the same value for both <code>min</code> and <code>max</code> in that dimension. All samplers handle this correctly:</p> <pre><code># 3D state space (x, y, z) with z fixed at 0\nsampler = UniformRandomSampler(\n    min_limits=[-10.0, -20.0, 0.0],\n    max_limits=[10.0, 20.0, 0.0],  # z is fixed at 0\n)\n</code></pre> <p>CsvSampler Exception</p> <p><code>CsvSampler</code> does not accept <code>min_limits</code>/<code>max_limits</code>\u2014bounds are computed from the CSV data.</p>"},{"location":"user-guide/samplers/#uniformrandomsampler","title":"UniformRandomSampler","text":"<p>Generates random samples uniformly distributed within the bounding hypercube.</p> <pre><code>from pybasin.sampler import UniformRandomSampler\nimport numpy as np\n\nsampler = UniformRandomSampler(\n    min_limits=[-np.pi, -2.0],\n    max_limits=[np.pi, 2.0],\n    device=\"cuda\",  # optional, auto-detects GPU\n)\n\n# Generate 10,000 samples (returns exactly 10,000)\nsamples = sampler.sample(n=10000)\n\n# Use a different seed for different random samples\nsamples = sampler.sample(n=10000, seed=42)\n\n# Disable seeding for non-reproducible randomness\nsamples = sampler.sample(n=10000, seed=None)\n</code></pre> <p>Default Seed</p> <p>The default seed is <code>299792458</code> (speed of light in m/s). This ensures reproducible results by default.</p>"},{"location":"user-guide/samplers/#gridsampler","title":"GridSampler","text":"<p>Generates evenly spaced samples in a regular grid pattern. Ideal for 2D visualizations and deterministic sampling.</p> <pre><code>from pybasin.sampler import GridSampler\nimport numpy as np\n\nsampler = GridSampler(\n    min_limits=[-np.pi, -2.0],\n    max_limits=[np.pi, 2.0],\n)\n\nsamples = sampler.sample(n=10000)  # Returns 10,000 samples (100\u00d7100 grid)\n</code></pre>"},{"location":"user-guide/samplers/#sample-count-scaling","title":"Sample Count Scaling","text":"<p>The grid sampler rounds up the requested sample count to form a complete grid. For a d-dimensional space, it computes:</p> \\[n_{\\text{per dim}} = \\lceil n^{1/d} \\rceil\\] <p>The actual number of samples returned is \\(n_{\\text{per dim}}^d\\).</p> <p>2D Examples:</p> Requested N Points per Dimension Actual Samples 50 \u230850^0.5\u2309 = 8 8\u00b2 = 64 100 \u2308100^0.5\u2309 = 10 10\u00b2 = 100 1,000 \u23081000^0.5\u2309 = 32 32\u00b2 = 1,024 20,000 \u230820000^0.5\u2309 = 142 142\u00b2 = 20,164"},{"location":"user-guide/samplers/#fixed-dimensions-and-sample-count","title":"Fixed Dimensions and Sample Count","text":"<p>When using fixed dimensions (see Fixed Dimensions), only the varying dimensions contribute to the grid calculation.</p> <p>Given \\(d\\) varying dimensions and requested \\(n\\) samples, the points per varying dimension is \\(\\lceil n^{1/d} \\rceil\\). Fixed dimensions always contribute exactly 1 point, so the total number of samples is:</p> \\[\\text{total} = \\underbrace{\\lceil n^{1/d} \\rceil \\times \\lceil n^{1/d} \\rceil \\times \\cdots}_{d \\text{ varying dims}} \\times \\underbrace{1 \\times 1 \\times \\cdots}_{\\text{fixed dims}} = (\\lceil n^{1/d} \\rceil)^d\\] <p>Example: 3D space with 1 fixed dimension and \\(n = 20000\\):</p> <ul> <li>\\(d = 2\\) varying dimensions \u2192 \\(20000^{1/2} = 141.42...\\), so \\(\\lceil 141.42 \\rceil = 142\\) points per dimension</li> <li>Total: \\(142 \\times 142 \\times 1 = 20164\\) samples</li> </ul>"},{"location":"user-guide/samplers/#gaussiansampler","title":"GaussianSampler","text":"<p>Generates samples from a Gaussian distribution centered at the midpoint of each dimension. Samples are clamped to stay within bounds.</p> <pre><code>from pybasin.sampler import GaussianSampler\n\nsampler = GaussianSampler(\n    min_limits=[-np.pi, -2.0],\n    max_limits=[np.pi, 2.0],\n    std_factor=0.2,  # \u03c3 = 20% of the range (default)\n)\n\nsamples = sampler.sample(n=10000)\n</code></pre> <p>The distribution parameters are computed as:</p> \\[\\mu_i = \\frac{\\text{min}_i + \\text{max}_i}{2}, \\quad \\sigma_i = \\text{std_factor} \\times (\\text{max}_i - \\text{min}_i)\\]"},{"location":"user-guide/samplers/#csvsampler","title":"CsvSampler","text":"<p>Loads pre-defined samples from a CSV file. Essential for reproducing results from MATLAB or other reference implementations.</p>"},{"location":"user-guide/samplers/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>csv_path</code> <code>str</code> or <code>Path</code> Required Path to the CSV file containing samples <code>coordinate_columns</code> <code>list[str]</code> Required Column names to use as state coordinates <code>label_column</code> <code>str</code> or <code>None</code> <code>None</code> Column name for ground truth labels <code>device</code> <code>str</code> or <code>None</code> <code>None</code> <code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect <pre><code>from pybasin.sampler import CsvSampler\n\nsampler = CsvSampler(\n    csv_path=\"data/initial_conditions.csv\",\n    coordinate_columns=[\"x1\", \"x2\"],      # Column names for state variables\n    label_column=\"attractor\",             # Optional: ground truth labels\n    device=\"cuda\",                        # Optional: auto-detects GPU if None\n)\n\n# Get all samples from the file\nsamples = sampler.sample()\n\n# Or get the first n samples\nsamples = sampler.sample(n=1000)\n\n# Access ground truth labels (if provided)\nlabels = sampler.labels  # numpy array or None\n</code></pre> <p>Bounds Auto-Detection</p> <p>Unlike other samplers, <code>CsvSampler</code> does not require <code>min_limits</code> and <code>max_limits</code>. These are automatically computed from the data in the CSV file.</p>"},{"location":"user-guide/samplers/#exceptions","title":"Exceptions","text":"Exception Condition <code>FileNotFoundError</code> CSV file does not exist at the specified path <code>ValueError</code> Coordinate columns not found in CSV <code>ValueError</code> Label column not found in CSV (when specified) <code>ValueError</code> Requested <code>n</code> samples exceeds available data"},{"location":"user-guide/samplers/#properties","title":"Properties","text":"Property Type Description <code>labels</code> <code>np.ndarray</code> or <code>None</code> Ground truth labels from CSV <code>n_samples</code> <code>int</code> Total number of samples in the file"},{"location":"user-guide/samplers/#creating-custom-samplers","title":"Creating Custom Samplers","text":"<p>Inherit from <code>Sampler</code> and implement the <code>sample</code> method:</p> <pre><code>from pybasin.sampler import Sampler\nimport torch\n\nclass LatinHypercubeSampler(Sampler):\n    \"\"\"Latin Hypercube sampling for better space coverage.\"\"\"\n\n    display_name: str = \"Latin Hypercube Sampler\"\n\n    def __init__(\n        self,\n        min_limits: list[float],\n        max_limits: list[float],\n        device: str | None = None,\n    ):\n        super().__init__(min_limits, max_limits, device)\n\n    def sample(self, n: int) -&gt; torch.Tensor:\n        # Your implementation here\n        # Must return tensor of shape (n, self.state_dim)\n        ...\n</code></pre>"},{"location":"user-guide/samplers/#base-class-attributes","title":"Base Class Attributes","text":"<p>After calling <code>super().__init__()</code>, these attributes are available:</p> Attribute Type Description <code>device</code> <code>torch.device</code> Target device (<code>cuda:0</code> or <code>cpu</code>) <code>min_limits</code> <code>torch.Tensor</code> Minimum bounds as <code>float32</code> tensor on <code>device</code> <code>max_limits</code> <code>torch.Tensor</code> Maximum bounds as <code>float32</code> tensor on <code>device</code> <code>state_dim</code> <code>int</code> Number of state dimensions (length of limits)"},{"location":"user-guide/samplers/#requirements","title":"Requirements","text":"<ol> <li>Call <code>super().__init__()</code> with <code>min_limits</code>, <code>max_limits</code>, and <code>device</code></li> <li>Return a <code>torch.Tensor</code> of shape <code>(n, self.state_dim)</code></li> <li>Use <code>self.device</code> when creating tensors to ensure GPU compatibility</li> <li>Use <code>float32</code> dtype for consistency with the base class</li> </ol>"},{"location":"user-guide/solution/","title":"Solution","text":"<p>The <code>Solution</code> class stores all results from a basin stability estimation run. After integration, feature extraction, and classification complete, the solution object holds trajectories, extracted features, cluster labels, and metadata. You can inspect it directly or pass it to plotters for visualization.</p> <p><code>BasinStabilityEstimator</code> creates a <code>Solution</code> automatically during the integration step and mutates it through the pipeline. Access it via <code>bse.solution</code> after calling <code>estimate_bs()</code>.</p>"},{"location":"user-guide/solution/#whats-stored","title":"What's Stored","text":"Property Type Description Available After <code>initial_condition</code> <code>torch.Tensor</code> Initial conditions (shape: <code>(N, n_states)</code>) Integration <code>time</code> <code>torch.Tensor</code> Time evaluation points (shape: <code>(n_steps,)</code>) Integration <code>y</code> <code>torch.Tensor</code> Trajectories (shape: <code>(n_steps, N, n_states)</code>) Integration <code>extracted_features</code> <code>torch.Tensor</code> or <code>None</code> Raw features pre-filtering (shape: <code>(N, F)</code>) Feature extraction <code>extracted_feature_names</code> <code>list[str]</code> or <code>None</code> Names of raw features Feature extraction <code>features</code> <code>torch.Tensor</code> or <code>None</code> Filtered features (shape: <code>(N, F')</code>) Feature selection or None <code>feature_names</code> <code>list[str]</code> or <code>None</code> Names of filtered features Feature selection or None <code>labels</code> <code>np.ndarray</code> or <code>None</code> Basin assignments for each IC (shape: <code>(N,)</code>) Classification <code>model_params</code> <code>dict[str, Any]</code> or <code>None</code> ODE parameters used during integration Integration"},{"location":"user-guide/solution/#accessing-the-solution","title":"Accessing the Solution","text":"<p>Retrieve the solution object from the estimator after running <code>estimate_bs()</code>:</p> <pre><code>from pybasin.basin_stability_estimator import BasinStabilityEstimator\n\nbse = BasinStabilityEstimator(ode_system=ode, sampler=sampler)\nresults = bse.estimate_bs()\n\n# Access the complete solution\nsolution = bse.solution\n\n# Trajectories for all initial conditions\ntrajectories = solution.y  # (n_steps, N, n_states)\n\n# Initial conditions\nics = solution.initial_condition  # (N, n_states)\n\n# Cluster labels\nlabels = solution.labels  # (N,)\n</code></pre>"},{"location":"user-guide/solution/#features-and-feature-names","title":"Features and Feature Names","text":"<p>Raw features extracted from trajectories are stored separately from filtered features. Feature selectors reduce the raw set down to a smaller collection based on variance thresholds or other criteria.</p> <pre><code># Raw features before filtering\nraw_features = solution.extracted_features  # (N, F)\nraw_names = solution.extracted_feature_names  # list[str] with F names\n\n# Filtered features after selection\nfiltered_features = solution.features  # (N, F') where F' &lt;= F\nfiltered_names = solution.feature_names  # list[str] with F' names\n</code></pre> <p>If no feature selector is used, <code>features</code> and <code>feature_names</code> will be <code>None</code>. The raw features remain accessible via <code>extracted_features</code> and <code>extracted_feature_names</code>.</p>"},{"location":"user-guide/solution/#summary-output","title":"Summary Output","text":"<p>Call <code>get_summary()</code> to produce a JSON-serializable dictionary with key information. This is useful for logging or saving results to disk.</p> <pre><code>summary = solution.get_summary()\nprint(summary)\n</code></pre> <p>Output structure:</p> <pre><code>{\n    \"initial_condition\": [[x1, y1], [x2, y2], ...],  # N initial conditions\n    \"num_time_steps\": 1000,\n    \"trajectory_shape\": [1000, 10000, 2],\n    \"features\": [[f1, f2, ...], ...] or None,\n    \"labels\": [0, 1, 0, 2, ...] or None,\n    \"model_params\": {\"alpha\": 0.1, \"K\": 1.0, ...} or None,\n}\n</code></pre>"},{"location":"user-guide/solution/#manual-construction","title":"Manual Construction","text":"<p>While <code>BasinStabilityEstimator</code> normally creates the solution object for you, you can also construct one manually for standalone feature extraction or custom workflows:</p> <pre><code>import torch\nfrom pybasin.solution import Solution\n\n# Create synthetic trajectory data\ntime = torch.linspace(0, 1000, 1000)  # 1000 time points\ny0 = torch.randn(500, 2)  # 500 initial conditions, 2-state system\ny = torch.randn(1000, 500, 2)  # trajectories\n\nsolution = Solution(\n    initial_condition=y0,\n    time=time,\n    y=y,\n    model_params={\"param1\": 1.0, \"param2\": 2.5},\n)\n\n# Now you can pass this to a feature extractor\nfrom pybasin.feature_extractors import TorchFeatureExtractor\n\nextractor = TorchFeatureExtractor(features=\"minimal\")\nfeatures = extractor.extract_features(solution)\n</code></pre>"},{"location":"user-guide/solution/#shape-requirements","title":"Shape Requirements","text":"<p>The constructor enforces strict shape constraints and raises <code>AssertionError</code> if shapes mismatch:</p> <ul> <li><code>initial_condition</code>: 2D tensor of shape <code>(N, n_states)</code></li> <li><code>time</code>: 1D tensor of shape <code>(n_steps,)</code></li> <li><code>y</code>: 3D tensor of shape <code>(n_steps, N, n_states)</code></li> </ul> <p>The number of time steps in <code>y.shape[0]</code> must match <code>time.shape[0]</code>. The batch dimension <code>y.shape[1]</code> must match <code>initial_condition.shape[0]</code>. The state dimension <code>y.shape[2]</code> must match <code>initial_condition.shape[1]</code>.</p>"},{"location":"user-guide/solution/#modifying-the-solution","title":"Modifying the Solution","text":"<p>Three setter methods allow pipeline components to update the solution as results become available:</p>"},{"location":"user-guide/solution/#set_labelslabels","title":"<code>set_labels(labels)</code>","text":"<p>Assigns cluster or classification labels. Labels must be a NumPy array with length matching the number of initial conditions.</p> <pre><code>import numpy as np\n\nlabels = np.array([0, 1, 0, 2, 1, ...])  # length N\nsolution.set_labels(labels)\n</code></pre>"},{"location":"user-guide/solution/#set_extracted_featuresfeatures-names","title":"<code>set_extracted_features(features, names)</code>","text":"<p>Stores raw features extracted from trajectories before any filtering step.</p> <pre><code>features = torch.randn(500, 50)  # 500 samples, 50 features\nnames = [f\"feature_{i}\" for i in range(50)]\n\nsolution.set_extracted_features(features, names)\n</code></pre>"},{"location":"user-guide/solution/#set_featuresfeatures-namesnone","title":"<code>set_features(features, names=None)</code>","text":"<p>Stores filtered features after feature selection. Names are optional -- if omitted, <code>feature_names</code> remains unchanged.</p> <pre><code>filtered_features = torch.randn(500, 20)  # 500 samples, 20 selected features\nfiltered_names = [f\"selected_feature_{i}\" for i in range(20)]\n\nsolution.set_features(filtered_features, filtered_names)\n</code></pre> <p>Pipeline Usage</p> <p>In normal usage, you never call these setters yourself. Feature extractors, feature selectors, and predictors call them automatically as they process the solution object. These methods are documented here for completeness and for custom pipeline development.</p>"},{"location":"user-guide/solution/#related-documentation","title":"Related Documentation","text":"<ul> <li>API Reference -- Full method signatures and docstrings.</li> <li>Feature Extractors -- How features are extracted and stored in the solution.</li> <li>Feature Selectors -- How features are filtered and <code>feature_names</code> is updated.</li> <li>Basin Stability Estimator Overview -- How the solution object flows through the estimation pipeline.</li> </ul>"},{"location":"user-guide/solvers/","title":"Solvers","text":"<p>Solvers numerically integrate ODE systems from batches of initial conditions. Every solver in pyBasin conforms to <code>SolverProtocol</code>, accepts PyTorch tensors as input, returns PyTorch tensors as output, and supports persistent disk caching -- regardless of the underlying numerical backend.</p> <p>Unified Tensor Interface</p> <p>All solvers accept <code>torch.Tensor</code> inputs and return <code>torch.Tensor</code> outputs. Internal conversions to JAX arrays, NumPy arrays, or other formats happen transparently. You do not need to handle backend-specific tensor types.</p>"},{"location":"user-guide/solvers/#available-solvers","title":"Available Solvers","text":"Class Backend CPU GPU (CUDA) Event Functions Recommended For <code>JaxSolver</code> JAX/Diffrax Yes Yes Yes Default choice -- fastest on GPU <code>TorchDiffEqSolver</code> torchdiffeq Yes Yes No Fastest on CPU at large N <code>TorchOdeSolver</code> torchode Yes Yes No Independent per-trajectory step sizes <code>ScipyParallelSolver</code> scipy/sklearn Yes No No Debugging, reference baselines"},{"location":"user-guide/solvers/#common-parameters","title":"Common Parameters","text":"<p>All solvers share these constructor parameters:</p> Parameter Type Default Description <code>time_span</code> <code>tuple[float, float]</code> <code>(0, 1000)</code> Integration interval <code>(t_start, t_end)</code> <code>n_steps</code> <code>int</code> <code>1000</code> Number of evaluation points <code>device</code> <code>str</code> or <code>None</code> <code>None</code> <code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect <code>rtol</code> <code>float</code> <code>1e-8</code> Relative tolerance for adaptive stepping <code>atol</code> <code>float</code> <code>1e-6</code> Absolute tolerance for adaptive stepping <code>cache_dir</code> <code>str</code> or <code>None</code> <code>\".pybasin_cache\"</code> Cache directory path. <code>None</code> disables caching"},{"location":"user-guide/solvers/#generic-solver-api","title":"Generic Solver API","text":"<p>All solvers expose three capabilities through <code>SolverProtocol</code>:</p>"},{"location":"user-guide/solvers/#integrateode_system-y0","title":"<code>integrate(ode_system, y0)</code>","text":"<p>Solves the ODE system for a batch of initial conditions. The <code>y0</code> tensor must be 2D with shape <code>(batch, n_dims)</code>, where <code>batch</code> is the number of initial conditions and <code>n_dims</code> is the number of state variables. A <code>ValueError</code> is raised if <code>y0</code> is not 2D -- the error message suggests using <code>y0.unsqueeze(0)</code> for single trajectories.</p> <p>Returns a tuple <code>(t_eval, y_values)</code>:</p> <ul> <li><code>t_eval</code> has shape <code>(n_steps,)</code> -- the time points at which solutions are evaluated</li> <li><code>y_values</code> has shape <code>(n_steps, batch, n_dims)</code> -- the state at each time point for every initial condition</li> </ul> <pre><code>t_eval, y_values = solver.integrate(ode_system, y0)\n# t_eval.shape   -&gt; (n_steps,)\n# y_values.shape -&gt; (n_steps, batch, n_dims)\n</code></pre>"},{"location":"user-guide/solvers/#clone-device-n_steps_factor-cache_dir","title":"<code>clone(*, device, n_steps_factor, cache_dir)</code>","text":"<p>Creates a copy of the solver with optionally overridden settings. Useful for creating a high-resolution variant for plotting while keeping the original solver for computation:</p> <pre><code>plot_solver = solver.clone(n_steps_factor=10, device=\"cpu\")\n</code></pre> Parameter Type Default Description <code>device</code> <code>str</code> or <code>None</code> <code>None</code> Override the device. <code>None</code> keeps current device <code>n_steps_factor</code> <code>int</code> <code>1</code> Multiply <code>n_steps</code> by this factor <code>cache_dir</code> <code>str</code>, <code>None</code>, or <code>UNSET</code> <code>UNSET</code> Override cache directory. <code>None</code> disables caching. <code>UNSET</code> keeps current setting"},{"location":"user-guide/solvers/#device-attribute","title":"<code>device</code> attribute","text":"<p>A <code>torch.device</code> indicating where output tensors are placed. Always reflects the normalized device (e.g. <code>torch.device(\"cuda:0\")</code>, never bare <code>\"cuda\"</code>).</p>"},{"location":"user-guide/solvers/#behavior-notes","title":"Behavior Notes","text":"<p>Several behaviors apply to all solvers and happen automatically. Understanding them prevents unexpected surprises.</p>"},{"location":"user-guide/solvers/#device-auto-detection","title":"Device Auto-Detection","text":"<p>When <code>device=None</code>, the solver checks whether CUDA is available via <code>torch.cuda.is_available()</code> (or the JAX equivalent for <code>JaxSolver</code>). If a GPU is found, <code>\"cuda:0\"</code> is selected; otherwise the solver falls back to <code>\"cpu\"</code>. The string <code>\"cuda\"</code> is always normalized to <code>\"cuda:0\"</code> internally.</p>"},{"location":"user-guide/solvers/#float32-precision","title":"float32 Precision","text":"<p>All solvers operate in <code>float32</code>. Time evaluation points are created as <code>float32</code> tensors, and the solver logs a warning if <code>y0</code> arrives with a different dtype or on a mismatched device. No automatic casting is performed -- you must ensure <code>y0</code> is <code>float32</code>.</p>"},{"location":"user-guide/solvers/#ode-system-device-transfer","title":"ODE System Device Transfer","text":"<p>PyTorch-based solvers (<code>TorchDiffEqSolver</code>, <code>TorchOdeSolver</code>, <code>ScipyParallelSolver</code>) call <code>ode_system.to(self.device)</code> before integration. This moves the ODE system's parameters to the solver's device automatically. <code>JaxSolver</code> does not perform this transfer because JAX ODE systems are stateless and device placement is handled through <code>jax.device_put</code>.</p>"},{"location":"user-guide/solvers/#caching","title":"Caching","text":"<p>Every solver caches integration results to disk so that repeated runs with identical inputs skip the numerical integration entirely. Caching is controlled through the <code>cache_dir</code> constructor parameter, which defaults to <code>\".pybasin_cache\"</code>. Pass <code>None</code> to disable it.</p> <pre><code># Default -- cache under .pybasin_cache/ at the project root\nsolver = JaxSolver(time_span=(0, 1000), n_steps=5000)\n\n# Explicit subfolder for a specific system\nsolver = JaxSolver(time_span=(0, 1000), n_steps=5000, cache_dir=\".pybasin_cache/pendulum\")\n\n# No caching\nsolver = JaxSolver(time_span=(0, 1000), n_steps=5000, cache_dir=None)\n</code></pre>"},{"location":"user-guide/solvers/#path-resolution","title":"Path Resolution","text":"<p>Relative paths (like <code>\".pybasin_cache\"</code> or <code>\".pybasin_cache/pendulum\"</code>) are resolved from the project root, which is located by walking up the directory tree until a <code>pyproject.toml</code> or <code>.git</code> marker is found. Absolute paths are used as-is. The directory is created automatically if it does not exist.</p>"},{"location":"user-guide/solvers/#cache-keys","title":"Cache Keys","text":"<p>The cache key is an MD5 hash built from six components: the solver class name, the ODE system's source representation (via <code>get_str()</code>), the ODE system parameters, the serialized <code>y0</code> and <code>t_eval</code> tensors, and solver-specific configuration (tolerances, method, etc.). Changing any of these produces a different key, so stale results are never returned.</p>"},{"location":"user-guide/solvers/#storage-format","title":"Storage Format","text":"<p>Cached tensors are stored using safetensors, which provides fast, zero-copy loading without the security concerns of pickle. On cache load, tensors are moved to the solver's current device. Corrupted files are detected and deleted automatically rather than raising exceptions.</p>"},{"location":"user-guide/solvers/#jaxsolver","title":"JaxSolver","text":"<p>The recommended solver for most workloads. It uses Diffrax (Kidger, 2021) for numerical integration, with <code>jax.vmap</code> for batch processing and JIT compilation for performance. On GPU, <code>JaxSolver</code> achieves near-constant integration time regardless of sample count -- roughly 11.5 seconds for N ranging from 5,000 to 100,000 in benchmark tests. It is also the only solver that supports per-trajectory event-based early termination, which is critical for systems with unbounded trajectories.</p> <p>Default Solver</p> <p><code>JaxSolver</code> is the default and recommended solver. It delivers the best GPU performance and is the only solver supporting event functions for early trajectory termination. For CPU-only workloads at large sample sizes (N &gt;= 100k), <code>TorchDiffEqSolver</code> is faster. See the Solver Comparison benchmark for detailed numbers.</p> <p><code>JaxSolver</code> does not inherit from the <code>Solver</code> base class. It implements <code>SolverProtocol</code> independently with its own device handling and caching logic. Two construction modes are available: a generic API for standard ODE integration, and a <code>solver_args</code> mode that passes arguments directly to <code>diffrax.diffeqsolve()</code>.</p>"},{"location":"user-guide/solvers/#generic-api","title":"Generic API","text":"<pre><code>from pybasin.solvers import JaxSolver\nfrom diffrax import Dopri5\n\nsolver = JaxSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    device=\"cuda\",\n    method=Dopri5(),       # Diffrax solver instance\n    rtol=1e-8,\n    atol=1e-6,\n    max_steps=16**5,       # Maximum integrator steps\n    event_fn=None,         # Optional early termination\n)\n</code></pre>"},{"location":"user-guide/solvers/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_span</code> <code>tuple[float, float]</code> <code>(0, 1000)</code> Integration interval <code>(t_start, t_end)</code> <code>n_steps</code> <code>int</code> <code>1000</code> Number of evaluation points <code>device</code> <code>str</code> or <code>None</code> <code>None</code> <code>\"cuda\"</code>, <code>\"gpu\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect <code>method</code> Diffrax solver or <code>None</code> <code>None</code> Diffrax solver instance. Defaults to <code>Dopri5()</code> if <code>None</code> <code>rtol</code> <code>float</code> <code>1e-8</code> Relative tolerance for <code>PIDController</code> <code>atol</code> <code>float</code> <code>1e-6</code> Absolute tolerance for <code>PIDController</code> <code>max_steps</code> <code>int</code> <code>16**5</code> Maximum number of integrator steps (1,048,576) <code>event_fn</code> <code>Callable</code> or <code>None</code> <code>None</code> Event function for per-trajectory early termination <code>cache_dir</code> <code>str</code> or <code>None</code> <code>\".pybasin_cache\"</code> Cache directory path. <code>None</code> disables caching <p>Device String</p> <p>Unlike PyTorch-based solvers, <code>JaxSolver</code> also accepts <code>\"gpu\"</code> as a device string (mapped to JAX's GPU backend). Both <code>\"cuda\"</code> and <code>\"gpu\"</code> resolve to the same GPU device.</p>"},{"location":"user-guide/solvers/#tensor-conversion","title":"Tensor Conversion","text":"<p><code>JaxSolver</code> converts between PyTorch and JAX tensors at the integration boundary. On GPU, this conversion uses DLPack for zero-copy transfer -- no data is duplicated in device memory. On CPU, the conversion falls back to a NumPy intermediate. Input tensors are PyTorch; output tensors are PyTorch. You never interact with JAX arrays directly.</p>"},{"location":"user-guide/solvers/#solver_args-mode","title":"solver_args Mode","text":"<p>For advanced use cases (SDEs, CDEs, custom step-size controllers, or any configuration not exposed by the generic API), you can pass a dictionary of keyword arguments directly to <code>diffrax.diffeqsolve()</code>:</p> <pre><code>from diffrax import Dopri5, ODETerm, PIDController, SaveAt\nimport jax.numpy as jnp\n\nsolver = JaxSolver(\n    solver_args={\n        \"terms\": ODETerm(lambda t, y, args: -y),\n        \"solver\": Dopri5(),\n        \"t0\": 0,\n        \"t1\": 10,\n        \"dt0\": 0.1,\n        \"saveat\": SaveAt(ts=jnp.linspace(0, 10, 100)),\n        \"stepsize_controller\": PIDController(rtol=1e-5, atol=1e-5),\n    },\n)\n</code></pre> <p>When <code>solver_args</code> is provided, all other Diffrax-specific parameters (<code>time_span</code>, <code>n_steps</code>, <code>solver</code>, <code>rtol</code>, <code>atol</code>, <code>max_steps</code>, <code>event_fn</code>) are ignored entirely. The solver wraps each call with <code>jax.vmap</code> and injects <code>y0</code> per trajectory -- do not include <code>y0</code> in the dictionary.</p> <p>Baked-in Time Points</p> <p>In solver_args mode, the integration time points are determined by the <code>saveat</code> entry you provide. Calling <code>clone(n_steps_factor=10)</code> will not increase the time resolution -- the original <code>saveat.ts</code> is used as-is. A warning is logged if <code>n_steps_factor &gt; 1</code> in this mode.</p> <p>Because <code>solver_args</code> bypasses all automatic setup, no <code>ODETerm</code> wrapping, <code>PIDController</code> creation, or <code>SaveAt</code> construction is performed. You are responsible for providing a complete and valid set of Diffrax arguments.</p>"},{"location":"user-guide/solvers/#event-functions","title":"Event Functions","text":"<p>Event functions enable per-trajectory early termination, which is essential for systems where some trajectories diverge to infinity (e.g. the Lorenz system's \"broken butterfly\" regime). Each trajectory stops independently when the event triggers, while bounded trajectories continue integrating normally.</p> <p>The event function signature is <code>(t, y, args) -&gt; scalar Array</code>. Return a positive value to continue integration, or zero/negative to stop:</p> <pre><code>import jax.numpy as jnp\n\ndef lorenz_stop_event(t, y, args, **kwargs):\n    \"\"\"Stop integration when any state variable exceeds 200 in absolute value.\"\"\"\n    max_val = 200.0\n    return max_val - jnp.max(jnp.abs(y))\n</code></pre> <pre><code>solver = JaxSolver(\n    time_span=(0, 1000),\n    n_steps=4000,\n    device=\"cuda\",\n    event_fn=lorenz_stop_event,\n)\n</code></pre> <p>Internally, the event function is wrapped in a <code>diffrax.Event(cond_fn=event_fn)</code> and passed to <code>diffeqsolve</code>. For more details on handling diverging trajectories, see the Handling Unbounded Trajectories guide.</p> <p>Post-event state values are <code>inf</code></p> <p>When an event triggers early termination, Diffrax fills the remaining saved time points (those after the event) with <code>inf</code>. For example, if a trajectory diverges at \\(t = 50\\) but <code>saveat</code> requests points up to \\(t = 1000\\), all state values for \\(t &gt; 50\\) will be <code>inf</code>. Your feature extraction or classification code must handle this -- checking for <code>jnp.isinf</code> in the final state is a reliable way to detect terminated trajectories.</p> <p>Event functions in solver_args mode</p> <p>When using <code>solver_args</code>, include the event directly in the dictionary (e.g. as a <code>diffrax.Event</code> instance) rather than using the <code>event_fn</code> parameter.</p> <p>Reference:</p> <p>Kidger, P. (2021). On Neural Differential Equations. PhD thesis, University of Oxford. https://docs.kidger.site/diffrax/</p>"},{"location":"user-guide/solvers/#torchdiffeqsolver","title":"TorchDiffEqSolver","text":"<p>A PyTorch-native solver built on torchdiffeq (Chen, 2018). It supports both adaptive-step and fixed-step methods, runs on CPU and CUDA, and integrates directly with <code>ODESystem</code> subclasses (which inherit from <code>torch.nn.Module</code>). At large sample sizes on CPU (N = 100,000), <code>TorchDiffEqSolver</code> is roughly 2x faster than <code>JaxSolver</code> on CPU -- though <code>JaxSolver</code> on GPU remains substantially faster overall.</p> <pre><code>from pybasin.solvers.torchdiffeq_solver import TorchDiffEqSolver\n\nsolver = TorchDiffEqSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    device=\"cuda\",\n    method=\"dopri5\",\n    rtol=1e-8,\n    atol=1e-6,\n)\n</code></pre>"},{"location":"user-guide/solvers/#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_span</code> <code>tuple[float, float]</code> <code>(0, 1000)</code> Integration interval <code>(t_start, t_end)</code> <code>n_steps</code> <code>int</code> <code>1000</code> Number of evaluation points <code>device</code> <code>str</code> or <code>None</code> <code>None</code> <code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect <code>method</code> <code>str</code> <code>\"dopri5\"</code> Integration method (see table below) <code>rtol</code> <code>float</code> <code>1e-8</code> Relative tolerance for adaptive stepping <code>atol</code> <code>float</code> <code>1e-6</code> Absolute tolerance for adaptive stepping <code>cache_dir</code> <code>str</code> or <code>None</code> <code>\".pybasin_cache\"</code> Cache directory path. <code>None</code> disables caching"},{"location":"user-guide/solvers/#available-methods","title":"Available Methods","text":"Method Type Description <code>dopri5</code> Adaptive-step Dormand-Prince 5(4) (default) <code>dopri8</code> Adaptive-step Dormand-Prince 8(5,3) <code>bosh3</code> Adaptive-step Bogacki-Shampine 3(2) <code>euler</code> Fixed-step Forward Euler <code>rk4</code> Fixed-step Classic Runge-Kutta 4 <p>Integration runs under <code>torch.no_grad()</code>, so no gradient graph is constructed during forward integration. The solver calls <code>ode_system.to(self.device)</code> before integrating, which moves the ODE system's <code>nn.Module</code> parameters to the solver's device.</p> <p>Reference:</p> <p>Chen, R. T. Q. (2018). torchdiffeq. https://github.com/rtqichen/torchdiffeq</p>"},{"location":"user-guide/solvers/#torchodesolver","title":"TorchOdeSolver","text":"<p>A parallel ODE solver built on torchode (Lienen &amp; Gunnemann, 2022). Its distinguishing feature is independent step-size control per batch element: each trajectory can advance with its own time step, avoiding the performance penalty that arises when a single stiff trajectory forces small steps for the entire batch.</p> <pre><code>from pybasin.solvers.torchode_solver import TorchOdeSolver\n\nsolver = TorchOdeSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    device=\"cuda\",\n    method=\"dopri5\",\n    rtol=1e-8,\n    atol=1e-6,\n)\n</code></pre>"},{"location":"user-guide/solvers/#constructor-parameters_2","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_span</code> <code>tuple[float, float]</code> <code>(0, 1000)</code> Integration interval <code>(t_start, t_end)</code> <code>n_steps</code> <code>int</code> <code>1000</code> Number of evaluation points <code>device</code> <code>str</code> or <code>None</code> <code>None</code> <code>\"cuda\"</code>, <code>\"cpu\"</code>, or <code>None</code> for auto-detect <code>method</code> <code>str</code> <code>\"dopri5\"</code> Integration method (see table below) <code>rtol</code> <code>float</code> <code>1e-8</code> Relative tolerance for adaptive stepping <code>atol</code> <code>float</code> <code>1e-6</code> Absolute tolerance for adaptive stepping <code>cache_dir</code> <code>str</code> or <code>None</code> <code>\".pybasin_cache\"</code> Cache directory path. <code>None</code> disables caching"},{"location":"user-guide/solvers/#available-methods_1","title":"Available Methods","text":"Method Type Description <code>dopri5</code> Adaptive-step Dormand-Prince 5(4) (default) <code>tsit5</code> Adaptive-step Tsitouras 5(4) <code>euler</code> Fixed-step Forward Euler <code>heun</code> Fixed-step Heun's method <p>The method string is lowercased internally, so <code>\"Dopri5\"</code> and <code>\"dopri5\"</code> are equivalent. Integration runs under <code>torch.inference_mode()</code>. Internally, torchode uses <code>IntegralController</code> for adaptive step-size selection and <code>AutoDiffAdjoint</code> as the solver wrapper.</p> <p>Performance at Large N</p> <p>Benchmark results show that <code>TorchOdeSolver</code> scales poorly at large sample sizes. At N = 100,000 it took roughly 310 seconds on CUDA -- compared to about 11 seconds for <code>JaxSolver</code>. Consider <code>TorchOdeSolver</code> primarily when per-trajectory step-size independence is important for correctness, not for raw throughput. See the Solver Comparison benchmark for details.</p> <p>Reference:</p> <p>Lienen, M., &amp; Gunnemann, S. (2022). torchode: A Parallel ODE Solver for PyTorch. The Symbiosis of Deep Learning and Differential Equations II, NeurIPS. https://openreview.net/forum?id=uiKVKTiUYB0</p>"},{"location":"user-guide/solvers/#scipyparallelsolver","title":"ScipyParallelSolver","text":"<p>A CPU-only solver that delegates integration to <code>scipy.integrate.solve_ivp</code> and parallelizes across initial conditions using <code>sklearn.utils.parallel.Parallel</code> with the loky backend. Each trajectory is solved independently in a separate process. This solver is primarily useful for debugging, for validating results against a well-established reference implementation, and for accessing scipy's implicit solvers (<code>Radau</code>, <code>BDF</code>) which handle stiff systems.</p> <pre><code>from pybasin.solvers.scipy_solver import ScipyParallelSolver\n\nsolver = ScipyParallelSolver(\n    time_span=(0, 1000),\n    n_steps=5000,\n    n_jobs=-1,           # Use all CPU cores\n    method=\"RK45\",\n    rtol=1e-8,\n    atol=1e-6,\n    max_step=None,       # Defaults to (t_end - t_start) / 100\n)\n</code></pre>"},{"location":"user-guide/solvers/#constructor-parameters_3","title":"Constructor Parameters","text":"Parameter Type Default Description <code>time_span</code> <code>tuple[float, float]</code> <code>(0, 1000)</code> Integration interval <code>(t_start, t_end)</code> <code>n_steps</code> <code>int</code> <code>1000</code> Number of evaluation points <code>device</code> <code>str</code> or <code>None</code> <code>None</code> Only <code>\"cpu\"</code> is supported (see note below) <code>n_jobs</code> <code>int</code> <code>-1</code> Number of parallel workers (<code>-1</code> for all CPU cores) <code>method</code> <code>str</code> <code>\"RK45\"</code> <code>scipy.integrate.solve_ivp</code> method <code>rtol</code> <code>float</code> <code>1e-6</code> Relative tolerance <code>atol</code> <code>float</code> <code>1e-8</code> Absolute tolerance <code>max_step</code> <code>float</code> or <code>None</code> <code>None</code> Maximum step size. Defaults to <code>(t_end - t_start) / 100</code> <code>cache_dir</code> <code>str</code> or <code>None</code> <code>\".pybasin_cache\"</code> Cache directory path. <code>None</code> disables caching <p>CPU Only</p> <p>If you pass <code>device=\"cuda\"</code>, the solver logs a warning and silently falls back to CPU. No error is raised. This applies to both the constructor and <code>clone()</code>.</p>"},{"location":"user-guide/solvers/#available-methods_2","title":"Available Methods","text":"<p>Scipy provides explicit methods (<code>RK45</code>, <code>RK23</code>, <code>DOP853</code>) and implicit methods for stiff problems (<code>Radau</code>, <code>BDF</code>, <code>LSODA</code>). See the scipy.integrate.solve_ivp documentation for the full list.</p>"},{"location":"user-guide/solvers/#parallelization-behavior","title":"Parallelization Behavior","text":"<p>When <code>batch_size &gt; 1</code> and <code>n_jobs != 1</code>, trajectories are distributed across worker processes using the loky backend. For a single trajectory (<code>batch_size == 1</code>) or when <code>n_jobs == 1</code>, execution is sequential with no multiprocessing overhead. Each worker converts tensors from PyTorch to NumPy, calls <code>solve_ivp</code>, and converts results back -- so this solver carries per-trajectory conversion overhead that the GPU-based solvers avoid.</p>"},{"location":"user-guide/solvers/#see-also","title":"See Also","text":"<ul> <li>Solver Comparison benchmark -- detailed timing data across backends and sample sizes</li> <li>Handling Unbounded Trajectories -- event functions and zero-masking strategies for diverging systems</li> </ul>"}]}